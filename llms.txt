Project Structure:
ğŸ“ twars-url2md
â”œâ”€â”€ ğŸ“ .github
â”‚   â””â”€â”€ ğŸ“ workflows
â”‚       â”œâ”€â”€ ğŸ“„ ci.yml
â”‚       â””â”€â”€ ğŸ“„ docs.yml
â”œâ”€â”€ ğŸ“ docs
â”œâ”€â”€ ğŸ“ example.com
â”‚   â””â”€â”€ ğŸ“„ index.md
â”œâ”€â”€ ğŸ“ example.org
â”‚   â””â”€â”€ ğŸ“„ index.md
â”œâ”€â”€ ğŸ“ issues
â”‚   â”œâ”€â”€ ğŸ“ resolved
â”‚   â”‚   â””â”€â”€ ğŸ“„ RESOLVED_ISSUES_SUMMARY.md
â”‚   â””â”€â”€ ğŸ“„ issuetest.py
â”œâ”€â”€ ğŸ“ scripts
â”‚   â”œâ”€â”€ ğŸ“„ build.sh
â”‚   â”œâ”€â”€ ğŸ“„ release.sh
â”‚   â””â”€â”€ ğŸ“„ test.sh
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“„ cli.rs
â”‚   â”œâ”€â”€ ğŸ“„ html.rs
â”‚   â”œâ”€â”€ ğŸ“„ lib.rs
â”‚   â”œâ”€â”€ ğŸ“„ main.rs
â”‚   â”œâ”€â”€ ğŸ“„ markdown.rs
â”‚   â”œâ”€â”€ ğŸ“„ tests.rs
â”‚   â””â”€â”€ ğŸ“„ url.rs
â”œâ”€â”€ ğŸ“ src_docs
â”‚   â”œâ”€â”€ ğŸ“ md
â”‚   â”‚   â”œâ”€â”€ ğŸ“ assets
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ extra.css
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ advanced.md
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ api.md
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ architecture.md
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ configuration.md
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ contributing.md
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ index.md
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ installation.md
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ quickstart.md
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ testing.md
â”‚   â”‚   â””â”€â”€ ğŸ“„ usage.md
â”‚   â””â”€â”€ ğŸ“„ mkdocs.yml
â”œâ”€â”€ ğŸ“ testdata
â”‚   â””â”€â”€ ğŸ“ out
â”‚       â””â”€â”€ ğŸ“ helpx.adobe.com
â”‚           â””â”€â”€ ğŸ“ pl
â”‚               â””â”€â”€ ğŸ“ indesign
â”‚                   â””â”€â”€ ... (depth limit reached)
â”œâ”€â”€ ğŸ“ tests
â”‚   â”œâ”€â”€ ğŸ“ common
â”‚   â”‚   â””â”€â”€ ğŸ“„ mod.rs
â”‚   â”œâ”€â”€ ğŸ“ fixtures
â”‚   â”‚   â”œâ”€â”€ ğŸ“ expected
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ simple_output.md
â”‚   â”‚   â”œâ”€â”€ ğŸ“ html
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ complex.html
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ simple.html
â”‚   â”‚   â””â”€â”€ ğŸ“ urls
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ mixed_content.txt
â”‚   â”‚       â””â”€â”€ ğŸ“„ test_urls.txt
â”‚   â”œâ”€â”€ ğŸ“ integration
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ e2e_tests.rs
â”‚   â”‚   â””â”€â”€ ğŸ“„ mod.rs
â”‚   â”œâ”€â”€ ğŸ“ unit
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ mod.rs
â”‚   â”‚   â””â”€â”€ ğŸ“„ url_tests.rs
â”‚   â”œâ”€â”€ ğŸ“„ benchmarks.rs
â”‚   â””â”€â”€ ğŸ“„ tests.rs
â”œâ”€â”€ ğŸ“ work
â”‚   â”œâ”€â”€ ğŸ“ html-to-text-comparison
â”‚   â”‚   â”œâ”€â”€ ğŸ“ out
â”‚   â”‚   â”œâ”€â”€ ğŸ“ src
â”‚   â”‚   â””â”€â”€ ğŸ“ target
â”‚   â”‚       â””â”€â”€ ğŸ“ release
â”‚   â”‚           â”œâ”€â”€ ğŸ“ deps
â”‚   â”‚           â”‚   â””â”€â”€ ... (depth limit reached)
â”‚   â”‚           â”œâ”€â”€ ğŸ“ examples
â”‚   â”‚           â”‚   â””â”€â”€ ... (depth limit reached)
â”‚   â”‚           â””â”€â”€ ğŸ“ incremental
â”‚   â”‚               â””â”€â”€ ... (depth limit reached)
â”‚   â””â”€â”€ ğŸ“ out
â”‚       â”œâ”€â”€ ğŸ“ github.com
â”‚       â”‚   â””â”€â”€ ğŸ“ huggingface
â”‚       â”‚       â””â”€â”€ ğŸ“ huggingface_hub
â”‚       â”‚           â””â”€â”€ ... (depth limit reached)
â”‚       â”œâ”€â”€ ğŸ“ huggingface.co
â”‚       â”‚   â””â”€â”€ ğŸ“ docs
â”‚       â”‚       â”œâ”€â”€ ğŸ“ api-inference
â”‚       â”‚       â”‚   â””â”€â”€ ... (depth limit reached)
â”‚       â”‚       â””â”€â”€ ğŸ“ huggingface_hub
â”‚       â”‚           â””â”€â”€ ... (depth limit reached)
â”‚       â””â”€â”€ ğŸ“ raw.githubusercontent.com
â”‚           â”œâ”€â”€ ğŸ“ huggingface
â”‚           â”‚   â””â”€â”€ ğŸ“ huggingface_hub
â”‚           â”‚       â””â”€â”€ ... (depth limit reached)
â”‚           â””â”€â”€ ğŸ“ scope3data
â”‚               â””â”€â”€ ğŸ“ scope3ai-py
â”‚                   â””â”€â”€ ... (depth limit reached)
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ AGENTS.md
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md
â”œâ”€â”€ ğŸ“„ build.rs
â”œâ”€â”€ ğŸ“„ build.sh
â”œâ”€â”€ ğŸ“„ Cargo.toml
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ CLAUDE.md
â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md
â”œâ”€â”€ ğŸ“„ DEVELOPMENT.md
â”œâ”€â”€ ğŸ“„ GEMINI.md
â”œâ”€â”€ ğŸ“„ install.sh
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ test_http.rs
â”œâ”€â”€ ğŸ“„ TESTS.md
â””â”€â”€ ğŸ“„ TODO.md


<documents>
<document index="1">
<source>.cursorrules</source>
<document_content>
# https://github.com/twardoch/twars-url2md

## 1. Project Overview

`twars-url2md` is a Rust CLI application that downloads URLs and converts them to clean, readable Markdown files. It can extract URLs from various text formats and process them concurrently.

## 2. Commands

### 2.1. Development
```bash
# Run tests
cargo test --all-features

# Format code
cargo fmt

# Run linter
cargo clippy --all-targets --all-features

# Build release binary
cargo build --release

# Run with arguments
cargo run -- [OPTIONS]
```

### 2.2. Publishing
```bash
# Verify package before publishing
cargo package

# Publish to crates.io
cargo publish
```

## 3. Architecture

The codebase follows a modular design with clear separation of concerns:

- `src/main.rs` - Entry point with panic handling wrapper
- `src/lib.rs` - Core processing logic and orchestration
- `src/cli.rs` - Command-line interface using Clap
- `src/url.rs` - URL extraction and validation logic
- `src/html.rs` - HTML fetching and cleaning using Monolith
- `src/markdown.rs` - HTML to Markdown conversion using htmd

### 3.1. Key Design Decisions

1. **Panic Recovery**: The application catches panics from the Monolith library to prevent crashes during HTML processing (see `src/main.rs:10-20`).

2. **Concurrent Processing**: Uses Tokio with adaptive concurrency based on available CPUs for parallel URL processing (see `src/lib.rs:107-120`).

3. **Error Handling**: Uses `anyhow` for error propagation with custom error types and retry logic for network failures.

4. **Output Organization**: Generates file paths based on URL structure when using `-o` option, creating a repository-like structure.

## 4. Testing

Run tests with `cargo test --all-features`. The test module is in `src/tests.rs` and includes unit tests for URL extraction and processing logic.

## 5. Dependencies

- **Async Runtime**: tokio with full features
- **HTTP Client**: reqwest with vendored OpenSSL
- **HTML Processing**: monolith for cleaning, htmd for Markdown conversion
- **CLI**: clap with derive feature
- **Utilities**: indicatif (progress bars), rayon (parallelism)

## 6. Active Development

See `TODO.md` for the modernization plan, which includes:
- Migration from OpenSSL to rustls
- Integration of structured logging with tracing
- Enhanced error reporting
- Performance optimizations

When implementing new features, ensure compatibility with the async architecture and maintain the modular structure.

# Working principles for software development

## 7. When you write code (in any language)

- Iterate gradually, avoiding major changes 
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code youâ€™re writing with the rest of the code. 
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line 
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions 
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase:
  - README.md (purpose and functionality) 
  - CHANGELOG.md (past changes)
  - TODO.md (future goals)
  - PROGRESS.md (detailed flat task list)

## 8. Use MCP tools if you can

Before and during coding (if have access to tools), you should: 

- consult the `context7` tool for most up-to-date software package documentation;
- ask intelligent questions to the `deepseek/deepseek-r1-0528:free` model via the `chat_completion` tool to get additional reasoning;
- also consult the `openai/o3` model via the `chat_completion` tool for additional reasoning and help with the task;
- use the `sequentialthinking` tool to think about the best way to solve the task; 
- use the `perplexity_ask` and `duckduckgo_web_search` tools to gather up-to-date information or context;

## 9. Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter. 

## 10. If you write Python

- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with 

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

After Python changes run:

```
uzpy run .; fd -e py -x autoflake {}; fd -e py -x pyupgrade --py312-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 11. Additional guidelines

Ask before extending/refactoring existing code in a way that may add complexity or break things. 

When youâ€™re finished, print "Wait, but" to go back, think & reflect, revise & improvement what youâ€™ve done (but donâ€™t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". 

## 12. Virtual team work

Be creative, diligent, critical, relentless & funny! Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## 13. Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The URL-to-markdown conversion system consists of three primary business components:

### 13.1. URL Processing and Content Extraction (Importance: 95)
- Collects URLs from multiple input sources (stdin, files)
- Validates and resolves URLs against base URLs when needed
- Implements a robust retry mechanism for handling transient network failures
- Processes URLs concurrently with dynamic CPU core adaptation
- Located in `src/cli.rs` and `src/url.rs`

### 13.2. HTML Processing Pipeline (Importance: 90)
- Fetches HTML content using Monolith for specialized content extraction
- Implements fallback mechanisms for non-HTML content
- Handles custom HTML processing with configurable options
- Transforms complex HTML structures into clean markdown
- Located in `src/html.rs`

### 13.3. Content Organization and Output Management (Importance: 85)
- Creates structured output paths mirroring URL hierarchies
- Supports packing mode for combining multiple URLs into single documents
- Maintains original URL ordering in packed output
- Handles both remote URLs and local file paths consistently
- Located in `src/lib.rs` and `src/markdown.rs`

### 13.4. Integration Points

The system connects these components through:

1. URL Collection -> HTML Processing
- URLs are validated and normalized before processing
- Concurrent processing queue manages HTML fetching
- Retry logic handles failed attempts

2. HTML Processing -> Content Organization
- Processed HTML is transformed to markdown
- Output paths are generated based on URL structure
- Content is either packed or distributed based on mode

The build system embeds version and build metadata for traceability, ensuring each deployment can be traced to its source state.

To read the full codebase, run `npx repomix -o llms.txt .` and then read `llms.txt`

$END$



If you work with Python, use 'uv pip' instead of 'pip', and use 'uvx hatch test' instead of 'python -m pytest'. 

When I say /report, you must: Read all `./TODO.md` and `./PLAN.md` files and analyze recent changes. Document all changes in `./CHANGELOG.md`. From `./TODO.md` and `./PLAN.md` remove things that are done. Make sure that `./PLAN.md` contains a detailed, clear plan that discusses specifics, while `./TODO.md` is its flat simplified itemized `- [ ]`-prefixed representation. When I say /work, you must work in iterations like so: Read all `./TODO.md` and `./PLAN.md` files and reflect. Work on the tasks. Think, contemplate, research, reflect, refine, revise. Be careful, curious, vigilant, energetic. Verify your changes. Think aloud. Consult, research, reflect. Then update `./PLAN.md` and `./TODO.md` with tasks that will lead to improving the work youâ€™ve just done. Then '/report', and then iterate again.
</document_content>
</document>

<document index="2">
<source>.github/workflows/ci.yml</source>
<document_content>
name: CI/CD Pipeline

on:
  push:
    branches: [ "main" ]
    tags: [ "v*" ]
  pull_request:
    branches: [ "main" ]

permissions:
  contents: write
  packages: write

env:
  CARGO_TERM_COLOR: always
  RUSTFLAGS: "-D warnings"

jobs:
  test:
    name: Test Suite
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        rust: [stable, beta]
        exclude:
          # Skip beta on macOS and Windows for faster CI
          - os: macos-latest
            rust: beta
          - os: windows-latest
            rust: beta
    
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@master
        with:
          toolchain: ${{ matrix.rust }}
          components: rustfmt, clippy

      - name: Rust Cache
        uses: Swatinem/rust-cache@v2
        with:
          key: ${{ matrix.os }}-${{ matrix.rust }}

      - name: Check formatting
        run: cargo fmt --check
        if: matrix.rust == 'stable'

      - name: Run clippy
        run: cargo clippy --all-targets --all-features

      - name: Run unit tests
        run: cargo test --lib --all-features

      - name: Run integration tests
        run: cargo test --test '*' --all-features

      - name: Run doc tests
        run: cargo test --doc --all-features

      - name: Test build
        run: cargo build --release --all-features

  security-audit:
    name: Security Audit
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: rustsec/audit-check@v1.4.1
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

  coverage:
    name: Code Coverage
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Install cargo-tarpaulin
        run: cargo install cargo-tarpaulin

      - name: Run coverage
        run: cargo tarpaulin --out Xml --all-features

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./cobertura.xml
          fail_ci_if_error: false

  docs-check:
    name: Documentation Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install MkDocs dependencies
        run: |
          pip install --upgrade pip
          pip install mkdocs-material
          pip install mkdocs-minify-plugin
          pip install pymdown-extensions

      - name: Validate MkDocs configuration
        run: |
          cd src_docs
          mkdocs config

      - name: Build documentation (dry run)
        run: |
          cd src_docs
          mkdocs build --verbose --clean --strict

      - name: Check documentation links
        run: |
          echo "Checking for basic markdown syntax issues..."
          find src_docs/md -name "*.md" -exec echo "Validating: {}" \;
          # Basic validation - check files are not empty and have content
          find src_docs/md -name "*.md" -exec test -s {} \; -print

  create-release:
    name: Create Release
    needs: [test, security-audit, docs-check]
    if: startsWith(github.ref, 'refs/tags/')
    runs-on: ubuntu-latest
    outputs:
      upload_url: ${{ steps.create_release.outputs.upload_url }}
      release_id: ${{ steps.create_release.outputs.id }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get tag name
        id: tag_name
        run: echo "tag=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT

      - name: Create Release
        id: create_release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: ${{ steps.tag_name.outputs.tag }}
          release_name: Release ${{ steps.tag_name.outputs.tag }}
          draft: false
          prerelease: ${{ contains(steps.tag_name.outputs.tag, '-') }}
          body: |
            ## Changes
            
            This release includes various improvements and bug fixes.
            
            ## Installation
            
            ### From GitHub Releases
            
            Download the appropriate binary for your platform from the assets below.
            
            ### From crates.io
            
            ```bash
            cargo install twars-url2md
            ```
            
            ### From Source
            
            ```bash
            git clone https://github.com/twardoch/twars-url2md.git
            cd twars-url2md
            git checkout ${{ steps.tag_name.outputs.tag }}
            cargo build --release
            ```

  build-release:
    name: Build Release Binary
    needs: create-release
    if: startsWith(github.ref, 'refs/tags/')
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        include:
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu
            name: twars-url2md-linux-x86_64
            cross: false
          - os: ubuntu-latest
            target: aarch64-unknown-linux-gnu
            name: twars-url2md-linux-aarch64
            cross: true
          - os: ubuntu-latest
            target: x86_64-unknown-linux-musl
            name: twars-url2md-linux-x86_64-musl
            cross: true
          - os: macos-latest
            target: x86_64-apple-darwin
            name: twars-url2md-macos-x86_64
            cross: false
          - os: macos-latest
            target: aarch64-apple-darwin
            name: twars-url2md-macos-aarch64
            cross: false
          - os: windows-latest
            target: x86_64-pc-windows-msvc
            name: twars-url2md-windows-x86_64.exe
            cross: false

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: ${{ matrix.target }}

      - name: Install cross-compilation tools
        if: matrix.cross
        run: |
          cargo install cross --git https://github.com/cross-rs/cross

      - name: Build binary
        run: |
          if [ "${{ matrix.cross }}" = "true" ]; then
            cross build --release --target ${{ matrix.target }}
          else
            cargo build --release --target ${{ matrix.target }}
          fi

      - name: Prepare binary
        shell: bash
        run: |
          binary_name="twars-url2md"
          if [ "${{ runner.os }}" = "Windows" ]; then
            binary_name="twars-url2md.exe"
          fi
          
          binary_path="target/${{ matrix.target }}/release/$binary_name"
          
          if [ "${{ runner.os }}" = "Windows" ]; then
            7z a "${{ matrix.name }}.zip" "$binary_path"
            echo "asset_name=${{ matrix.name }}.zip" >> $GITHUB_ENV
            echo "asset_path=./${{ matrix.name }}.zip" >> $GITHUB_ENV
          else
            tar czf "${{ matrix.name }}.tar.gz" -C "target/${{ matrix.target }}/release" "$binary_name"
            echo "asset_name=${{ matrix.name }}.tar.gz" >> $GITHUB_ENV
            echo "asset_path=./${{ matrix.name }}.tar.gz" >> $GITHUB_ENV
          fi

      - name: Upload Release Asset
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.create-release.outputs.upload_url }}
          asset_path: ${{ env.asset_path }}
          asset_name: ${{ env.asset_name }}
          asset_content_type: application/octet-stream

  publish-crate:
    name: Publish to crates.io
    needs: [test, create-release]
    if: startsWith(github.ref, 'refs/tags/')
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Publish to crates.io
        run: cargo publish --token ${CRATES_TOKEN} --allow-dirty
        env:
          CRATES_TOKEN: ${{ secrets.CRATES_IO_TOKEN }}

</document_content>
</document>

<document index="3">
<source>.github/workflows/docs.yml</source>
<document_content>
name: Build and Deploy Documentation

on:
  push:
    branches: [ "main" ]
    paths:
      - 'src_docs/**'
      - '_github/workflows/docs.yml'
  pull_request:
    branches: [ "main" ]
    paths:
      - 'src_docs/**'
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build:
    name: Build Documentation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install mkdocs-material
          pip install mkdocs-minify-plugin
          pip install pymdown-extensions

      - name: Configure Git for MkDocs
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"

      - name: Build documentation
        run: |
          cd src_docs
          mkdocs build --verbose --clean --strict

      - name: Verify build output
        run: |
          echo "Build completed. Contents of docs directory:"
          ls -la docs/
          echo "Checking for index.html:"
          ls -la docs/index.html

      - name: Setup Pages
        if: github.ref == 'refs/heads/main'
        uses: actions/configure-pages@v3

      - name: Upload artifact
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-pages-artifact@v2
        with:
          path: './docs'

  deploy:
    name: Deploy to GitHub Pages
    if: github.ref == 'refs/heads/main'
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v2

  validate:
    name: Validate Documentation
    runs-on: ubuntu-latest
    needs: build
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install validation tools
        run: |
          pip install mkdocs-material
          pip install mkdocs-minify-plugin
          pip install pymdown-extensions

      - name: Validate configuration
        run: |
          cd src_docs
          mkdocs config

      - name: Check for broken links (basic validation)
        run: |
          cd src_docs
          mkdocs build --verbose
          echo "Documentation validation completed"

      - name: Validate markdown files
        run: |
          # Check that all markdown files have proper front matter or content
          find src_docs/md -name "*.md" -exec echo "Checking: {}" \;
          find src_docs/md -name "*.md" -exec head -5 {} \;
</document_content>
</document>

<document index="4">
<source>.gitignore</source>
<document_content>
**/*.rs.bk
*.pdb
._*
.apdisk
.AppleDB
.AppleDesktop
.AppleDouble
.com.apple.timemachine.donotpresent
.DocumentRevisions-V100
.DS_Store
.fseventsd
.idea/
.LSOverride
.Spotlight-V100
.TemporaryItems
.Trashes
.VolumeIcon.icns
.vscode/
debug/
Icon
Network Trash Folder
target/
Temporary Items
work/
/.specstory

</document_content>
</document>

<document index="5">
<source>.pre-commit-config.yaml.bak</source>
<document_content>
repos:
- repo: https://github.com/doublify/pre-commit-rust
  rev: v1.0
  hooks:
    - id: fmt
      args: ['--all', '--', '--check']
    - id: cargo-check
    - id: clippy
      args: ['--', -D, warnings]

- repo: https://github.com/pre-commit/pre-commit-hooks
  rev: v4.5.0
  hooks:
    - id: trailing-whitespace
    - id: end-of-file-fixer
    - id: check-yaml
    - id: check-toml
    - id: check-added-large-files

</document_content>
</document>

<document index="6">
<source>AGENTS.md</source>
<document_content>
# https://github.com/twardoch/twars-url2md

## 1. Project Overview

`twars-url2md` is a Rust CLI application that downloads URLs and converts them to clean, readable Markdown files. It can extract URLs from various text formats and process them concurrently.

## 2. Commands

### 2.1. Development
```bash
# Run tests
cargo test --all-features

# Format code
cargo fmt

# Run linter
cargo clippy --all-targets --all-features

# Build release binary
cargo build --release

# Run with arguments
cargo run -- [OPTIONS]
```

### 2.2. Publishing
```bash
# Verify package before publishing
cargo package

# Publish to crates.io
cargo publish
```

## 3. Architecture

The codebase follows a modular design with clear separation of concerns:

- `src/main.rs` - Entry point with panic handling wrapper
- `src/lib.rs` - Core processing logic and orchestration
- `src/cli.rs` - Command-line interface using Clap
- `src/url.rs` - URL extraction and validation logic
- `src/html.rs` - HTML fetching and cleaning using Monolith
- `src/markdown.rs` - HTML to Markdown conversion using htmd

### 3.1. Key Design Decisions

1. **Panic Recovery**: The application catches panics from the Monolith library to prevent crashes during HTML processing (see `src/main.rs:10-20`).

2. **Concurrent Processing**: Uses Tokio with adaptive concurrency based on available CPUs for parallel URL processing (see `src/lib.rs:107-120`).

3. **Error Handling**: Uses `anyhow` for error propagation with custom error types and retry logic for network failures.

4. **Output Organization**: Generates file paths based on URL structure when using `-o` option, creating a repository-like structure.

## 4. Testing

Run tests with `cargo test --all-features`. The test module is in `src/tests.rs` and includes unit tests for URL extraction and processing logic.

## 5. Dependencies

- **Async Runtime**: tokio with full features
- **HTTP Client**: reqwest with vendored OpenSSL
- **HTML Processing**: monolith for cleaning, htmd for Markdown conversion
- **CLI**: clap with derive feature
- **Utilities**: indicatif (progress bars), rayon (parallelism)

## 6. Active Development

See `TODO.md` for the modernization plan, which includes:
- Migration from OpenSSL to rustls
- Integration of structured logging with tracing
- Enhanced error reporting
- Performance optimizations

When implementing new features, ensure compatibility with the async architecture and maintain the modular structure.

# Working principles for software development

## 7. When you write code (in any language)

- Iterate gradually, avoiding major changes 
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code youâ€™re writing with the rest of the code. 
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line 
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions 
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase:
  - README.md (purpose and functionality) 
  - CHANGELOG.md (past changes)
  - TODO.md (future goals)
  - PROGRESS.md (detailed flat task list)

## 8. Use MCP tools if you can

Before and during coding (if have access to tools), you should: 

- consult the `context7` tool for most up-to-date software package documentation;
- ask intelligent questions to the `deepseek/deepseek-r1-0528:free` model via the `chat_completion` tool to get additional reasoning;
- also consult the `openai/o3` model via the `chat_completion` tool for additional reasoning and help with the task;
- use the `sequentialthinking` tool to think about the best way to solve the task; 
- use the `perplexity_ask` and `duckduckgo_web_search` tools to gather up-to-date information or context;

## 9. Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter. 

## 10. If you write Python

- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with 

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

After Python changes run:

```
uzpy run .; fd -e py -x autoflake {}; fd -e py -x pyupgrade --py312-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 11. Additional guidelines

Ask before extending/refactoring existing code in a way that may add complexity or break things. 

When youâ€™re finished, print "Wait, but" to go back, think & reflect, revise & improvement what youâ€™ve done (but donâ€™t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". 

## 12. Virtual team work

Be creative, diligent, critical, relentless & funny! Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## 13. Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The URL-to-markdown conversion system consists of three primary business components:

### 13.1. URL Processing and Content Extraction (Importance: 95)
- Collects URLs from multiple input sources (stdin, files)
- Validates and resolves URLs against base URLs when needed
- Implements a robust retry mechanism for handling transient network failures
- Processes URLs concurrently with dynamic CPU core adaptation
- Located in `src/cli.rs` and `src/url.rs`

### 13.2. HTML Processing Pipeline (Importance: 90)
- Fetches HTML content using Monolith for specialized content extraction
- Implements fallback mechanisms for non-HTML content
- Handles custom HTML processing with configurable options
- Transforms complex HTML structures into clean markdown
- Located in `src/html.rs`

### 13.3. Content Organization and Output Management (Importance: 85)
- Creates structured output paths mirroring URL hierarchies
- Supports packing mode for combining multiple URLs into single documents
- Maintains original URL ordering in packed output
- Handles both remote URLs and local file paths consistently
- Located in `src/lib.rs` and `src/markdown.rs`

### 13.4. Integration Points

The system connects these components through:

1. URL Collection -> HTML Processing
- URLs are validated and normalized before processing
- Concurrent processing queue manages HTML fetching
- Retry logic handles failed attempts

2. HTML Processing -> Content Organization
- Processed HTML is transformed to markdown
- Output paths are generated based on URL structure
- Content is either packed or distributed based on mode

The build system embeds version and build metadata for traceability, ensuring each deployment can be traced to its source state.

To read the full codebase, run `npx repomix -o llms.txt .` and then read `llms.txt`

$END$



If you work with Python, use 'uv pip' instead of 'pip', and use 'uvx hatch test' instead of 'python -m pytest'. 

When I say /report, you must: Read all `./TODO.md` and `./PLAN.md` files and analyze recent changes. Document all changes in `./CHANGELOG.md`. From `./TODO.md` and `./PLAN.md` remove things that are done. Make sure that `./PLAN.md` contains a detailed, clear plan that discusses specifics, while `./TODO.md` is its flat simplified itemized `- [ ]`-prefixed representation. When I say /work, you must work in iterations like so: Read all `./TODO.md` and `./PLAN.md` files and reflect. Work on the tasks. Think, contemplate, research, reflect, refine, revise. Be careful, curious, vigilant, energetic. Verify your changes. Think aloud. Consult, research, reflect. Then update `./PLAN.md` and `./TODO.md` with tasks that will lead to improving the work youâ€™ve just done. Then '/report', and then iterate again.
</document_content>
</document>

<document index="7">
<source>ARCHITECTURE.md</source>
<document_content>
# Architecture Documentation

## Overview

`twars-url2md` is designed as a high-performance, concurrent web scraping and conversion tool. The architecture emphasizes modularity, error resilience, and scalability.

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CLI Interface                          â”‚
â”‚                        (src/main.rs)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Core Library (src/lib.rs)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ URL Extractorâ”‚  â”‚ HTTP Client  â”‚  â”‚ Output Manager    â”‚   â”‚
â”‚  â”‚ (src/url.rs) â”‚  â”‚(src/html.rs) â”‚  â”‚ (src/lib.rs)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ HTML Cleaner â”‚  â”‚ MD Converter â”‚  â”‚ Error Handler     â”‚   â”‚
â”‚  â”‚ (Monolith)   â”‚  â”‚ (htmd)       â”‚  â”‚ (anyhow)         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. CLI Interface (`src/cli.rs`, `src/main.rs`)

**Responsibilities:**
- Parse command-line arguments using Clap
- Handle input from files or stdin
- Configure logging and verbosity
- Implement panic recovery for robustness

**Key Features:**
- Flexible input methods
- Custom error messages for better UX
- Panic hook to prevent crashes from malformed HTML

### 2. URL Processing (`src/url.rs`)

**Responsibilities:**
- Extract URLs from various text formats
- Validate and normalize URLs
- Create filesystem paths from URL structure
- Handle relative URL resolution

**Design Decisions:**
- Uses `linkify` for robust URL detection
- Supports HTML parsing with `html5ever`
- Handles local file paths (file:// protocol)
- Deduplicates URLs automatically

### 3. HTML Processing (`src/html.rs`)

**Responsibilities:**
- Fetch HTML content from URLs
- Clean HTML using Monolith
- Handle timeouts and retries
- Manage HTTP client configuration

**Architecture Highlights:**
- Curl-based HTTP client with browser-like behavior
- Panic recovery for Monolith operations
- Configurable retry mechanism with exponential backoff
- Resource cleanup and connection pooling

**HTTP Client Configuration:**
- Auto-negotiates HTTP version (HTTP/2 preferred)
- Sends comprehensive browser headers to avoid bot detection
- User-Agent: Chrome 120 on macOS
- Includes Sec-Ch-Ua headers for modern CDN compatibility
- 20-second connection timeout, 60-second total timeout

### 4. Markdown Conversion (`src/markdown.rs`)

**Responsibilities:**
- Convert cleaned HTML to Markdown
- Preserve document structure
- Handle edge cases gracefully

**Implementation:**
- Thin wrapper around `htmd` library
- Fallback for conversion failures
- Preserves semantic structure

### 5. Core Library (`src/lib.rs`)

**Responsibilities:**
- Orchestrate URL processing pipeline
- Manage concurrent operations
- Handle output file generation
- Progress tracking for batch operations

**Concurrency Model:**
- Adaptive worker pool based on CPU cores
- Semaphore-based concurrency limiting
- Async/await with Tokio runtime
- Progress bars for user feedback

## Data Flow

```
Input (File/Stdin)
       â”‚
       â–¼
URL Extraction â”€â”€â”€â”€â”€â”€â–º URL Validation
       â”‚                     â”‚
       â”‚                     â–¼
       â”‚              URL Normalization
       â”‚                     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         Concurrent Processing
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
         â–¼               â–¼
    HTTP Fetch      Local File Read
         â”‚               â”‚
         â–¼               â–¼
    HTML Cleaning   Pass-through
         â”‚               â”‚
         â–¼               â–¼
    MD Conversion   MD Conversion
         â”‚               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
           Output Writing
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
         â–¼               â–¼
    Directory        Packed File
    Structure          Output
```

## Error Handling Strategy

### 1. Graceful Degradation
- Monolith panic â†’ Simple HTML cleanup fallback
- HTTP timeout â†’ Curl fallback
- Conversion failure â†’ Basic text extraction

### 2. Retry Mechanism
```rust
Retry Logic:
- Initial attempt
- Retry 1: Wait 1 second
- Retry 2: Wait 2 seconds  
- Retry 3: Wait 4 seconds
- Report failure
```

### 3. Error Propagation
- Uses `anyhow` for rich error context
- Collects all errors for batch reporting
- Non-blocking: one failure doesn't stop others

## Performance Optimizations

### 1. Concurrency
- Worker pool size: `min(CPU_COUNT * 2, 16)`
- Prevents resource exhaustion
- Balances throughput and system load

### 2. Memory Management
- Streaming for large documents
- Reuse of HTTP client connections
- Efficient string operations

### 3. I/O Optimization
- Async file operations
- Buffered writing
- Directory creation batching

## Security Considerations

### 1. Content Security
- JavaScript execution disabled
- No external resource loading
- CSS and images stripped by default
- iframe content ignored

### 2. Network Security
- User agent spoofing for compatibility
- SSL/TLS verification (configurable)
- Timeout protection against slow servers

### 3. File System Security
- Path sanitization for output files
- No directory traversal attacks
- Safe handling of special characters

## Extension Points

### 1. URL Extractors
New extractors can be added by:
- Implementing pattern matching logic
- Adding to `extract_urls_from_text`
- Following existing validation patterns

### 2. Output Formats
Additional formats could include:
- JSON structured data
- EPUB for e-readers
- Plain text extraction

### 3. Processing Plugins
Potential plugin points:
- Custom HTML processors
- Content filters
- Metadata extractors

## Testing Strategy

### Unit Tests
- Module-level testing with 40+ unit tests
- Mock HTTP responses using curl
- Edge case coverage for URL parsing, HTML processing
- Test fixtures for various HTML structures

### Integration Tests
- End-to-end workflows (6+ integration test files)
- Real HTML processing with local files
- Concurrent operation testing
- Output mode verification (directory, single file, pack)

### Performance Tests
- Load testing with 100+ URLs
- Memory usage profiling
- Bottleneck identification
- Timeout and retry mechanism testing

### Issue Verification Suite
- Comprehensive test script (`issues/issuetest.py`)
- Validates all reported issues are resolved
- Tests CLI functionality, output modes, logging
- Ensures regression prevention

## Future Enhancements

### 1. Streaming Architecture
- Process large documents without full memory load
- Progressive output generation
- Real-time processing pipeline

### 2. Distributed Processing
- Job queue for URL processing
- Horizontal scaling capability
- Result aggregation service

### 3. Smart Caching
- Content deduplication
- Incremental updates
- ETa/Last-Modified support

## CDN Compatibility

### Known Compatible CDNs
- **Cloudflare**: Full compatibility with bot detection bypass
- **Fastly**: Works correctly with HTTP/2 negotiation
- **Akamai**: Handles edge cases properly
- **Adobe CDN**: Fixed timeout issues with proper headers

### Key Compatibility Features
1. **HTTP Version Negotiation**: Never forces HTTP/1.1, allows natural ALPN
2. **Browser Headers**: Sends full set of modern browser headers
3. **TLS Fingerprint**: Uses curl which has browser-like TLS behavior
4. **User-Agent**: Mimics real Chrome browser

### Header Strategy
The application sends these headers to ensure CDN compatibility:
- User-Agent matching Chrome 120
- Accept headers for HTML content
- Sec-Ch-Ua headers for Client Hints
- Sec-Fetch headers for request metadata
- Cache-Control and Pragma headers

## Dependencies

### Core Dependencies
- `tokio`: Async runtime
- `curl`: HTTP client (primary)
- `monolith`: HTML cleaning
- `htmd`: Markdown conversion
- `clap`: CLI parsing
- `anyhow`: Error handling

### Design Rationale
- **Tokio**: Industry standard async runtime
- **Curl**: Better compatibility with CDNs than pure Rust clients
- **Monolith**: Best-in-class HTML cleaning
- **htmd**: Fast, accurate MD conversion

## Deployment Considerations

### Binary Size
- Release builds with optimization
- Strip symbols for smaller size
- Consider static linking trade-offs

### Platform Support
- Native binaries for major platforms
- Cross-compilation setup
- CI/CD for automated builds

### Configuration
- Environment variables for runtime config
- No configuration files needed
- Self-contained operation
</document_content>
</document>

<document index="8">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- **Git-tag-based semversioning**: Version is now automatically determined from git tags during build
- **Comprehensive test suite**: Added extensive unit tests, integration tests, and benchmark tests
- **Multi-platform CI/CD**: GitHub Actions now builds for Linux (x86_64, aarch64, musl), macOS (x86_64, aarch64), and Windows (x86_64)
- **Security audit**: Added automated security auditing with cargo-audit
- **Code coverage**: Integrated code coverage reporting with tarpaulin
- **Cross-compilation support**: Added support for cross-compilation to multiple architectures
- **Local build scripts**: Added convenient scripts for building, testing, and releasing
- **Installation script**: Added one-liner installation script for easy setup
- **Release automation**: Automated release process with GitHub Actions
- **Enhanced documentation**: Updated README with comprehensive installation options
- Comprehensive test suite (`issues/issuetest.py`) to verify all GitHub issues
- Enhanced logging documentation in README.md

### Changed
- **Improved CI/CD pipeline**: Enhanced GitHub Actions workflow with better error handling and multi-platform support
- **Updated dependencies**: Refreshed dependencies to latest versions
- **Enhanced error handling**: Better error messages and retry logic
- **Improved performance**: Optimized build configuration for better release binaries
- Updated RUST_LOG documentation to show correct module-specific syntax

### Fixed
- **Build system**: Fixed issues with version handling and build metadata
- **Test reliability**: Improved test stability and coverage
- **Documentation**: Fixed various documentation issues and outdated information
- Fixed doctest failures by adding `extract_all` field to Config examples
- Clarified default output behavior (creates files in current directory when no -o flag)

### Verified Issues (Complete Resolution)

#### Issue #104: Adobe CDN Timeout Issue âœ…
- **Fixed**: Removed forced HTTP/1.1 that caused 60-second timeouts with 0 bytes
- **Solution**: Allow curl to auto-negotiate HTTP version (prefers HTTP/2)
- **Result**: Adobe HelpX URLs now fetch in <1 second instead of timing out

#### Issue #105: Fix Help Option Not Working âœ…
- **Status**: Already working in current implementation
- **Verified**: `-h`, `--help`, `-V`, `--version` all display correctly
- **No changes needed**: Clap properly handles help/version display

#### Issue #106: Fix Output Writing Issues âœ…
- **Status**: All output modes functioning correctly
- **Verified Modes**:
  - Directory: `-o dir/` â†’ `dir/domain.com/path/file.md`
  - Single file: `-o file.md` â†’ single markdown file
  - Pack: `-p packed.md` â†’ combined content file
  - Default: No flags â†’ files in current directory

#### Issue #107: Smart HTML Content Extraction âœ…
- **Implemented**: `ContentExtractor` module with smart filtering logic
- **Added**: `-a/--all` flag to bypass extraction for full content
- **Status**: Framework complete, ready for HTML pipeline integration

#### Issue #108: Remove Panic Recovery Wrapper âœ…
- **Verified**: Application handles all error cases gracefully
- **Tested**: Malformed URLs, empty input, invalid files
- **Result**: Clean error messages without panics

#### Issue #109: Logging Framework Documentation âœ…
- **Added**: Comprehensive logging section to README.md
- **Documented**: RUST_LOG usage with module-specific syntax
- **Examples**: Debug, info, trace levels with filtering

#### Issue #110: Enhanced Testing Strategy âœ…
- **Achieved**: 78+ tests across unit and integration suites
- **Created**: Issue verification suite (`issues/issuetest.py`)
- **Coverage**: CLI, output modes, error handling, logging

## [1.5.0] - 2025-06-25

### Added
- Added comprehensive browser-like headers to improve website compatibility and avoid bot detection (html.rs:190-203)
  - Accept headers matching Chrome browser patterns
  - Sec-Ch-Ua headers for User-Agent Client Hints
  - Sec-Fetch headers for request metadata
  - Cache-Control and Pragma headers
- Added detailed tracing for better debugging and monitoring
- Added fallback mechanism for HTML to Markdown conversion failures
- Added extensive research documentation for solving curl timeout issues with CDNs

### Fixed
- **CRITICAL FIX**: Resolved timeout issues with Adobe HelpX and other CDN-protected websites (issue #104)
  - Removed forced HTTP/1.1 version that was causing CDNs to accept connections but never send data
  - Now allows curl to auto-negotiate HTTP version (preferring HTTP/2 for better compatibility)
  - This fixes the exact 60-second timeout with 0 bytes received issue
- Fixed `-h` and `--help` options not printing help message (cli.rs:69)
- Fixed `--version` option not displaying version information properly
- Enhanced error message when no input is provided to show usage examples (cli.rs:58-63)
- Removed unused import warning in url.rs

### Changed
- **HTTP Client Configuration**: Completely revamped HTTP request handling for modern web compatibility
  - Removed problematic `easy.http_version(curl::easy::HttpVersion::V11)` forcing
  - Enhanced User-Agent string to match real Chrome browser
  - Added full set of browser headers to pass CDN bot detection
- Help and version errors are now properly printed before exiting the application
- Error message for missing input now includes helpful usage examples
- Improved error handling with more informative messages and fallback options

### Technical Details
- The timeout issue was caused by forcing HTTP/1.1 without proper ALPN negotiation
- Modern CDNs (Fastly, Cloudflare, Akamai) detect this as bot behavior
- Solution allows curl to handle HTTP version negotiation naturally
- Added headers make requests indistinguishable from real browser traffic

## [1.4.2] - Previous release
</document_content>
</document>

<document index="9">
<source>CLAUDE.md</source>
<document_content>
# https://github.com/twardoch/twars-url2md

## 1. Project Overview

`twars-url2md` is a Rust CLI application that downloads URLs and converts them to clean, readable Markdown files. It can extract URLs from various text formats and process them concurrently.

## 2. Commands

### 2.1. Development
```bash
# Run tests
cargo test --all-features

# Format code
cargo fmt

# Run linter
cargo clippy --all-targets --all-features

# Build release binary
cargo build --release

# Run with arguments
cargo run -- [OPTIONS]
```

### 2.2. Publishing
```bash
# Verify package before publishing
cargo package

# Publish to crates.io
cargo publish
```

## 3. Architecture

The codebase follows a modular design with clear separation of concerns:

- `src/main.rs` - Entry point with panic handling wrapper
- `src/lib.rs` - Core processing logic and orchestration
- `src/cli.rs` - Command-line interface using Clap
- `src/url.rs` - URL extraction and validation logic
- `src/html.rs` - HTML fetching and cleaning using Monolith
- `src/markdown.rs` - HTML to Markdown conversion using htmd

### 3.1. Key Design Decisions

1. **Panic Recovery**: The application catches panics from the Monolith library to prevent crashes during HTML processing (see `src/main.rs:10-20`).

2. **Concurrent Processing**: Uses Tokio with adaptive concurrency based on available CPUs for parallel URL processing (see `src/lib.rs:107-120`).

3. **Error Handling**: Uses `anyhow` for error propagation with custom error types and retry logic for network failures.

4. **Output Organization**: Generates file paths based on URL structure when using `-o` option, creating a repository-like structure.

## 4. Testing

Run tests with `cargo test --all-features`. The test module is in `src/tests.rs` and includes unit tests for URL extraction and processing logic.

## 5. Dependencies

- **Async Runtime**: tokio with full features
- **HTTP Client**: reqwest with vendored OpenSSL
- **HTML Processing**: monolith for cleaning, htmd for Markdown conversion
- **CLI**: clap with derive feature
- **Utilities**: indicatif (progress bars), rayon (parallelism)

## 6. Active Development

See `TODO.md` for the modernization plan, which includes:
- Migration from OpenSSL to rustls
- Integration of structured logging with tracing
- Enhanced error reporting
- Performance optimizations

When implementing new features, ensure compatibility with the async architecture and maintain the modular structure.

# Working principles for software development

## 7. When you write code (in any language)

- Iterate gradually, avoiding major changes 
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code youâ€™re writing with the rest of the code. 
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line 
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions 
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase:
  - README.md (purpose and functionality) 
  - CHANGELOG.md (past changes)
  - TODO.md (future goals)
  - PROGRESS.md (detailed flat task list)

## 8. Use MCP tools if you can

Before and during coding (if have access to tools), you should: 

- consult the `context7` tool for most up-to-date software package documentation;
- ask intelligent questions to the `deepseek/deepseek-r1-0528:free` model via the `chat_completion` tool to get additional reasoning;
- also consult the `openai/o3` model via the `chat_completion` tool for additional reasoning and help with the task;
- use the `sequentialthinking` tool to think about the best way to solve the task; 
- use the `perplexity_ask` and `duckduckgo_web_search` tools to gather up-to-date information or context;

## 9. Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter. 

## 10. If you write Python

- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with 

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

After Python changes run:

```
uzpy run .; fd -e py -x autoflake {}; fd -e py -x pyupgrade --py312-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 11. Additional guidelines

Ask before extending/refactoring existing code in a way that may add complexity or break things. 

When youâ€™re finished, print "Wait, but" to go back, think & reflect, revise & improvement what youâ€™ve done (but donâ€™t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". 

## 12. Virtual team work

Be creative, diligent, critical, relentless & funny! Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## 13. Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The URL-to-markdown conversion system consists of three primary business components:

### 13.1. URL Processing and Content Extraction (Importance: 95)
- Collects URLs from multiple input sources (stdin, files)
- Validates and resolves URLs against base URLs when needed
- Implements a robust retry mechanism for handling transient network failures
- Processes URLs concurrently with dynamic CPU core adaptation
- Located in `src/cli.rs` and `src/url.rs`

### 13.2. HTML Processing Pipeline (Importance: 90)
- Fetches HTML content using Monolith for specialized content extraction
- Implements fallback mechanisms for non-HTML content
- Handles custom HTML processing with configurable options
- Transforms complex HTML structures into clean markdown
- Located in `src/html.rs`

### 13.3. Content Organization and Output Management (Importance: 85)
- Creates structured output paths mirroring URL hierarchies
- Supports packing mode for combining multiple URLs into single documents
- Maintains original URL ordering in packed output
- Handles both remote URLs and local file paths consistently
- Located in `src/lib.rs` and `src/markdown.rs`

### 13.4. Integration Points

The system connects these components through:

1. URL Collection -> HTML Processing
- URLs are validated and normalized before processing
- Concurrent processing queue manages HTML fetching
- Retry logic handles failed attempts

2. HTML Processing -> Content Organization
- Processed HTML is transformed to markdown
- Output paths are generated based on URL structure
- Content is either packed or distributed based on mode

The build system embeds version and build metadata for traceability, ensuring each deployment can be traced to its source state.

To read the full codebase, run `npx repomix -o llms.txt .` and then read `llms.txt`

$END$



If you work with Python, use 'uv pip' instead of 'pip', and use 'uvx hatch test' instead of 'python -m pytest'. 

When I say /report, you must: Read all `./TODO.md` and `./PLAN.md` files and analyze recent changes. Document all changes in `./CHANGELOG.md`. From `./TODO.md` and `./PLAN.md` remove things that are done. Make sure that `./PLAN.md` contains a detailed, clear plan that discusses specifics, while `./TODO.md` is its flat simplified itemized `- [ ]`-prefixed representation. When I say /work, you must work in iterations like so: Read all `./TODO.md` and `./PLAN.md` files and reflect. Work on the tasks. Think, contemplate, research, reflect, refine, revise. Be careful, curious, vigilant, energetic. Verify your changes. Think aloud. Consult, research, reflect. Then update `./PLAN.md` and `./TODO.md` with tasks that will lead to improving the work youâ€™ve just done. Then '/report', and then iterate again.
</document_content>
</document>

<document index="10">
<source>CONTRIBUTING.md</source>
<document_content>
# Contributing to twars-url2md

Thank you for your interest in contributing to twars-url2md! This document provides guidelines and instructions for contributing to the project.

## Table of Contents

- [Code of Conduct](#code-of-conduct)
- [Getting Started](#getting-started)
- [Development Setup](#development-setup)
- [Making Contributions](#making-contributions)
- [Coding Standards](#coding-standards)
- [Testing](#testing)
- [Documentation](#documentation)
- [Submitting Changes](#submitting-changes)
- [Release Process](#release-process)

## Code of Conduct

By participating in this project, you agree to abide by our Code of Conduct:

- Be respectful and inclusive
- Welcome newcomers and help them get started
- Focus on constructive criticism
- Accept feedback gracefully
- Prioritize the community's best interests

## Getting Started

1. **Fork the repository** on GitHub
2. **Clone your fork** locally:
   ```bash
   git clone https://github.com/YOUR_USERNAME/twars-url2md.git
   cd twars-url2md
   ```
3. **Add upstream remote**:
   ```bash
   git remote add upstream https://github.com/twardoch/twars-url2md.git
   ```

## Development Setup

### Prerequisites

- Rust 1.70.0 or later
- Cargo
- Git

### Initial Setup

```bash
# Install Rust (if not already installed)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Clone and setup
git clone https://github.com/twardoch/twars-url2md.git
cd twars-url2md

# Build the project
cargo build

# Run tests
cargo test

# Run the tool
cargo run -- --help
```

### Recommended Tools

- **rustfmt**: Code formatting
  ```bash
  rustup component add rustfmt
  ```
- **clippy**: Rust linter
  ```bash
  rustup component add clippy
  ```
- **cargo-edit**: Managing dependencies
  ```bash
  cargo install cargo-edit
  ```

## Making Contributions

### Types of Contributions

We welcome various types of contributions:

1. **Bug Fixes**: Fix issues reported in GitHub Issues
2. **Features**: Add new functionality (discuss major features first)
3. **Documentation**: Improve docs, add examples, fix typos
4. **Tests**: Increase test coverage
5. **Performance**: Optimize code for better performance
6. **Refactoring**: Improve code structure and maintainability

### Contribution Workflow

1. **Create an issue** (if one doesn't exist) describing what you plan to work on
2. **Create a feature branch**:
   ```bash
   git checkout -b feature/your-feature-name
   ```
3. **Make your changes** following our coding standards
4. **Write/update tests** for your changes
5. **Update documentation** if needed
6. **Commit your changes** with clear commit messages
7. **Push to your fork** and submit a pull request

### Issue Resolution Process

When working on reported issues:

1. **Analyze the Issue**:
   - Read the issue description carefully
   - Reproduce the problem locally
   - Document your findings

2. **Create Verification Tests**:
   - Add tests to `issues/issuetest.py` for the specific issue
   - Ensure tests fail before implementing the fix
   - Tests should cover edge cases

3. **Implement the Fix**:
   - Make minimal changes to resolve the issue
   - Follow existing code patterns
   - Add debug logging if helpful

4. **Verify Resolution**:
   ```bash
   # Run the issue verification suite
   python3 issues/issuetest.py
   
   # Run all Rust tests
   cargo test --all-features
   ```

5. **Document the Fix**:
   - Update CHANGELOG.md with detailed resolution notes
   - Move the issue file to `issues/resolved/`
   - Update any affected documentation

6. **Example Issue Resolution**:
   ```python
   # In issues/issuetest.py
   def test_issue_104():
       """Test Issue #104: Adobe CDN timeout fix"""
       result = subprocess.run(
           ['cargo', 'run', '--', '--stdin'],
           input='https://helpx.adobe.com/pdf/illustrator_reference.pdf\n',
           capture_output=True,
           text=True,
           timeout=10  # Should complete quickly now
       )
       assert result.returncode == 0
       assert 'illustrator_reference.md' in result.stdout
   ```

### Commit Message Guidelines

Follow conventional commit format:

```
type(scope): subject

body

footer
```

Types:
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation changes
- `style`: Code style changes (formatting, etc.)
- `refactor`: Code refactoring
- `test`: Test additions or changes
- `chore`: Build process or auxiliary tool changes

Example:
```
feat(url): add support for custom URL patterns

- Add regex-based URL extraction
- Support configurable patterns
- Update documentation

Closes #123
```

## Coding Standards

### Rust Style Guide

1. **Format code** with rustfmt:
   ```bash
   cargo fmt
   ```

2. **Check with clippy**:
   ```bash
   cargo clippy --all-targets --all-features
   ```

3. **Naming conventions**:
   - Use `snake_case` for functions and variables
   - Use `CamelCase` for types and traits
   - Use `SCREAMING_SNAKE_CASE` for constants

4. **Error handling**:
   - Use `Result<T, E>` for fallible operations
   - Provide context with `anyhow::Context`
   - Avoid `unwrap()` except in tests

5. **Documentation**:
   - Document all public APIs
   - Include examples in doc comments
   - Use `///` for public items, `//` for internal comments

### Code Organization

```
src/
â”œâ”€â”€ lib.rs          # Library root with public API
â”œâ”€â”€ main.rs         # CLI binary entry point
â”œâ”€â”€ cli.rs          # CLI argument parsing
â”œâ”€â”€ url.rs          # URL extraction and validation
â”œâ”€â”€ html.rs         # HTML fetching and processing
â””â”€â”€ markdown.rs     # Markdown conversion

tests/
â”œâ”€â”€ unit/           # Unit tests
â”œâ”€â”€ integration/    # Integration tests
â””â”€â”€ fixtures/       # Test data
```

## Testing

### Running Tests

```bash
# Run all tests
cargo test

# Run with all features
cargo test --all-features

# Run specific test
cargo test test_name

# Run with output
cargo test -- --nocapture

# Run benchmarks
cargo bench

# Run issue verification tests
python3 issues/issuetest.py
```

### Writing Tests

1. **Unit tests**: Place in `#[cfg(test)]` modules in source files
2. **Integration tests**: Place in `tests/` directory
3. **Issue verification tests**: Add to `issues/issuetest.py`
4. **Test coverage**: Aim for >80% coverage
5. **Test naming**: Use descriptive names like `test_extract_urls_from_html_with_base_url`

Example test:
```rust
#[test]
fn test_url_extraction() {
    let text = "Visit https://example.com";
    let urls = extract_urls_from_text(text, None);
    assert_eq!(urls.len(), 1);
    assert_eq!(urls[0], "https://example.com");
}
```

## Documentation

### Code Documentation

1. **Module docs**: Add module-level documentation
   ```rust
   //! This module handles URL extraction and validation.
   //!
   //! It provides functions to extract URLs from various text formats
   //! and validate them according to configurable rules.
   ```

2. **Function docs**: Document all public functions
   ```rust
   /// Extract URLs from text content.
   ///
   /// # Arguments
   ///
   /// * `text` - The text to extract URLs from
   /// * `base_url` - Optional base URL for resolving relative URLs
   ///
   /// # Examples
   ///
   /// ```
   /// let urls = extract_urls_from_text("Visit https://example.com", None);
   /// assert_eq!(urls.len(), 1);
   /// ```
   pub fn extract_urls_from_text(text: &str, base_url: Option<&str>) -> Vec<String> {
   ```

3. **Generate docs**:
   ```bash
   cargo doc --no-deps --open
   ```

### User Documentation

- Update README.md for user-facing changes
- Add examples for new features
- Update CLI help text in clap attributes

## Submitting Changes

### Pull Request Process

1. **Update your branch** with latest upstream changes:
   ```bash
   git fetch upstream
   git rebase upstream/main
   ```

2. **Run quality checks**:
   ```bash
   cargo fmt
   cargo clippy --all-targets --all-features
   cargo test
   cargo doc --no-deps
   ```

3. **Push to your fork**:
   ```bash
   git push origin feature/your-feature-name
   ```

4. **Create Pull Request** on GitHub with:
   - Clear title describing the change
   - Description of what changed and why
   - Reference to related issues
   - Screenshots/examples if applicable

### Review Process

1. Maintainers will review your PR
2. Address any feedback
3. Once approved, maintainers will merge your PR

## Release Process

Releases are managed by maintainers:

1. **Version bump** in Cargo.toml
2. **Update CHANGELOG.md**
3. **Create git tag**:
   ```bash
   git tag -a v1.2.3 -m "Release v1.2.3"
   git push origin v1.2.3
   ```
4. **GitHub Actions** automatically:
   - Runs tests
   - Creates GitHub release
   - Builds binaries
   - Publishes to crates.io

## Getting Help

- **Questions**: Open a GitHub Discussion
- **Bugs**: Open a GitHub Issue with reproduction steps
- **Security**: Email security concerns privately
- **Issue Status**: Check `issues/resolved/` for previously fixed issues

## Recognition

Contributors are recognized in:
- GitHub contributors page
- Release notes
- Project documentation

Thank you for contributing to twars-url2md!
</document_content>
</document>

<document index="11">
<source>Cargo.toml</source>
<document_content>
[package]
name = "twars-url2md"
version = "0.0.0"
edition = "2021"
authors = ["Adam Twardoch <adam+github@twardoch.com>"]
description = "A powerful CLI tool that fetches web pages and converts them to clean Markdown format using Monolith for content extraction and htmd for conversion"
documentation = "https://github.com/twardoch/twars-url2md"
homepage = "https://github.com/twardoch/twars-url2md"
repository = "https://github.com/twardoch/twars-url2md"
license = "MIT"
readme = "README.md"
keywords = ["markdown", "html", "converter", "web", "cli"]
categories = ["command-line-utilities", "text-processing", "web-programming"]
rust-version = "1.70.0"
build = "build.rs"


[package.metadata]
msrv = "1.70.0"


[badges.maintenance]
status = "actively-developed"


[dependencies]
base64 = "0.22"
cssparser = "0.34"
encoding_rs = "0.8"
linkify = "0.10"
num_cpus = "1.16"
sha2 = "0.10"
rayon = "1.8"


[dependencies.markup5ever]
version = "0.16"
features = []


[dependencies.markup5ever_rcdom]
version = "0.2"
features = []


[dependencies.anyhow]
version = "^1.0"
features = []


[dependencies.clap]
version = "4.5"
features = ["derive"]


[dependencies.futures]
version = "0.3.30"
features = []


[dependencies.html5ever]
version = "0.26"
features = []


[dependencies.htmd]
version = "0.1"
features = []


[dependencies.indicatif]
version = "0.17"
features = []

[dependencies.tracing]
version = "0.1"
features = ["log"]

[dependencies.tracing-subscriber]
version = "0.3"
features = ["env-filter", "fmt"]


[dependencies.monolith]
version = "^2.10"
features = []


[dependencies.regex]
version = "1.10"
default-features = false
features = ["std", "perf-dfa", "unicode-perl"]


[dependencies.tokio]
version = "1.36"
features = ["full"]


[dependencies.curl]
version = "0.4"


[dependencies.url]
version = "2.5"
features = []


[dependencies.scraper]
version = "0.18"
features = []


[dependencies.html2text]
version = "0.12"
features = []


[dev-dependencies]
tempfile = "3.10"
mockito = "1.2"


[build-dependencies.built]
version = "0.7"
features = ["chrono"]


[profile.release]
lto = true
codegen-units = 1
panic = "unwind"
strip = true
opt-level = 3


[profile.dev]
opt-level = 0
debug = true


[[bin]]
name = "twars-url2md"
path = "src/main.rs"

</document_content>
</document>

<document index="12">
<source>DEVELOPMENT.md</source>
<document_content>
# Development Guide

This guide provides information for developers who want to contribute to or work with the `twars-url2md` codebase.

## Table of Contents

- [Development Environment Setup](#development-environment-setup)
- [Project Structure](#project-structure)
- [Building and Testing](#building-and-testing)
- [Scripts](#scripts)
- [Release Process](#release-process)
- [CI/CD Pipeline](#cicd-pipeline)
- [Contributing](#contributing)

## Development Environment Setup

### Prerequisites

- **Rust**: Version 1.70.0 or later
- **Git**: For version control
- **curl**: For HTTP requests (used by dependencies)

### Getting Started

1. **Clone the repository:**
   ```bash
   git clone https://github.com/twardoch/twars-url2md.git
   cd twars-url2md
   ```

2. **Install dependencies:**
   ```bash
   cargo build
   ```

3. **Run tests:**
   ```bash
   cargo test
   ```

## Project Structure

```
twars-url2md/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs         # Entry point and panic handling
â”‚   â”œâ”€â”€ lib.rs          # Core processing logic
â”‚   â”œâ”€â”€ cli.rs          # Command-line interface
â”‚   â”œâ”€â”€ url.rs          # URL extraction and validation
â”‚   â”œâ”€â”€ html.rs         # HTML fetching and processing
â”‚   â”œâ”€â”€ markdown.rs     # Markdown conversion
â”‚   â””â”€â”€ tests.rs        # Unit tests
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/           # Unit tests
â”‚   â”œâ”€â”€ integration/    # Integration tests
â”‚   â””â”€â”€ benchmarks.rs   # Performance benchmarks
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build.sh        # Build script
â”‚   â”œâ”€â”€ test.sh         # Test script
â”‚   â””â”€â”€ release.sh      # Release script
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml      # GitHub Actions CI/CD
â”œâ”€â”€ build.rs            # Build script for version handling
â”œâ”€â”€ install.sh          # Installation script
â”œâ”€â”€ Cargo.toml          # Dependencies and metadata
â”œâ”€â”€ README.md           # User documentation
â”œâ”€â”€ CHANGELOG.md        # Version history
â””â”€â”€ DEVELOPMENT.md      # This file
```

## Building and Testing

### Quick Development Commands

```bash
# Build debug version
cargo build

# Build release version
cargo build --release

# Run all tests
cargo test

# Run tests with output
cargo test -- --nocapture

# Run specific test
cargo test test_extract_urls_from_text

# Run clippy linter
cargo clippy --all-targets --all-features

# Format code
cargo fmt

# Check formatting
cargo fmt --check
```

### Using the Build Scripts

We provide convenient scripts for common development tasks:

```bash
# Build the project
./scripts/build.sh

# Run comprehensive tests
./scripts/test.sh

# Run only unit tests
./scripts/test.sh --unit-only

# Run with benchmarks
./scripts/test.sh --benchmark

# Prepare a release (dry run)
./scripts/release.sh --version 1.5.0 --dry-run
```

## Scripts

### build.sh

Builds the project and extracts version information from git tags.

**Usage:**
```bash
./scripts/build.sh
```

**Features:**
- Cleans previous builds
- Extracts git version information
- Builds optimized release binary
- Copies binary to `builds/` directory with version info

### test.sh

Runs the complete test suite with various options.

**Usage:**
```bash
./scripts/test.sh [OPTIONS]
```

**Options:**
- `--unit-only`: Run only unit tests
- `--integration-only`: Run only integration tests  
- `--benchmark`: Run benchmark tests
- `--no-clippy`: Skip clippy linting
- `--no-format`: Skip format checking
- `--verbose`: Enable verbose output

### release.sh

Handles the complete release process including git tagging.

**Usage:**
```bash
./scripts/release.sh --version 1.5.0 [OPTIONS]
```

**Options:**
- `--version VERSION`: Version to release (required)
- `--dry-run`: Show what would be done without making changes
- `--skip-tests`: Skip running tests
- `--skip-build`: Skip building the project
- `--force`: Force release even if working directory is dirty

## Release Process

### Semantic Versioning

We follow [Semantic Versioning (semver)](https://semver.org/):

- **Major** (X.0.0): Breaking changes
- **Minor** (0.X.0): New features, backwards compatible
- **Patch** (0.0.X): Bug fixes, backwards compatible

### Version Management

Versions are automatically determined from git tags:

1. **Tagged releases**: Version comes from git tag (e.g., `v1.5.0` â†’ `1.5.0`)
2. **Development builds**: Version includes commit info (e.g., `1.4.2-dev.5.g1234567`)
3. **Dirty builds**: Version includes `-dirty` suffix

### Creating a Release

1. **Update CHANGELOG.md** with new version information
2. **Run tests** to ensure everything works:
   ```bash
   ./scripts/test.sh
   ```
3. **Create release** (dry run first):
   ```bash
   ./scripts/release.sh --version 1.5.0 --dry-run
   ./scripts/release.sh --version 1.5.0
   ```
4. **Push to GitHub** (done automatically by script)
5. **Monitor CI/CD** pipeline at [GitHub Actions](https://github.com/twardoch/twars-url2md/actions)

## CI/CD Pipeline

### GitHub Actions Workflow

Our CI/CD pipeline (`.github/workflows/ci.yml`) includes:

1. **Test Suite**: Runs on Linux, macOS, and Windows with stable and beta Rust
2. **Security Audit**: Automated vulnerability scanning with `cargo-audit`
3. **Code Coverage**: Coverage reporting with `tarpaulin`
4. **Multi-platform Builds**: Creates binaries for:
   - Linux: x86_64, aarch64, musl
   - macOS: x86_64, aarch64
   - Windows: x86_64
5. **Release Automation**: Publishes to GitHub Releases and crates.io

### Pipeline Triggers

- **Pull Requests**: Run tests and security audit
- **Main Branch**: Run full test suite
- **Tags** (v*): Run tests, build releases, and publish

### Secrets Configuration

The following secrets must be configured in GitHub:

- `CRATES_IO_TOKEN`: For publishing to crates.io
- `GITHUB_TOKEN`: Automatically provided by GitHub

## Contributing

### Code Style

- Follow Rust standard formatting (`cargo fmt`)
- Use `cargo clippy` to catch common issues
- Write comprehensive tests for new features
- Update documentation for user-facing changes

### Testing Guidelines

- **Unit tests**: Test individual functions and modules
- **Integration tests**: Test complete workflows
- **Benchmark tests**: Measure performance for critical paths
- **Error handling**: Test error conditions and edge cases

### Pull Request Process

1. **Fork** the repository
2. **Create** a feature branch
3. **Make** your changes with tests
4. **Run** the test suite: `./scripts/test.sh`
5. **Update** documentation if needed
6. **Submit** a pull request

### Development Best Practices

- Keep commits focused and atomic
- Write descriptive commit messages
- Test edge cases and error conditions
- Consider performance implications
- Document public APIs thoroughly
- Follow security best practices

## Architecture Notes

### Core Components

1. **URL Processing** (`url.rs`): Extract and validate URLs from various formats
2. **HTML Fetching** (`html.rs`): Robust HTTP client with retry logic
3. **Markdown Conversion** (`markdown.rs`): Clean HTML-to-Markdown conversion
4. **CLI Interface** (`cli.rs`): User-friendly command-line interface

### Design Principles

- **Robustness**: Handle network failures and malformed input gracefully
- **Performance**: Leverage async/await and concurrency for speed
- **Usability**: Provide clear error messages and helpful defaults
- **Modularity**: Keep components focused and testable

### Error Handling

- Use `anyhow` for error propagation with context
- Implement retry logic for transient failures
- Provide user-friendly error messages
- Log detailed information for debugging

## Support

For questions or issues:

1. **Check** the [README](README.md) for usage information
2. **Review** existing [GitHub Issues](https://github.com/twardoch/twars-url2md/issues)
3. **Create** a new issue for bugs or feature requests
4. **Join** discussions in the repository

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
</document_content>
</document>

<document index="13">
<source>GEMINI.md</source>
<document_content>
# https://github.com/twardoch/twars-url2md

## 1. Project Overview

`twars-url2md` is a Rust CLI application that downloads URLs and converts them to clean, readable Markdown files. It can extract URLs from various text formats and process them concurrently.

## 2. Commands

### 2.1. Development
```bash
# Run tests
cargo test --all-features

# Format code
cargo fmt

# Run linter
cargo clippy --all-targets --all-features

# Build release binary
cargo build --release

# Run with arguments
cargo run -- [OPTIONS]
```

### 2.2. Publishing
```bash
# Verify package before publishing
cargo package

# Publish to crates.io
cargo publish
```

## 3. Architecture

The codebase follows a modular design with clear separation of concerns:

- `src/main.rs` - Entry point with panic handling wrapper
- `src/lib.rs` - Core processing logic and orchestration
- `src/cli.rs` - Command-line interface using Clap
- `src/url.rs` - URL extraction and validation logic
- `src/html.rs` - HTML fetching and cleaning using Monolith
- `src/markdown.rs` - HTML to Markdown conversion using htmd

### 3.1. Key Design Decisions

1. **Panic Recovery**: The application catches panics from the Monolith library to prevent crashes during HTML processing (see `src/main.rs:10-20`).

2. **Concurrent Processing**: Uses Tokio with adaptive concurrency based on available CPUs for parallel URL processing (see `src/lib.rs:107-120`).

3. **Error Handling**: Uses `anyhow` for error propagation with custom error types and retry logic for network failures.

4. **Output Organization**: Generates file paths based on URL structure when using `-o` option, creating a repository-like structure.

## 4. Testing

Run tests with `cargo test --all-features`. The test module is in `src/tests.rs` and includes unit tests for URL extraction and processing logic.

## 5. Dependencies

- **Async Runtime**: tokio with full features
- **HTTP Client**: reqwest with vendored OpenSSL
- **HTML Processing**: monolith for cleaning, htmd for Markdown conversion
- **CLI**: clap with derive feature
- **Utilities**: indicatif (progress bars), rayon (parallelism)

## 6. Active Development

See `TODO.md` for the modernization plan, which includes:
- Migration from OpenSSL to rustls
- Integration of structured logging with tracing
- Enhanced error reporting
- Performance optimizations

When implementing new features, ensure compatibility with the async architecture and maintain the modular structure.

# Working principles for software development

## 7. When you write code (in any language)

- Iterate gradually, avoiding major changes 
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code youâ€™re writing with the rest of the code. 
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line 
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions 
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase:
  - README.md (purpose and functionality) 
  - CHANGELOG.md (past changes)
  - TODO.md (future goals)
  - PROGRESS.md (detailed flat task list)

## 8. Use MCP tools if you can

Before and during coding (if have access to tools), you should: 

- consult the `context7` tool for most up-to-date software package documentation;
- ask intelligent questions to the `deepseek/deepseek-r1-0528:free` model via the `chat_completion` tool to get additional reasoning;
- also consult the `openai/o3` model via the `chat_completion` tool for additional reasoning and help with the task;
- use the `sequentialthinking` tool to think about the best way to solve the task; 
- use the `perplexity_ask` and `duckduckgo_web_search` tools to gather up-to-date information or context;

## 9. Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter. 

## 10. If you write Python

- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with 

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

After Python changes run:

```
uzpy run .; fd -e py -x autoflake {}; fd -e py -x pyupgrade --py312-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 11. Additional guidelines

Ask before extending/refactoring existing code in a way that may add complexity or break things. 

When youâ€™re finished, print "Wait, but" to go back, think & reflect, revise & improvement what youâ€™ve done (but donâ€™t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". 

## 12. Virtual team work

Be creative, diligent, critical, relentless & funny! Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## 13. Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The URL-to-markdown conversion system consists of three primary business components:

### 13.1. URL Processing and Content Extraction (Importance: 95)
- Collects URLs from multiple input sources (stdin, files)
- Validates and resolves URLs against base URLs when needed
- Implements a robust retry mechanism for handling transient network failures
- Processes URLs concurrently with dynamic CPU core adaptation
- Located in `src/cli.rs` and `src/url.rs`

### 13.2. HTML Processing Pipeline (Importance: 90)
- Fetches HTML content using Monolith for specialized content extraction
- Implements fallback mechanisms for non-HTML content
- Handles custom HTML processing with configurable options
- Transforms complex HTML structures into clean markdown
- Located in `src/html.rs`

### 13.3. Content Organization and Output Management (Importance: 85)
- Creates structured output paths mirroring URL hierarchies
- Supports packing mode for combining multiple URLs into single documents
- Maintains original URL ordering in packed output
- Handles both remote URLs and local file paths consistently
- Located in `src/lib.rs` and `src/markdown.rs`

### 13.4. Integration Points

The system connects these components through:

1. URL Collection -> HTML Processing
- URLs are validated and normalized before processing
- Concurrent processing queue manages HTML fetching
- Retry logic handles failed attempts

2. HTML Processing -> Content Organization
- Processed HTML is transformed to markdown
- Output paths are generated based on URL structure
- Content is either packed or distributed based on mode

The build system embeds version and build metadata for traceability, ensuring each deployment can be traced to its source state.

To read the full codebase, run `npx repomix -o llms.txt .` and then read `llms.txt`

$END$



If you work with Python, use 'uv pip' instead of 'pip', and use 'uvx hatch test' instead of 'python -m pytest'. 

When I say /report, you must: Read all `./TODO.md` and `./PLAN.md` files and analyze recent changes. Document all changes in `./CHANGELOG.md`. From `./TODO.md` and `./PLAN.md` remove things that are done. Make sure that `./PLAN.md` contains a detailed, clear plan that discusses specifics, while `./TODO.md` is its flat simplified itemized `- [ ]`-prefixed representation. When I say /work, you must work in iterations like so: Read all `./TODO.md` and `./PLAN.md` files and reflect. Work on the tasks. Think, contemplate, research, reflect, refine, revise. Be careful, curious, vigilant, energetic. Verify your changes. Think aloud. Consult, research, reflect. Then update `./PLAN.md` and `./TODO.md` with tasks that will lead to improving the work youâ€™ve just done. Then '/report', and then iterate again.
</document_content>
</document>

<document index="14">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="15">
<source>PLAN.md</source>
<document_content>
# Code Streamlining Plan for twars-url2md v1.0 MVP

## Executive Summary

This plan outlines a comprehensive approach to streamline twars-url2md into a focused, performant v1.0 MVP. The primary goal is to eliminate redundancy, remove incomplete features, simplify overly complex code, and create a lean, maintainable codebase that excels at its core function: converting URLs to Markdown.

## Current State Analysis

### 1. Codebase Statistics
- Total source files: 7 Rust files (main.rs, lib.rs, cli.rs, url.rs, html.rs, markdown.rs, content_extractor.rs)
- Significant dead code and TODOs
- Excessive documentation and comments
- Redundant retry logic implementations
- Incomplete features not integrated into the pipeline

### 2. Major Issues Identified

#### 2.1 Incomplete Features
- **content_extractor.rs**: Completely unfinished module with TODO comments everywhere
  - Not integrated into HTML processing pipeline
  - Framework exists but no actual DOM manipulation implemented
  - Adds 487 lines of non-functional code
  - The `--all` flag exists in CLI but doesn't actually change behavior

#### 2.2 Code Duplication
- **Retry Logic**: Multiple implementations of retry logic
  - `process_url_with_retry` and `process_url_content_with_retry` in html.rs
  - Nearly identical code with minor variations
  - Could be consolidated into a single generic retry wrapper

- **URL Processing Functions**: 
  - `process_url_async` and `process_url_with_content` are nearly identical
  - Only difference is whether content is returned
  - Could be unified with a single function and option return

#### 2.3 Overly Complex Error Handling
- **Panic Recovery in main.rs**:
  - Complex panic hook and catch_unwind wrapper
  - Comments indicate "TODO: Re-evaluate this after error handling refinement"
  - The run() function already handles errors properly
  - Monolith panic handling is already isolated in html.rs

#### 2.4 Excessive Documentation
- **Verbose Comments**: Many comments are unnecessarily wordy
  - Example: 15-line comment blocks for simple 3-line functions
  - Excessive inline comments explaining obvious code
  - Redundant module-level documentation

- **Research Files**: Large research documents (4 files, ~1000 lines total) about already-fixed issues

#### 2.5 Dead Code and Unused Features
- **Unused Imports**: Several unused imports throughout
- **Dead Functions**: `fetch_html` function in html.rs marked with `#[allow(dead_code)]`
- **Commented Out Code**: Large blocks of commented code in tests
- **Unused Test Infrastructure**: Complex test setup for features that don't exist

#### 2.6 Configuration Complexity
- **Unused Config Fields**: `extract_all` field in Config struct serves no purpose
- **Verbose Logging**: Excessive debug logging throughout
- **Complex Progress Bar Setup**: Could be simplified

## Detailed Streamlining Plan

### Phase 1: Remove Incomplete Features

#### 1.1 Remove ContentExtractor Module Entirely
**Rationale**: This module is completely non-functional and adds complexity without value.

**Actions**:
1. Delete `src/content_extractor.rs` (487 lines)
2. Remove import from lib.rs: `mod content_extractor;`
3. Remove `extract_all` field from Config struct
4. Remove `--all` flag from CLI args
5. Update all Config instantiations to remove `extract_all`
6. Remove related tests

**Impact**: -500+ lines of dead code, cleaner API

#### 1.2 Remove Research Folder
**Rationale**: These files document an already-fixed issue and serve no purpose.

**Actions**:
1. Delete entire `research/` folder
2. Remove references from .cursorrules if any

**Impact**: -1000+ lines of documentation for fixed issues

### Phase 2: Consolidate Duplicate Code

#### 2.1 Unify Retry Logic
**Rationale**: Two nearly identical retry functions can be consolidated.

**Actions**:
1. Create a single generic retry wrapper:
```rust
async fn retry_operation<F, T, Fut>(
    operation: F,
    max_retries: u32,
    operation_name: &str,
) -> Result<T, anyhow::Error>
where
    F: Fn() -> Fut,
    Fut: Future<Output = Result<T, anyhow::Error>>,
```
2. Replace both retry functions with calls to the generic wrapper
3. Remove duplicate code

**Impact**: -100+ lines, more maintainable

#### 2.2 Unify URL Processing Functions
**Rationale**: Two functions doing essentially the same thing.

**Actions**:
1. Merge `process_url_async` and `process_url_with_content` into:
```rust
async fn process_url(url: &str, output_path: Option<PathBuf>) -> Result<Option<String>>
```
2. Return None when content isn't needed, Some(content) when it is
3. Update all callers

**Impact**: -50+ lines, cleaner API

### Phase 3: Simplify Error Handling

#### 3.1 Remove Panic Recovery Wrapper
**Rationale**: The application already handles errors gracefully.

**Actions**:
1. Remove custom panic hook from main.rs
2. Remove catch_unwind references
3. Simplify main() to just call run() and handle the Result
4. Keep panic handling only where needed (Monolith in html.rs)

**Impact**: -30+ lines, simpler error flow

#### 3.2 Simplify Monolith Error Handling
**Rationale**: Current implementation is overly defensive.

**Actions**:
1. Simplify the Monolith processing timeout logic
2. Remove verbose panic message construction
3. Use simple fallback without extensive logging

**Impact**: -20+ lines, cleaner code

### Phase 4: Reduce Verbosity

#### 4.1 Streamline Comments
**Rationale**: Excessive comments make code harder to read.

**Actions**:
1. Remove obvious inline comments like `// Pass by reference`
2. Reduce module documentation to essential information
3. Remove redundant function documentation
4. Keep only comments that explain "why", not "what"

**Impact**: -200+ lines of comments, more readable code

#### 4.2 Reduce Logging
**Rationale**: Too much debug logging clutters the code.

**Actions**:
1. Remove redundant debug logs
2. Keep only essential info/warn/error logs
3. Remove function entry/exit logging
4. Consolidate similar log messages

**Impact**: -50+ lines, cleaner execution flow

### Phase 5: Remove Dead Code

#### 5.1 Remove Unused Functions
**Actions**:
1. Delete `fetch_html` function (marked dead_code)
2. Remove unused test functions
3. Remove commented-out test code
4. Delete unused imports

**Impact**: -100+ lines

#### 5.2 Clean Up Test Suite
**Actions**:
1. Remove tests for non-existent features (content_extractor)
2. Consolidate redundant tests
3. Remove excessive test setup code
4. Focus on core functionality tests

**Impact**: -200+ lines

### Phase 6: Simplify Configuration

#### 6.1 Streamline Progress Bar
**Actions**:
1. Simplify progress bar initialization
2. Remove fallback style logic
3. Use default styling

**Impact**: -20 lines

#### 6.2 Simplify Build Configuration
**Actions**:
1. Remove unnecessary build.rs complexity
2. Simplify version information
3. Remove unused build-time variables

**Impact**: -10 lines

## Implementation Priority

### High Priority (Core Functionality)
1. Remove content_extractor.rs and all references
2. Consolidate retry logic
3. Unify URL processing functions
4. Remove panic recovery wrapper

### Medium Priority (Code Quality)
1. Streamline comments and documentation
2. Reduce verbose logging
3. Clean up test suite
4. Remove dead code

### Low Priority (Nice to Have)
1. Simplify progress bar
2. Minor refactoring of html.rs
3. Update documentation files

## Expected Outcomes

### Quantitative Improvements
- **Code Reduction**: ~2000 lines removed (30% reduction)
- **Complexity**: Cyclomatic complexity reduced by ~40%
- **Build Time**: Faster compilation due to less code
- **Binary Size**: Smaller executable

### Qualitative Improvements
- **Maintainability**: Easier to understand and modify
- **Focus**: Clear MVP functionality without distractions
- **Performance**: Less overhead from unused features
- **Reliability**: Simpler code means fewer bugs

## Success Metrics

1. All existing tests pass
2. Binary size reduced by at least 20%
3. Code coverage remains above 70%
4. No functionality regression
5. Improved startup and processing time

## Risks and Mitigation

### Risk 1: Removing Too Much
**Mitigation**: Keep all git history, can revert if needed

### Risk 2: Breaking Existing Functionality
**Mitigation**: Run full test suite after each major change

### Risk 3: User Confusion from Removed Features
**Mitigation**: Update documentation to reflect MVP scope

## Conclusion

This streamlining plan will transform twars-url2md from a feature-bloated work-in-progress into a focused, performant MVP that excels at its core mission. By removing ~2000 lines of non-functional code, consolidating duplicates, and simplifying complex patterns, we'll have a maintainable codebase ready for v1.0 release.

The key is ruthless prioritization: if it doesn't directly contribute to "download URL, convert to clean Markdown," it should be removed or deferred to post-1.0.
</document_content>
</document>

<document index="16">
<source>README.md</source>
<document_content>
# twars-url2md

[![Crates.io](https://img.shields.io/crates/v/twars-url2md)](https://crates.io/crates/twars-url2md)
[![Downloads](https://img.shields.io/crates/d/twars-url2md)](https://crates.io/crates/twars-url2md)
[![Documentation](https://docs.rs/twars-url2md/badge.svg)](https://docs.rs/twars-url2md)
[![GitHub Release](https://img.shields.io/github/v/release/twardoch/twars-url2md)](https://github.com/twardoch/twars-url2md/releases/latest)
![GitHub Release Date](https://img.shields.io/github/release-date/twardoch/twars-url2md)
![GitHub commits since latest release](https://img.shields.io/github/commits-since/twardoch/twars-url2md/latest)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![CI](https://github.com/twardoch/twars-url2md/actions/workflows/ci.yml/badge.svg)](https://github.com/twardoch/twars-url2md/actions)
[![Security audit](https://img.shields.io/badge/security-audit-green)](https://github.com/twardoch/twars-url2md/actions)
[![Multi-platform](https://img.shields.io/badge/platform-linux%20%7C%20macos%20%7C%20windows-blue)](https://github.com/twardoch/twars-url2md/releases)

**`twars-url2md`** is a fast and robust command-line tool and Rust library that fetches web pages, intelligently cleans up their HTML content, and converts them into clean, readable Markdown files. It's designed for high-performance batch processing, making it ideal for archiving, research, content conversion, or any task requiring structured text from web sources.

## Table of Contents

- [What it Does](#what-it-does)
- [Who It's For](#who-its-for)
- [Why It's Useful](#why-its-useful)
- [Installation](#installation)
  - [Pre-compiled Binaries (Recommended)](#pre-compiled-binaries-recommended)
  - [Install via Cargo](#install-via-cargo)
  - [Build from Source](#build-from-source)
- [Usage](#usage)
  - [Command-Line Interface (CLI)](#command-line-interface-cli)
  - [Library Usage](#library-usage)
- [Technical Details](#technical-details)
  - [Architecture Overview](#architecture-overview)
  - [Key Technical Features](#key-technical-features)
  - [HTTP Client Details & CDN Compatibility](#http-client-details--cdn-compatibility)
  - [Output Structure Options](#output-structure-options)
  - [Logging Framework](#logging-framework)
  - [Build Metadata](#build-metadata)
- [Development](#development)
  - [Prerequisites](#prerequisites)
  - [Getting Started](#getting-started)
  - [Building](#building)
  - [Code Quality & Formatting](#code-quality--formatting)
  - [Testing](#testing)
  - [Documentation](#documentation)
  - [Security Audit](#security-audit)
  - [Publishing (for maintainers)](#publishing-for-maintainers)
- [Contributing](#contributing)
  - [Areas for Contribution](#areas-for-contribution)
- [Troubleshooting](#troubleshooting)
  - [Common Issues & Solutions](#common-issues--solutions)
  - [Debugging with Verbose Logging](#debugging-with-verbose-logging)
- [License](#license)
- [Author](#author)
- [Acknowledgments](#acknowledgments)

## What it Does

`twars-url2md` takes one or more URLs (or local HTML files) as input. For each URL, it:

1.  **Fetches** the web page content, handling various network conditions and CDN behaviors.
2.  **Cleans** the HTML by removing scripts, styles, ads, and other unnecessary elements to isolate the core content, powered by [Monolith](https://github.com/Y2Z/monolith).
3.  **Converts** the cleaned HTML into well-formatted Markdown using [htmd](https://github.com/letmutex/htmd).
4.  **Saves** the Markdown to your local filesystem, either as individual files organized by URL structure or packed into a single file.

## Who It's For

This tool is valuable for:

*   **Developers:** Integrating web content conversion into applications or build processes.
*   **Researchers & Academics:** Archiving web pages for citation, analysis, or offline reading.
*   **Content Creators & Curators:** Converting articles or blog posts into Markdown for easier editing and republishing.
*   **Knowledge Workers:** Building personal knowledge bases from online resources.
*   **Anyone** needing to quickly and reliably transform web pages into a clean, portable text format.

## Why It's Useful

`twars-url2md` stands out due to its combination of speed, reliability, and the quality of its output:

*   ğŸš€ **High-Performance Processing**: Leverages asynchronous operations and adaptive concurrency (scaling with your CPU cores) for rapid batch processing of hundreds of URLs.
*   ğŸ“ **Clean & Readable Markdown**: Produces well-structured Markdown that accurately represents the original content's hierarchy.
*   ğŸ§¹ **Advanced HTML Cleaning**: Goes beyond simple conversion by first pruning irrelevant HTML elements, resulting in focused and clutter-free Markdown.
*   ğŸ”„ **Robust Error Handling**: Features automatic retries with exponential backoff for network issues and graceful recovery from HTML parsing errors, ensuring maximum success during large jobs.
*   ğŸ”— **Intelligent URL Handling**:
    *   Extracts URLs from plain text, HTML, and Markdown input.
    *   Resolves relative links if a base URL is provided.
    *   Supports processing local HTML files (`/path/to/file.html` or `file:///path/to/file.html`).
    *   Validates URLs and filters out non-HTTP(S) schemes.
*   ğŸ›¡ï¸ **CDN Compatibility**: Designed to work smoothly with modern CDNs (like Cloudflare, Fastly, Akamai), employing browser-like headers and HTTP/2 support to avoid bot detection and connection issues.
*   ğŸ“‚ **Flexible Input/Output**:
    *   Accepts URLs via command-line arguments, input files, or standard input (stdin).
    *   Organizes output into a logical directory structure mirroring the source URLs or creates a single `.md` file.
    *   Can "pack" content from multiple URLs into one consolidated Markdown file, with original URLs as headers.
*   ğŸ–¥ï¸ **Cross-Platform**: Works consistently on Windows, macOS, and Linux.
*   ğŸ”§ **Configurable**: Offers options for verbose logging, custom output paths, and retry limits.

## Installation

You can install `twars-url2md` using pre-compiled binaries (recommended for most users) or by building it from source using Cargo.

### Pre-compiled Binaries (Recommended)

#### Quick Installation (One-liner)

**Linux and macOS:**
```bash
curl -fsSL https://raw.githubusercontent.com/twardoch/twars-url2md/main/install.sh | bash
```

**Or with custom install directory:**
```bash
curl -fsSL https://raw.githubusercontent.com/twardoch/twars-url2md/main/install.sh | bash -s -- --install-dir ~/.local/bin
```

#### Manual Installation

Download the latest release for your platform from the [GitHub Releases page](https://github.com/twardoch/twars-url2md/releases/latest).

**macOS:**

```bash
# Intel x86_64
curl -L https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-macos-x86_64.tar.gz | tar xz
# Apple Silicon (M1/M2)
curl -L https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-macos-aarch64.tar.gz | tar xz
# Move to a directory in your PATH
sudo mv twars-url2md /usr/local/bin/
```

**Linux:**

```bash
# x86_64
curl -L https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-linux-x86_64.tar.gz | tar xz
# ARM64 (aarch64)
curl -L https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-linux-aarch64.tar.gz | tar xz
# Static binary (musl)
curl -L https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-linux-x86_64-musl.tar.gz | tar xz
# Move to a directory in your PATH
sudo mv twars-url2md /usr/local/bin/
```

**Windows:**

```powershell
# Download
Invoke-WebRequest -Uri https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-windows-x86_64.zip -OutFile twars-url2md.zip
# Extract
Expand-Archive twars-url2md.zip -DestinationPath .
# Move twars-url2md.exe to a directory in your system's PATH
# For example, move it to C:\Windows\System32 or add its current directory to PATH
```
*Note: For Windows, ensure the directory where you place `twars-url2md.exe` is included in your `PATH` environment variable to run it from any command prompt.*

### Install via Cargo

If you have Rust installed (version 1.70.0 or later), you can install `twars-url2md` directly from Crates.io:

```bash
cargo install twars-url2md
```

### Build from Source

To build `twars-url2md` from the source code:

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/twardoch/twars-url2md.git
    cd twars-url2md
    ```
2.  **Build in release mode:**
    ```bash
    cargo build --release
    ```
    The executable will be located at `target/release/twars-url2md`.
3.  **Install (optional):**
    You can install it to your Cargo binary path (`~/.cargo/bin/`) by running:
    ```bash
    cargo install --path .
    ```

After installation, verify it by running:
```bash
twars-url2md --version
```

## Usage

`twars-url2md` can be used as a command-line tool or as a library in your Rust projects.

### Command-Line Interface (CLI)

The basic syntax for the CLI is:

```bash
twars-url2md [OPTIONS] [URLS...]
```

If URLs are provided directly as arguments, they will be processed. Otherwise, use `--input` or `--stdin`.

**CLI Options:**

You can view all options by running `twars-url2md --help`. Here are the main ones:

*   `-i, --input <FILE>`: Input file containing URLs (one per line, or text with extractable URLs).
*   `-o, --output <PATH>`: Output directory for Markdown files. If `<PATH>` ends with `.md`, all content will be saved into this single file instead of a directory structure (unless `--pack` is also used).
*   `--stdin`: Read URLs from standard input.
*   `--base-url <URL>`: Base URL for resolving relative links found in the input content (e.g., if parsing URLs from an HTML page).
*   `-p, --pack <FILE.md>`: Pack all converted Markdown content into a single specified `.md` file. Each URL's content will be headed by its original URL.
*   `-v, --verbose`: Enable verbose output with detailed logging (INFO and DEBUG levels).
*   `-h, --help`: Print help information.
*   `-V, --version`: Print version information.

**Input Formats:**

The tool can extract URLs from various input sources when using `-i` or `--stdin`:

1.  **Plain URLs:** A list of URLs, one per line.
    ```
    https://example.com
    https://another-site.com/page
    ```
2.  **HTML Content:** Text containing HTML with links (e.g., `<a href="https://example.com">Example</a>`).
3.  **Markdown Content:** Text containing Markdown links (e.g., `[Example](https://example.com)`).
4.  **Local Files:** Paths to local HTML files.
    ```
    /path/to/your/file.html
    file:///absolute/path/to/another/file.html
    ```
    *Note: For local files, the content is read and converted to Markdown directly.*

**CLI Examples:**

1.  **Process a single URL, output to console (default if no -o or --pack):**
    ```bash
    twars-url2md https://www.rust-lang.org
    ```
    *(Note: This will print Markdown to stdout. For saving, use `-o` or `--pack`)*

2.  **Process multiple URLs, save to default directory structure (`./output/<domain>/...`):**
    ```bash
    twars-url2md https://www.rust-lang.org https://crates.io -o ./output
    ```

3.  **Process URLs from a file, save to a custom directory:**
    ```bash
    # urls.txt contains one URL per line
    twars-url2md -i urls.txt -o ./markdown_files
    ```

4.  **Process URLs from stdin, with verbose logging:**
    ```bash
    echo "https://example.com" | twars-url2md --stdin -o ./output -v
    ```

5.  **Extract URLs from a webpage and process them (using `curl` as an example source):**
    ```bash
    curl -s https://news.ycombinator.com | \
      twars-url2md --stdin --base-url https://news.ycombinator.com -o ./hn_articles
    ```

6.  **Process local HTML files (using `find` to supply file paths):**
    ```bash
    find . -name "*.html" | twars-url2md --stdin -o ./local_markdown
    ```

7.  **Create a single combined Markdown file from multiple URLs (`--pack`):**
    ```bash
    twars-url2md -i urls.txt --pack combined_report.md
    ```
    *Each URL's content in `combined_report.md` will be preceded by a header like `# https://example.com/some/page`.*

8.  **Output to a single `.md` file (alternative to directory structure):**
    ```bash
    # This is useful if you have one primary URL or want a simpler output than --pack
    twars-url2md https://example.com/main_article -o article.md
    ```
    *If multiple URLs are processed and output is a single file (not using `--pack`), their content will be concatenated.*

9.  **Use both individual file output and packed output:**
    ```bash
    twars-url2md -i urls.txt -o ./individual_files --pack all_content.md
    ```

### Library Usage

`twars-url2md` can also be used as a Rust library to integrate its functionality into your own projects.

Add it to your `Cargo.toml`:

```toml
[dependencies]
twars-url2md = "0.3.0" # Replace with the latest version from crates.io
tokio = { version = "1", features = ["full"] }
anyhow = "1"
```
*(Check [Crates.io](https://crates.io/crates/twars-url2md) for the most current version number.)*

**Example:**

```rust
use twars_url2md::{process_urls, Config, url::extract_urls_from_text};
use std::path::PathBuf;
use anyhow::Result;

#[tokio::main]
async fn main() -> Result<()> {
    // Example text from which to extract URLs
    let text_with_urls = "Check out https://www.rust-lang.org and also see https://crates.io for packages.";
    
    // Extract URLs. A base_url (Option<&str>) can be provided if needed.
    let urls_to_process = extract_urls_from_text(text_with_urls, None);

    if urls_to_process.is_empty() {
        println!("No URLs found to process.");
        return Ok(());
    }

    // Configure the processing task
    let config = Config {
        verbose: true,        // Enable detailed internal logging (uses `tracing` crate)
        max_retries: 3,       // Max *additional* retries after the initial attempt (so, 3 means up to 4 total attempts)
        output_base: PathBuf::from("./my_markdown_output"), // Base path for output
        single_file: false,   // False: create directory structure; True: if output_base is a file.md, save all to it
        has_output: true,     // True if output_base is a path to save to, false if just processing (e.g. for pack_file only)
        pack_file: Some(PathBuf::from("./packed_documentation.md")), // Optional: combine all into one .md file
    };

    // Ensure the output directory exists if not using pack_file primarily or single_file mode
     if config.has_output && !config.single_file && config.output_base.extension().is_none() {
        if !config.output_base.exists() {
            tokio::fs::create_dir_all(&config.output_base).await?;
        }
    } else if config.has_output && config.single_file { // Output is a single file
        if let Some(parent) = config.output_base.parent() { // Ensure parent directory for the single file exists
            if !parent.exists() {
                 tokio::fs::create_dir_all(parent).await?;
            }
        }
    }
    
    // Process the URLs
    // `process_urls` returns a Result containing a list of (String, anyhow::Error) for failed URLs.
    match process_urls(urls_to_process, config).await {
        Ok(errors) => {
            if errors.is_empty() {
                println!("All URLs processed successfully!");
            } else {
                eprintln!("Some URLs failed to process:");
                for (url, error) in errors {
                    eprintln!("- {}: {}", url, error);
                }
            }
        }
        Err(e) => {
            eprintln!("A critical error occurred during processing: {}", e);
        }
    }
    
    Ok(())
}
```

For more detailed information on the library API, please refer to the [official documentation on docs.rs](https://docs.rs/twars-url2md).

## Technical Details

This section provides a deeper dive into the architecture and inner workings of `twars-url2md`.

### Architecture Overview

`twars-url2md` is built with a modular design in Rust, emphasizing performance, concurrency, and resilience.

**Core Components:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI / Library  â”‚â”€â”€â”€â”€â–¶â”‚   URL Extractor   â”‚â”€â”€â”€â”€â–¶â”‚    HTTP Client   â”‚
â”‚  (Input Handler) â”‚     â”‚  (src/url.rs)     â”‚     â”‚  (src/html.rs)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                            â”‚
                                                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output Writer   â”‚â—€â”€â”€â”€â”€â”‚ Markdown Converterâ”‚â—€â”€â”€â”€â”€â”‚   HTML Cleaner   â”‚
â”‚ (File System)    â”‚     â”‚ (src/markdown.rs) â”‚     â”‚ (Monolith Lib)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1.  **CLI / Library Interface (`src/cli.rs`, `src/lib.rs`):**
    *   Handles command-line argument parsing (using `clap`) or library function calls.
    *   Manages input sources (files, stdin, direct arguments).
    *   Orchestrates the overall processing flow.

2.  **URL Extractor & Validator (`src/url.rs`):**
    *   Extracts URLs from various text formats (plain text, HTML, Markdown) using libraries like `linkify`.
    *   Validates, normalizes, and deduplicates URLs.
    *   Resolves relative URLs against a provided base URL.
    *   Generates structured output paths based on URL components.

3.  **HTTP Client (`src/html.rs`):**
    *   Fetches web content using a robust, `curl`-based HTTP client (`curl_rust` crate).
    *   Configured to mimic browser behavior to enhance compatibility with modern websites and CDNs (see "HTTP Client Details & CDN Compatibility" below).
    *   Implements retry logic with exponential backoff for transient network errors.

4.  **HTML Cleaner (Monolith Integration):**
    *   Utilizes the [Monolith library](https://github.com/Y2Z/monolith) to process and clean raw HTML.
    *   Strips scripts, styles, iframes, and other non-essential elements.
    *   Aims to isolate the main article or content body of the page.
    *   Includes panic recovery: if Monolith encounters an issue with severely malformed HTML, `twars-url2md` attempts a fallback to basic HTML processing or skips the URL, logging an error and allowing the batch job to continue.

5.  **Markdown Converter (`src/markdown.rs`):**
    *   Converts the cleaned HTML into Markdown using the [htmd library](https://github.com/letmutex/htmd).
    *   Preserves semantic structure (headings, lists, links, etc.).

6.  **Output Writer (`src/lib.rs`):**
    *   Manages writing the Markdown content to the filesystem.
    *   Supports creating a directory structure based on URL paths, outputting to a single specified file, or packing all content into one file.

### Key Technical Features

*   **Concurrent Processing:**
    *   Leverages Tokio's asynchronous runtime for non-blocking I/O.
    *   Processes multiple URLs concurrently using an adaptive worker pool. The number of concurrent workers is typically based on available CPU cores (e.g., `min(CPU_COUNT * 2, 16)`), optimizing throughput without overloading the system.
    *   Uses `futures::stream::StreamExt::buffer_unordered` for managing concurrent tasks.

*   **Robust Error Handling:**
    *   Uses the `anyhow` crate for flexible and context-rich error reporting.
    *   **Retry Mechanism:** For network-related issues during fetching, `twars-url2md` automatically retries. The `Config.max_retries` field (default `2` for CLI, configurable for library use) specifies the number of *additional* attempts after the first one. So, `max_retries: 2` means up to 3 total attempts. Retries use exponential backoff.
    *   **Panic Recovery:** Specifically for the Monolith HTML cleaning stage, if Monolith panics (e.g., due to extremely malformed HTML), the application catches the panic for that specific URL, logs an error, and attempts to fall back to a simpler HTML processing path or skips the URL, allowing the batch job to continue.

### HTTP Client Details & CDN Compatibility
The built-in HTTP client is carefully configured to maximize compatibility with various web servers and Content Delivery Networks (CDNs):
*   **Underlying Engine:** Uses `curl` via the `curl_rust` crate, known for its robustness and wide protocol support.
*   **User-Agent:** Sends a common browser User-Agent string (e.g., `Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36`) to appear like a standard browser. This is defined in `src/lib.rs` as `USER_AGENT_STRING`.
*   **HTTP/2 Support:** Auto-negotiates HTTP version, preferring HTTP/2 where available for better performance and compatibility with CDNs that might otherwise challenge or block older clients.
*   **Browser-like Headers:** Sends a comprehensive set of HTTP headers (e.g., `Accept`, `Accept-Language`, `Sec-Ch-Ua`, `Sec-Fetch-Site`, etc.) to further mimic legitimate browser traffic, reducing the likelihood of being blocked by bot detection systems.
*   **Timeouts:** Implements connection and total request timeouts (e.g., 20s for connection, 60s total by default in `curl_http_client`) to prevent indefinite hangs.

### Output Structure Options
*   **Directory Structure (default with `-o <directory>`):** Creates a hierarchy like `output_dir/example.com/path/to/page.md`.
*   **Single File (with `-o <filename>.md`):** Concatenates all Markdown content into the specified file. If multiple URLs are processed, their content is simply joined. This mode is active if the path given to `-o` ends with `.md` and is not a directory.
*   **Packed File (with `--pack <filename>.md`):** Combines Markdown from all URLs into a single file. Each URL's content is clearly demarcated by a heading like `# <URL>`. This mode also preserves the original input order of URLs in the output file.

### Logging Framework
*   Utilizes the `tracing` crate for structured and configurable logging.
*   Log levels (from most to least severe): `ERROR`, `WARN`, `INFO`, `DEBUG`, `TRACE`.
*   Verbosity is controlled by the `-v, --verbose` flag (enables INFO and DEBUG for `twars_url2md` modules by setting `RUST_LOG=info,twars_url2md=debug`).
*   Finer-grained control is available via the `RUST_LOG` environment variable (e.g., `RUST_LOG=twars_url2md=trace` for maximum detail from this application, or `RUST_LOG=info` for general info level). See `src/main.rs` for initialization logic.

### Build Metadata
*   The build process (via `build.rs`) embeds build time, target architecture, and profile (debug/release) into the binary. This information is accessible via the `twars-url2md --version` command (see `src/lib.rs` `version()` function).

## Development

This section provides guidance for setting up a development environment, building the project, and running tests.

### Prerequisites

*   **Rust:** Version 1.70.0 or later. You can install Rust via [rustup](https://rustup.rs/).
*   **Cargo:** The Rust package manager (installed with Rust).
*   **Git:** For cloning the repository.
*   **(Optional) Python 3:** For running the issue verification suite (`issues/issuetest.py`).

### Getting Started

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/twardoch/twars-url2md.git
    cd twars-url2md
    ```

### Building

*   **Debug build:**
    ```bash
    cargo build
    ```
    The executable will be in `target/debug/twars-url2md`.

*   **Release build (optimized):**
    ```bash
    cargo build --release
    ```
    The executable will be in `target/release/twars-url2md`.

*   **Run directly with arguments (debug mode):**
    ```bash
    cargo run -- -i urls.txt -o ./output
    ```

### Code Quality & Formatting

Consistent code style is maintained using `rustfmt`, and `clippy` is used for linting.

*   **Format code:**
    ```bash
    cargo fmt
    ```

*   **Run linter (Clippy):**
    ```bash
    cargo clippy --all-targets --all-features -- -D warnings
    ```
    *(The `-D warnings` flag promotes warnings to errors, ensuring high code quality.)*

### Testing

The project has a suite of unit and integration tests.

*   **Run all tests:**
    ```bash
    cargo test --all-features
    ```

*   **Run a specific test function:**
    ```bash
    # Example: cargo test test_url_extraction --all-features
    cargo test <TEST_FUNCTION_NAME> --all-features
    ```
    *(Replace `<TEST_FUNCTION_NAME>` with the actual test function name.)*

*   **Run tests with output (e.g., for debugging print statements):**
    ```bash
    cargo test --all-features -- --nocapture
    ```

*   **Run only integration tests:**
    Integration tests are typically located in the `tests/` directory (e.g., `tests/integration/e2e_tests.rs`).
    ```bash
    # Example: cargo test --test e2e_tests --all-features
    cargo test --test <INTEGRATION_TEST_FILENAME_WITHOUT_RS> --all-features
    ```

*   **Issue Verification Suite:**
    The project includes an issue verification script to test various CLI functionalities and confirm fixes for reported issues.
    ```bash
    python3 issues/issuetest.py
    ```
    *(Ensure you have Python 3 installed and any dependencies listed in or for that script.)*

### Documentation

*   **Generate and open local documentation:**
    ```bash
    cargo doc --no-deps --open
    ```

### Security Audit

*   **Check for known vulnerabilities in dependencies:**
    ```bash
    cargo audit
    ```
    *(You may need to install `cargo-audit` first: `cargo install cargo-audit`)*

### Publishing (for maintainers)

*   **Verify package before publishing:**
    ```bash
    cargo package
    ```
*   **Publish to Crates.io:**
    ```bash
    cargo publish
    ```

## Contributing

Contributions are welcome and greatly appreciated! Whether it's bug reports, feature suggestions, documentation improvements, or code contributions, your help makes `twars-url2md` better.

Please see the [**CONTRIBUTING.md**](CONTRIBUTING.md) file for detailed guidelines on how to contribute to the project, including information on reporting issues, submitting pull requests, and the code of conduct.

### Areas for Contribution

If you're looking for ways to contribute, here are some areas where help would be valuable:

*   **Additional Output Formats:** Implementing converters for formats like AsciiDoc, reStructuredText, or others.
*   **Performance Optimizations:** Identifying and improving bottlenecks in processing speed or memory usage.
*   **Enhanced Error Messages:** Making error messages even more user-friendly and actionable.
*   **Expanded Test Coverage:** Adding more unit, integration, or end-to-end tests, especially for edge cases.
*   **Documentation:** Improving the README, API documentation, or adding usage examples.
*   **New Features:** Proposing and implementing new functionalities that align with the tool's purpose.

Before starting significant work, it's a good idea to open an issue to discuss your proposed changes.

## Troubleshooting

If you encounter issues while using `twars-url2md`, this section may help.

### Common Issues & Solutions

*   **SSL/TLS Certificate Errors:**
    *   **Issue:** You might see errors related to SSL/TLS certificate validation, especially with sites using self-signed certificates or older TLS configurations.
    *   **Details:** `twars-url2md` uses `curl` which typically relies on the system's certificate store. Ensure your system's CA certificates are up-to-date. For specific problematic sites, this can be complex. The tool aims for secure defaults.
    *   **Action:** Check verbose output (`-v`) for more details on the TLS handshake. If it's a corporate environment with a custom CA, ensure that CA is trusted by your system.

*   **CDN-Protected Sites (e.g., Cloudflare, Akamai, Adobe):**
    *   **Issue:** Previously, some sites behind aggressive CDNs might have blocked requests or timed out.
    *   **Solution:** Recent versions of `twars-url2md` have significantly improved CDN compatibility by:
        *   Auto-negotiating HTTP/2, which is preferred by many CDNs.
        *   Sending a comprehensive set of browser-like HTTP headers (including User-Agent, Sec-CH-UA, etc.) to avoid simplistic bot detection.
        *   Using `curl` as the underlying HTTP client, which has a network stack more aligned with browsers.
    *   **Action:** Ensure you are using the latest version of `twars-url2md`. If you still encounter issues, verbose logging (`-v` or `RUST_LOG`) can provide clues.

*   **Timeouts on Large or Slow Pages:**
    *   **Issue:** The tool might time out when processing very large pages or pages from slow-responding servers.
    *   **Details:** Default timeouts are generally generous (e.g., 60 seconds for the entire request).
    *   **Action:** Check your network connection. If the page is exceptionally large or the server is consistently slow, this might be unavoidable with default settings. Verbose output can confirm if a timeout is the cause.

*   **Monolith Panics or Poor Conversion on Specific Pages:**
    *   **Issue:** The Monolith library (for HTML cleaning) or the `htmd` library (for Markdown conversion) might struggle with extremely complex, malformed, or unusual HTML structures.
    *   **Details:** `twars-url2md` includes panic recovery for Monolith. If Monolith panics, the tool logs an error and attempts to fall back to a more basic HTML processing step or skips the URL. This prevents the entire batch from failing.
    *   **Action:** If a specific page consistently fails to convert well, it might be due to its unique structure. You can report such pages as issues, providing the URL.

### Debugging with Verbose Logging

For more detailed insight into what the tool is doing, especially when troubleshooting, use verbose logging.

*   **Using the `-v` flag:**
    The simplest way to get more logs is to add the `-v` or `--verbose` flag to your command. This typically sets the log level to show `INFO` messages from all crates and `DEBUG` messages from `twars_url2md` itself (`RUST_LOG=info,twars_url2md=debug`).
    ```bash
    twars-url2md -i urls.txt -o output -v
    ```

*   **Using the `RUST_LOG` Environment Variable:**
    For more fine-grained control, you can use the `RUST_LOG` environment variable. `twars-url2md` uses the `tracing` library.

    **Syntax:** `RUST_LOG="target[span{field=value}]=level"` (simplified: `RUST_LOG="crate_name=level,another_crate=level"`)

    **Examples:**
    *   Enable `DEBUG` level for `twars_url2md` and `INFO` for everything else (similar to `-v`):
        ```bash
        RUST_LOG=info,twars_url2md=debug twars-url2md -i urls.txt -o output
        ```
    *   Enable `TRACE` level for `twars_url2md` (very detailed, for deep debugging):
        ```bash
        RUST_LOG=twars_url2md=trace twars-url2md -i urls.txt -o output
        ```
    *   Enable `DEBUG` for the HTML processing module specifically:
        ```bash
        RUST_LOG=twars_url2md::html=debug twars-url2md -i urls.txt -o output
        ```
    *   Show all `DEBUG` level messages from all crates:
        ```bash
        RUST_LOG=debug twars-url2md -i urls.txt -o output
        ```

    **Logging Levels (most to least verbose):**
    *   `TRACE`: Extremely detailed information, typically for fine-grained debugging.
    *   `DEBUG`: Detailed information useful for debugging.
    *   `INFO`: Informational messages about the progress of the application.
    *   `WARN`: Warnings about potential issues that don't stop execution.
    *   `ERROR`: Errors that prevent a specific operation (e.g., processing one URL) but don't crash the application.

If you consistently encounter an issue not covered here, consider opening an issue on the [GitHub repository](https://github.com/twardoch/twars-url2md/issues) with detailed information, including the command you ran, the output, and logs if possible.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Author

Adam Twardoch ([@twardoch](https://github.com/twardoch))

## Acknowledgments

`twars-url2md` builds upon the excellent work of others:

*   [Monolith](https://github.com/Y2Z/monolith) for robust HTML cleaning and resource embedding.
*   [htmd](https://github.com/letmutex/htmd) for efficient HTML-to-Markdown conversion.
*   The Rust community and the creators of the many high-quality crates used in this project.

---

For bug reports, feature requests, or questions, please open an issue on the [GitHub repository](https://github.com/twardoch/twars-url2md/issues).
</document_content>
</document>

<document index="17">
<source>TESTS.md</source>
<document_content>
# Comprehensive Test Plan for twars-url2md

## Overview

This document outlines the comprehensive test strategy for the `twars-url2md` project, a Rust CLI application that downloads URLs and converts them to clean, readable Markdown files. The test suite aims to ensure reliability, correctness, and robustness across all components.

## Test Structure

### 1. Unit Tests

#### 1.1 URL Module (`src/url.rs`)
- **URL Extraction**
  - Extract URLs from plain text
  - Extract URLs from Markdown links
  - Extract URLs from HTML href attributes
  - Handle mixed format inputs
  - Handle URLs with special characters and encodings
  - Test empty and malformed inputs

- **URL Validation**
  - Validate HTTP/HTTPS URLs
  - Reject invalid protocols
  - Handle relative URLs with base URL resolution
  - Test URL normalization (trailing slashes, fragments)
  - Handle internationalized domain names (IDN)

- **URL Processing**
  - Deduplicate URLs correctly
  - Preserve URL ordering when required
  - Handle query parameters and fragments appropriately

#### 1.2 HTML Module (`src/html.rs`)
- **HTML Fetching**
  - Mock HTTP requests for different status codes (200, 404, 500, etc.)
  - Test timeout handling
  - Test redirect following (301, 302)
  - Handle large HTML documents
  - Test content-type validation

- **Monolith Integration**
  - Test HTML cleaning with various configurations
  - Handle JavaScript-heavy pages
  - Test CSS inlining behavior
  - Verify image handling (base64 encoding vs. external)
  - Test iframe and embedded content handling

- **Error Recovery**
  - Test panic recovery from Monolith crashes
  - Verify fallback mechanisms for non-HTML content
  - Test retry logic for transient failures

#### 1.3 Markdown Module (`src/markdown.rs`)
- **HTML to Markdown Conversion**
  - Test basic HTML elements (p, div, span)
  - Test headings (h1-h6) conversion
  - Test list conversion (ul, ol, nested lists)
  - Test link preservation
  - Test image alt text and URLs
  - Test code blocks and inline code
  - Test table conversion
  - Test blockquote handling
  - Handle malformed HTML gracefully

- **Content Cleaning**
  - Remove script and style tags
  - Preserve semantic structure
  - Handle special characters and entities
  - Test Unicode handling

#### 1.4 CLI Module (`src/cli.rs`)
- **Argument Parsing**
  - Test all CLI flags and options
  - Validate mutually exclusive options
  - Test default values
  - Verify help text accuracy
  - Test argument validation and error messages

- **Input Handling**
  - Read from stdin
  - Read from files
  - Handle multiple input sources
  - Test glob pattern expansion
  - Handle non-existent files gracefully

#### 1.5 Library Module (`src/lib.rs`)
- **Orchestration Logic**
  - Test sequential vs. concurrent processing modes
  - Verify CPU core detection and adaptation
  - Test progress reporting
  - Verify output organization logic

- **Path Generation**
  - Test URL-to-filesystem path conversion
  - Handle special characters in paths
  - Test collision handling
  - Verify directory creation
  - Test both flat and hierarchical output modes

### 2. Integration Tests

#### 2.1 End-to-End Workflows
- **Single URL Processing**
  - Download and convert a simple HTML page
  - Process a complex web page with images and styles
  - Handle a non-HTML resource (PDF, image)
  - Test local file processing

- **Batch Processing**
  - Process multiple URLs from a file
  - Test concurrent processing limits
  - Verify output file organization
  - Test packed mode output

- **Error Scenarios**
  - Network timeouts
  - Invalid URLs
  - Server errors
  - Disk write failures
  - Insufficient permissions

#### 2.2 Monolith Integration
- Test full Monolith pipeline with real HTML
- Verify CSS and JavaScript handling
- Test resource embedding options
- Confirm panic recovery in real scenarios

### 3. Performance Tests

#### 3.1 Concurrency Testing
- **Load Testing**
  - Process 100+ URLs concurrently
  - Measure memory usage under load
  - Test connection pool limits
  - Verify rate limiting behavior

- **Resource Management**
  - Test file descriptor limits
  - Monitor thread pool usage
  - Verify cleanup of temporary resources

#### 3.2 Benchmarks
- Benchmark URL extraction performance
- Benchmark HTML to Markdown conversion
- Measure overhead of concurrent processing
- Profile memory allocation patterns

### 4. Property-Based Tests

Using `proptest` or similar:
- Generate random URL patterns
- Test with arbitrary HTML structures
- Fuzz input parsing
- Verify invariants hold under random inputs

### 5. Regression Tests

- Specific test cases for reported bugs
- Edge cases discovered in production
- Platform-specific issues (Windows paths, etc.)

## Test Data and Fixtures

### Mock Data
- Sample HTML files of varying complexity
- Mock HTTP responses for different scenarios
- Test URLs covering various patterns
- Malformed inputs for error testing

### Test Servers
- Mock HTTP server for integration tests
- Configurable response delays
- Error injection capabilities
- Redirect chain testing

## Testing Infrastructure

### Continuous Integration
- Run full test suite on PR
- Platform matrix (Linux, macOS, Windows)
- Rust version compatibility testing
- Dependency audit

### Code Coverage
- Target: 80% line coverage minimum
- 100% coverage for critical paths
- Coverage reports in CI

### Test Organization
```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ url_tests.rs
â”‚   â”œâ”€â”€ html_tests.rs
â”‚   â”œâ”€â”€ markdown_tests.rs
â”‚   â””â”€â”€ cli_tests.rs
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ e2e_tests.rs
â”‚   â”œâ”€â”€ monolith_tests.rs
â”‚   â””â”€â”€ concurrent_tests.rs
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ html/
â”‚   â”œâ”€â”€ urls/
â”‚   â””â”€â”€ expected/
â””â”€â”€ common/
    â”œâ”€â”€ mod.rs
    â””â”€â”€ helpers.rs
```

## Testing Commands

```bash
# Run all tests
cargo test --all-features

# Run unit tests only
cargo test --lib

# Run integration tests
cargo test --test '*'

# Run with coverage
cargo tarpaulin --out Html

# Run benchmarks
cargo bench

# Run specific test module
cargo test url::tests

# Run tests with logging
RUST_LOG=debug cargo test -- --nocapture
```

## Test Implementation Priority

1. **Phase 1: Core Functionality**
   - URL extraction and validation
   - Basic HTML to Markdown conversion
   - CLI argument parsing

2. **Phase 2: Integration**
   - End-to-end workflows
   - Monolith integration
   - Error handling

3. **Phase 3: Robustness**
   - Concurrent processing
   - Property-based tests
   - Performance benchmarks

4. **Phase 4: Polish**
   - Platform-specific tests
   - Regression test suite
   - Documentation tests

## Success Criteria

- All tests pass in CI
- No flaky tests
- Test execution < 60 seconds
- Clear test names and documentation
- Easy to add new test cases
- Comprehensive error scenario coverage
</document_content>
</document>

<document index="18">
<source>TODO.md</source>
<document_content>
# twars-url2md v1.0 MVP Streamlining Tasks

## ğŸ¯ Focus: Create a lean, performant tool that excels at URL-to-Markdown conversion

## ğŸ”¥ High Priority - Core Cleanup

### Remove Non-Functional Features
- [x] Delete src/content_extractor.rs entirely
- [x] Remove `mod content_extractor` from lib.rs
- [x] Remove `extract_all` field from Config struct
- [x] Remove `--all` flag from CLI
- [x] Update all Config instantiations
- [x] Delete content_extractor tests

### Consolidate Duplicate Code
- [x] Create generic retry wrapper function
- [x] Replace process_url_with_retry with generic version
- [ ] Replace process_url_content_with_retry with generic version (kept due to complexity)
- [x] Merge process_url_async and process_url_with_content
- [x] Update all function callers

### Simplify Error Handling
- [x] Remove panic hook from main.rs
- [x] Simplify main() function
- [x] Remove catch_unwind references
- [ ] Simplify Monolith panic handling in html.rs (kept for safety)

## ğŸ§¹ Medium Priority - Code Quality

### Remove Dead Code
- [x] Delete fetch_html function in html.rs
- [x] Remove unused imports throughout
- [x] Delete commented-out test code
- [x] Remove unused test infrastructure

### Streamline Documentation
- [ ] Remove obvious inline comments
- [ ] Reduce verbose function documentation
- [ ] Simplify module-level docs
- [ ] Keep only "why" comments, remove "what" comments

### Reduce Logging
- [ ] Remove redundant debug logs
- [ ] Remove function entry/exit logging
- [ ] Consolidate similar log messages
- [ ] Keep only essential info/warn/error logs

## ğŸ“ Low Priority - Final Cleanup

### Project Structure
- [x] Delete research/ folder
- [ ] Clean up test fixtures
- [ ] Remove obsolete issue files
- [ ] Update .gitignore

### Configuration Simplification
- [ ] Simplify progress bar setup
- [ ] Remove unnecessary build.rs complexity
- [ ] Streamline version information

### Documentation Updates
- [ ] Update README.md to reflect MVP scope
- [ ] Simplify CONTRIBUTING.md
- [ ] Update CHANGELOG.md
- [ ] Clean up code examples

## âœ… Success Criteria

- [ ] All existing tests pass
- [ ] Binary size reduced by 20%+
- [ ] ~2000 lines of code removed
- [ ] No functionality regression
- [ ] Faster build times

## ğŸš« Out of Scope for v1.0

- Smart content extraction
- Advanced HTML processing options
- Plugin system
- Authentication features
- JavaScript rendering

---

**Last updated: 2025-06-25**
</document_content>
</document>

# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/build.rs
# Language: rust



<document index="19">
<source>build.sh</source>
<document_content>
#!/bin/bash
# this_file: build.sh
# Build script for twars-url2md - A powerful CLI tool for converting web pages to Markdown

set -euo pipefail

npx repomix -o llms.txt .

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Function to check if command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Check prerequisites
check_prerequisites() {
    print_status "Checking prerequisites..."

    if ! command_exists cargo; then
        print_error "cargo is not installed. Please install Rust toolchain first."
        exit 1
    fi

    if ! command_exists rustc; then
        print_error "rustc is not installed. Please install Rust toolchain first."
        exit 1
    fi

    print_success "Prerequisites check passed"
}

# Function to show help
show_help() {
    echo "Usage: $0 [OPTIONS]"
    echo ""
    echo "Build script for twars-url2md"
    echo ""
    echo "OPTIONS:"
    echo "  -h, --help       Show this help message"
    echo "  -f, --format     Format code only"
    echo "  -l, --lint       Run linter only"
    echo "  -t, --test       Run tests only"
    echo "  -b, --build      Build release binary only"
    echo "  -d, --dev        Build development binary only"
    echo "  -c, --clean      Clean build artifacts"
    echo "  -p, --package    Package for publishing"
    echo "  -a, --all        Run all steps (format, lint, test, build) [default]"
    echo "  --skip-format    Skip formatting step"
    echo "  --skip-lint      Skip linting step"
    echo "  --skip-test      Skip testing step"
    echo ""
    echo "Examples:"
    echo "  $0                    # Run all steps"
    echo "  $0 --format          # Format code only"
    echo "  $0 --test            # Run tests only"
    echo "  $0 --all --skip-test # Run all except tests"
}

# Default values
FORMAT=false
LINT=false
TEST=false
BUILD=false
DEV_BUILD=false
CLEAN=false
PACKAGE=false
ALL=true
SKIP_FORMAT=false
SKIP_LINT=false
SKIP_TEST=false

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
    -h | --help)
        show_help
        exit 0
        ;;
    -f | --format)
        FORMAT=true
        ALL=false
        ;;
    -l | --lint)
        LINT=true
        ALL=false
        ;;
    -t | --test)
        TEST=true
        ALL=false
        ;;
    -b | --build)
        BUILD=true
        ALL=false
        ;;
    -d | --dev)
        DEV_BUILD=true
        ALL=false
        ;;
    -c | --clean)
        CLEAN=true
        ALL=false
        ;;
    -p | --package)
        PACKAGE=true
        ALL=false
        ;;
    -a | --all)
        ALL=true
        ;;
    --skip-format)
        SKIP_FORMAT=true
        ;;
    --skip-lint)
        SKIP_LINT=true
        ;;
    --skip-test)
        SKIP_TEST=true
        ;;
    *)
        print_error "Unknown option: $1"
        echo "Use --help for usage information"
        exit 1
        ;;
    esac
    shift
done

# Main execution
main() {
    print_status "Starting build process for twars-url2md..."
    check_prerequisites

    # Clean if requested
    if [[ $CLEAN == true ]]; then
        print_status "Cleaning build artifacts..."
        cargo clean
        print_success "Clean completed"
        return 0
    fi

    # Package if requested
    if [[ $PACKAGE == true ]]; then
        print_status "Packaging for publishing..."
        cargo package
        print_success "Package completed"
        return 0
    fi

    # Format code
    if [[ ($ALL == true && $SKIP_FORMAT == false) || $FORMAT == true ]]; then
        print_status "Formatting code..."
        if cargo fmt -- --check >/dev/null 2>&1; then
            print_success "Code is already formatted"
        else
            print_warning "Code needs formatting, applying..."
            cargo fmt
            print_success "Code formatting completed"
        fi
    fi

    # Run linter
    if [[ ($ALL == true && $SKIP_LINT == false) || $LINT == true ]]; then
        print_status "Running linter (clippy)..."
        cargo clippy --all-targets --all-features -- -D warnings
        print_success "Linting completed"
    fi

    # Run tests
    if [[ ($ALL == true && $SKIP_TEST == false) || $TEST == true ]]; then
        print_status "Running tests..."
        cargo test --all-features
        print_success "Tests completed"
    fi

    # Build development binary
    if [[ $DEV_BUILD == true ]]; then
        print_status "Building development binary..."
        cargo build
        print_success "Development build completed"
        print_status "Binary location: target/debug/twars-url2md"
    fi

    # Build release binary
    if [[ ($ALL == true) || $BUILD == true ]]; then
        print_status "Building release binary..."
        cargo build --release
        print_success "Release build completed"
        print_status "Binary location: target/release/twars-url2md"

        # Show binary info
        if [[ -f "target/release/twars-url2md" ]]; then
            BINARY_SIZE=$(du -h target/release/twars-url2md | cut -f1)
            print_status "Binary size: $BINARY_SIZE"
        fi
    fi

    print_success "Build process completed successfully!"
}

# Run main function
main

</document_content>
</document>

<document index="20">
<source>example.com/index.md</source>
<document_content>
Example Domain

body { background-color: #f0f0f2; margin: 0; padding: 0; font-family: -apple-system, system-ui, BlinkMacSystemFont, "Segoe UI", "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif; } div { width: 600px; margin: 5em auto; padding: 2em; background-color: #fdfdff; border-radius: 0.5em; box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02); } a:link, a:visited { color: #38488f; text-decoration: none; } @media (max-width: 700px) { div { margin: 0 auto; width: auto; } } 

# Example Domain

This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.

[More information...](https://www.iana.org/domains/example)
</document_content>
</document>

<document index="21">
<source>example.org/index.md</source>
<document_content>
Example Domain

body { background-color: #f0f0f2; margin: 0; padding: 0; font-family: -apple-system, system-ui, BlinkMacSystemFont, "Segoe UI", "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif; } div { width: 600px; margin: 5em auto; padding: 2em; background-color: #fdfdff; border-radius: 0.5em; box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02); } a:link, a:visited { color: #38488f; text-decoration: none; } @media (max-width: 700px) { div { margin: 0 auto; width: auto; } } 

# Example Domain

This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.

[More information...](https://www.iana.org/domains/example)
</document_content>
</document>

<document index="22">
<source>install.sh</source>
<document_content>
#!/bin/bash
# this_file: install.sh

# Installation script for twars-url2md
# This script automatically downloads and installs the latest release

set -euo pipefail

# Default values
INSTALL_DIR="${INSTALL_DIR:-/usr/local/bin}"
REPO="twardoch/twars-url2md"
GITHUB_API_URL="https://api.github.com"
FORCE=false
VERSION=""

# Color output functions
red() { echo -e "\033[31m$1\033[0m"; }
green() { echo -e "\033[32m$1\033[0m"; }
yellow() { echo -e "\033[33m$1\033[0m"; }
blue() { echo -e "\033[34m$1\033[0m"; }

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --install-dir)
            INSTALL_DIR="$2"
            shift 2
            ;;
        --version)
            VERSION="$2"
            shift 2
            ;;
        --force)
            FORCE=true
            shift
            ;;
        --help|-h)
            echo "twars-url2md installer"
            echo ""
            echo "Usage: $0 [OPTIONS]"
            echo ""
            echo "Options:"
            echo "  --install-dir DIR  Install directory (default: /usr/local/bin)"
            echo "  --version VERSION  Specific version to install (default: latest)"
            echo "  --force           Force installation even if already installed"
            echo "  --help, -h        Show this help message"
            echo ""
            echo "Examples:"
            echo "  $0"
            echo "  $0 --install-dir ~/.local/bin"
            echo "  $0 --version v1.4.2"
            echo "  curl -fsSL https://raw.githubusercontent.com/twardoch/twars-url2md/main/install.sh | bash"
            exit 0
            ;;
        *)
            red "âŒ Unknown option: $1"
            exit 1
            ;;
    esac
done

# Detect platform
detect_platform() {
    local os arch
    os="$(uname -s)"
    arch="$(uname -m)"
    
    case "$os" in
        Linux)
            case "$arch" in
                x86_64) echo "linux-x86_64" ;;
                aarch64|arm64) echo "linux-aarch64" ;;
                *) red "âŒ Unsupported architecture: $arch"; exit 1 ;;
            esac
            ;;
        Darwin)
            case "$arch" in
                x86_64) echo "macos-x86_64" ;;
                arm64) echo "macos-aarch64" ;;
                *) red "âŒ Unsupported architecture: $arch"; exit 1 ;;
            esac
            ;;
        MINGW*|MSYS*|CYGWIN*)
            case "$arch" in
                x86_64) echo "windows-x86_64" ;;
                *) red "âŒ Unsupported architecture: $arch"; exit 1 ;;
            esac
            ;;
        *)
            red "âŒ Unsupported operating system: $os"
            exit 1
            ;;
    esac
}

# Get latest release version
get_latest_version() {
    local version
    version=$(curl -fsSL "$GITHUB_API_URL/repos/$REPO/releases/latest" | grep '"tag_name"' | sed -E 's/.*"([^"]+)".*/\1/')
    if [ -z "$version" ]; then
        red "âŒ Failed to get latest version"
        exit 1
    fi
    echo "$version"
}

# Download and install binary
install_binary() {
    local version="$1"
    local platform="$2"
    local binary_name="twars-url2md"
    local archive_name="twars-url2md-$platform"
    local download_url extension
    
    # Determine file extension and download URL
    case "$platform" in
        windows-*)
            extension=".zip"
            binary_name="twars-url2md.exe"
            ;;
        *)
            extension=".tar.gz"
            ;;
    esac
    
    archive_name="$archive_name$extension"
    download_url="https://github.com/$REPO/releases/download/$version/$archive_name"
    
    echo "ğŸ“¥ Downloading $archive_name..."
    
    # Create temporary directory
    local temp_dir
    temp_dir=$(mktemp -d)
    trap "rm -rf '$temp_dir'" EXIT
    
    # Download archive
    if ! curl -fsSL -o "$temp_dir/$archive_name" "$download_url"; then
        red "âŒ Failed to download $archive_name"
        red "   URL: $download_url"
        exit 1
    fi
    
    # Extract archive
    echo "ğŸ“¦ Extracting binary..."
    cd "$temp_dir"
    case "$extension" in
        .zip)
            if command -v unzip >/dev/null 2>&1; then
                unzip -q "$archive_name"
            else
                red "âŒ unzip is required but not installed"
                exit 1
            fi
            ;;
        .tar.gz)
            tar -xzf "$archive_name"
            ;;
    esac
    
    # Check if binary exists
    if [ ! -f "$binary_name" ]; then
        red "âŒ Binary not found in archive"
        exit 1
    fi
    
    # Make binary executable
    chmod +x "$binary_name"
    
    # Create install directory if it doesn't exist
    if [ ! -d "$INSTALL_DIR" ]; then
        echo "ğŸ“ Creating install directory: $INSTALL_DIR"
        mkdir -p "$INSTALL_DIR"
    fi
    
    # Install binary
    echo "ğŸš€ Installing to $INSTALL_DIR..."
    if ! mv "$binary_name" "$INSTALL_DIR/"; then
        red "âŒ Failed to install binary to $INSTALL_DIR"
        red "   You may need to run with sudo or choose a different install directory"
        exit 1
    fi
    
    green "âœ… Successfully installed twars-url2md $version"
    echo "   Binary: $INSTALL_DIR/twars-url2md"
}

# Check if already installed
check_existing() {
    local binary_path="$INSTALL_DIR/twars-url2md"
    if [ -f "$binary_path" ] && [ "$FORCE" = false ]; then
        yellow "âš ï¸  twars-url2md is already installed at $binary_path"
        local current_version
        current_version=$("$binary_path" --version 2>/dev/null | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
        echo "   Current version: $current_version"
        echo "   Use --force to overwrite"
        exit 0
    fi
}

# Main installation
main() {
    echo "ğŸ”§ Installing twars-url2md..."
    echo ""
    
    # Detect platform
    local platform
    platform=$(detect_platform)
    echo "Platform: $platform"
    
    # Get version
    local version
    if [ -n "$VERSION" ]; then
        version="$VERSION"
        echo "Version: $version (specified)"
    else
        version=$(get_latest_version)
        echo "Version: $version (latest)"
    fi
    
    echo "Install directory: $INSTALL_DIR"
    echo ""
    
    # Check if already installed
    check_existing
    
    # Install binary
    install_binary "$version" "$platform"
    
    # Verify installation
    echo ""
    echo "ğŸ” Verifying installation..."
    local binary_path="$INSTALL_DIR/twars-url2md"
    if [ -f "$binary_path" ]; then
        local installed_version
        installed_version=$("$binary_path" --version 2>/dev/null || echo "Version check failed")
        echo "   $installed_version"
        green "âœ… Installation verified"
    else
        red "âŒ Installation verification failed"
        exit 1
    fi
    
    # Show usage information
    echo ""
    echo "ğŸ‰ Installation complete!"
    echo ""
    echo "Usage examples:"
    echo "  twars-url2md --input urls.txt --output ./markdown"
    echo "  echo 'https://example.com' | twars-url2md --stdin"
    echo "  twars-url2md --help"
    echo ""
    echo "Note: Make sure $INSTALL_DIR is in your PATH environment variable."
    if [[ ":$PATH:" != *":$INSTALL_DIR:"* ]]; then
        yellow "âš ï¸  $INSTALL_DIR is not in your PATH"
        echo "   Add it to your shell profile:"
        echo "   export PATH=\"$INSTALL_DIR:\$PATH\""
    fi
}

# Run main function
main "$@"
</document_content>
</document>

# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/issues/issuetest.py
# Language: python

import subprocess
import os
import sys
import tempfile
import shutil
import time
from pathlib import Path

def print_test_header((issue_num, description)):
    """Print a formatted test header"""

def print_result((success, message)):
    """Print a test result with color"""

def run_command((cmd, capture_output=True, check=False)):
    """Run a shell command and return the result"""

def test_issue_105(()):
    """Test Issue #105: Fix Help Option Not Working"""

def test_issue_106(()):
    """Test Issue #106: Fix Output Writing Issues"""

def test_issue_107(()):
    """Test Issue #107: Implement Smart HTML Content Extraction"""

def test_issue_108(()):
    """Test Issue #108: Remove Panic Recovery Wrapper from Main"""

def test_issue_109(()):
    """Test Issue #109: Update Documentation for Logging Framework"""

def test_issue_110(()):
    """Test Issue #110: Enhanced Testing Strategy"""

def main(()):
    """Run all issue tests"""


<document index="23">
<source>issues/resolved/RESOLVED_ISSUES_SUMMARY.md</source>
<document_content>
# Resolved Issues Summary

This document summarizes all issues that have been resolved in twars-url2md as of 2025-06-25.

## Issue #104: Adobe CDN Timeout Issue âœ…
**Status**: RESOLVED  
**Problem**: URLs from Adobe HelpX were timing out after 60 seconds with 0 bytes received  
**Solution**: Removed forced HTTP/1.1 version and added browser-like headers  
**Commit**: Fixed by removing `easy.http_version(curl::easy::HttpVersion::V11)` and adding comprehensive browser headers

## Issue #105: Fix Help Option Not Working âœ…
**Status**: RESOLVED  
**Problem**: Running `twars-url2md -h` or `--help` produced no output  
**Solution**: The issue was already fixed in the current codebase  
**Verification**: Both `-h` and `--help` display proper usage information

## Issue #106: Fix Output Writing Issues âœ…
**Status**: RESOLVED  
**Problem**: Output flags `-p out.md` or `-o out` didn't create files  
**Solution**: All output modes are working correctly in current implementation  
**Output Modes Verified**:
- Directory output: `-o dir/` creates `dir/domain.com/path/file.md`
- Single file output: `-o file.md` creates single markdown file
- Pack mode: `-p packed.md` combines multiple URLs into one file
- Default behavior: No flags creates files in current directory

## Issue #107: Implement Smart HTML Content Extraction âœ…
**Status**: RESOLVED (Framework implemented)  
**Problem**: Output included navigation, ads, sidebars instead of just main content  
**Solution**: Created `ContentExtractor` module and added `--all` flag  
**Implementation**:
- Added `src/content_extractor.rs` with extraction logic
- Added `-a/--all` flag to bypass smart extraction
- Framework ready for integration with HTML pipeline

## Issue #108: Remove Panic Recovery Wrapper from Main âœ…
**Status**: RESOLVED  
**Problem**: Using `catch_unwind` in main.rs masked underlying issues  
**Solution**: Application handles errors gracefully without top-level panics  
**Verification**: Tested with malformed input and empty input - no panics

## Issue #109: Update Documentation for Logging Framework âœ…
**Status**: RESOLVED  
**Problem**: Documentation didn't explain how to use tracing-based logging  
**Solution**: Added comprehensive logging documentation to README.md  
**Documentation Added**:
- RUST_LOG environment variable usage
- Module-specific logging syntax
- Log level explanations
- Examples for different scenarios

## Issue #110: Enhanced Testing Strategy âœ…
**Status**: RESOLVED  
**Problem**: Limited test coverage for network interactions and edge cases  
**Solution**: Comprehensive test suite with 78+ tests  
**Test Coverage**:
- 42+ unit tests
- 6+ integration test files
- Issue verification suite (`issues/issuetest.py`)
- All tests passing

## Verification

All issues have been verified using the comprehensive test suite:

```bash
python3 issues/issuetest.py
```

Result: **6/6 issues resolved** âœ…

## Key Improvements Summary

1. **Network Compatibility**: Fixed CDN timeout issues, especially with Adobe sites
2. **CLI Usability**: Help and version commands work properly
3. **Output Flexibility**: All output modes functioning correctly
4. **Content Quality**: Smart extraction framework ready for use
5. **Error Handling**: Graceful error handling without panics
6. **Developer Experience**: Comprehensive logging and documentation
7. **Code Quality**: Extensive test coverage ensuring reliability

---

*Last verified: 2025-06-25*
</document_content>
</document>

<document index="24">
<source>scripts/build.sh</source>
<document_content>
#!/bin/bash
# this_file: scripts/build.sh

set -euo pipefail

# Build script for twars-url2md
# This script handles building the project with proper version information

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$PROJECT_ROOT"

echo "ğŸ”¨ Building twars-url2md..."

# Clean previous builds
echo "ğŸ§¹ Cleaning previous builds..."
cargo clean

# Check if we're in a git repository
if [ -d .git ]; then
    echo "ğŸ“‹ Git repository detected"
    
    # Get git version info
    GIT_VERSION=$(git describe --tags --always --dirty 2>/dev/null || echo "unknown")
    GIT_COMMIT=$(git rev-parse HEAD 2>/dev/null || echo "unknown")
    GIT_BRANCH=$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo "unknown")
    
    echo "   Version: $GIT_VERSION"
    echo "   Commit: ${GIT_COMMIT:0:8}"
    echo "   Branch: $GIT_BRANCH"
else
    echo "âš ï¸  Not in a git repository"
fi

# Build in release mode
echo "ğŸ—ï¸  Building release binary..."
cargo build --release

# Get the binary path
BINARY_PATH="target/release/twars-url2md"

if [ -f "$BINARY_PATH" ]; then
    echo "âœ… Build successful!"
    echo "   Binary: $BINARY_PATH"
    echo "   Size: $(du -h "$BINARY_PATH" | cut -f1)"
    
    # Show version info
    echo "ğŸ“Š Version information:"
    "$BINARY_PATH" --version 2>/dev/null || echo "   (Version command not available)"
else
    echo "âŒ Build failed - binary not found"
    exit 1
fi

# Create builds directory if it doesn't exist
mkdir -p builds

# Copy binary to builds directory with version info
if [ -d .git ]; then
    BUILD_NAME="twars-url2md-${GIT_VERSION}-$(uname -s | tr '[:upper:]' '[:lower:]')-$(uname -m)"
else
    BUILD_NAME="twars-url2md-local-$(uname -s | tr '[:upper:]' '[:lower:]')-$(uname -m)"
fi

cp "$BINARY_PATH" "builds/$BUILD_NAME"
echo "ğŸ“¦ Binary copied to: builds/$BUILD_NAME"

echo "ğŸ‰ Build completed successfully!"
</document_content>
</document>

<document index="25">
<source>scripts/release.sh</source>
<document_content>
#!/bin/bash
# this_file: scripts/release.sh

set -euo pipefail

# Release script for twars-url2md
# This script handles the complete release process including tagging and publishing

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$PROJECT_ROOT"

# Color output functions
red() { echo -e "\033[31m$1\033[0m"; }
green() { echo -e "\033[32m$1\033[0m"; }
yellow() { echo -e "\033[33m$1\033[0m"; }
blue() { echo -e "\033[34m$1\033[0m"; }

# Default options
DRY_RUN=false
SKIP_TESTS=false
SKIP_BUILD=false
FORCE=false
VERSION=""

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        --skip-tests)
            SKIP_TESTS=true
            shift
            ;;
        --skip-build)
            SKIP_BUILD=true
            shift
            ;;
        --force)
            FORCE=true
            shift
            ;;
        --version)
            VERSION="$2"
            shift 2
            ;;
        --help|-h)
            echo "Usage: $0 [OPTIONS]"
            echo ""
            echo "Options:"
            echo "  --version VERSION  Specify version to release (e.g., 1.2.3)"
            echo "  --dry-run          Show what would be done without making changes"
            echo "  --skip-tests       Skip running tests"
            echo "  --skip-build       Skip building the project"
            echo "  --force            Force release even if working directory is dirty"
            echo "  --help, -h         Show this help message"
            echo ""
            echo "Examples:"
            echo "  $0 --version 1.2.3"
            echo "  $0 --version 1.2.3 --dry-run"
            echo "  $0 --version 1.2.3 --skip-tests"
            exit 0
            ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Validation
if [ -z "$VERSION" ]; then
    red "âŒ Version is required. Use --version to specify it."
    echo "Example: $0 --version 1.2.3"
    exit 1
fi

# Validate version format (semantic versioning)
if ! echo "$VERSION" | grep -E '^[0-9]+\.[0-9]+\.[0-9]+(-[a-zA-Z0-9.-]+)?$' > /dev/null; then
    red "âŒ Invalid version format. Use semantic versioning (e.g., 1.2.3 or 1.2.3-beta.1)"
    exit 1
fi

# Check if we're in a git repository
if [ ! -d .git ]; then
    red "âŒ Not in a git repository"
    exit 1
fi

# Check if working directory is clean
if [ "$FORCE" = false ] && [ -n "$(git status --porcelain)" ]; then
    red "âŒ Working directory is not clean. Commit your changes first or use --force."
    git status --short
    exit 1
fi

# Check if we're on the main branch
CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)
if [ "$CURRENT_BRANCH" != "main" ] && [ "$FORCE" = false ]; then
    red "âŒ Not on main branch (currently on $CURRENT_BRANCH). Use --force to override."
    exit 1
fi

# Check if tag already exists
if git rev-parse "v$VERSION" >/dev/null 2>&1; then
    red "âŒ Tag v$VERSION already exists"
    exit 1
fi

echo "ğŸš€ Preparing release v$VERSION..."
echo "   Current branch: $CURRENT_BRANCH"
echo "   Dry run: $DRY_RUN"

# Function to execute command with dry run support
execute() {
    local cmd="$1"
    local desc="$2"
    
    echo "ğŸ“‹ $desc"
    if [ "$DRY_RUN" = true ]; then
        blue "   [DRY RUN] Would execute: $cmd"
    else
        echo "   Executing: $cmd"
        eval "$cmd"
    fi
}

# Update version in Cargo.toml
echo "ğŸ“ Updating version in Cargo.toml..."
if [ "$DRY_RUN" = true ]; then
    blue "   [DRY RUN] Would update version to $VERSION in Cargo.toml"
else
    # Update version - keeping it as 0.0.0 since we use git tags
    echo "   Version will be set from git tag during build"
fi

# Run tests
if [ "$SKIP_TESTS" = false ]; then
    echo "ğŸ§ª Running tests..."
    if [ "$DRY_RUN" = true ]; then
        blue "   [DRY RUN] Would run: ./scripts/test.sh"
    else
        if [ -f "./scripts/test.sh" ]; then
            ./scripts/test.sh
        else
            cargo test --all-features
        fi
    fi
fi

# Build the project
if [ "$SKIP_BUILD" = false ]; then
    echo "ğŸ—ï¸  Building project..."
    if [ "$DRY_RUN" = true ]; then
        blue "   [DRY RUN] Would run: ./scripts/build.sh"
    else
        if [ -f "./scripts/build.sh" ]; then
            ./scripts/build.sh
        else
            cargo build --release
        fi
    fi
fi

# Create git tag
execute "git tag -a v$VERSION -m 'Release version $VERSION'" "Creating git tag v$VERSION"

# Push tag to remote
execute "git push origin v$VERSION" "Pushing tag to remote"

# Push any pending commits
execute "git push origin $CURRENT_BRANCH" "Pushing current branch to remote"

# Create release notes
RELEASE_NOTES_FILE="release-notes-v$VERSION.md"
echo "ğŸ“ Creating release notes..."
if [ "$DRY_RUN" = true ]; then
    blue "   [DRY RUN] Would create release notes in $RELEASE_NOTES_FILE"
else
    cat > "$RELEASE_NOTES_FILE" << EOF
# Release Notes v$VERSION

## Changes

<!-- Add your changes here -->

## Installation

### From GitHub Releases

Download the appropriate binary for your platform from the [releases page](https://github.com/twardoch/twars-url2md/releases/tag/v$VERSION).

### From Source

\`\`\`bash
git clone https://github.com/twardoch/twars-url2md.git
cd twars-url2md
git checkout v$VERSION
cargo build --release
\`\`\`

### From crates.io

\`\`\`bash
cargo install twars-url2md
\`\`\`

## Checksums

<!-- Checksums will be added by CI -->

EOF
    echo "   Created: $RELEASE_NOTES_FILE"
fi

# Show next steps
echo ""
green "ğŸ‰ Release v$VERSION prepared successfully!"
echo ""
echo "Next steps:"
echo "1. Edit $RELEASE_NOTES_FILE to add changelog information"
echo "2. The GitHub Actions workflow will automatically:"
echo "   - Build multi-platform binaries"
echo "   - Create a GitHub release"
echo "   - Publish to crates.io"
echo "   - Upload release artifacts"
echo ""
echo "Monitor the progress at:"
echo "https://github.com/twardoch/twars-url2md/actions"
echo ""
echo "Once the release is published, you can:"
echo "- Announce the release"
echo "- Update documentation"
echo "- Notify users"

if [ "$DRY_RUN" = true ]; then
    echo ""
    yellow "âš ï¸  This was a dry run. No actual changes were made."
    echo "Run without --dry-run to execute the release."
fi
</document_content>
</document>

<document index="26">
<source>scripts/test.sh</source>
<document_content>
#!/bin/bash
# this_file: scripts/test.sh

set -euo pipefail

# Test script for twars-url2md
# This script runs the complete test suite including unit, integration, and benchmark tests

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$PROJECT_ROOT"

echo "ğŸ§ª Running twars-url2md test suite..."

# Default test options
RUN_UNIT_TESTS=true
RUN_INTEGRATION_TESTS=true
RUN_BENCHMARK_TESTS=false
RUN_CLIPPY=true
RUN_FORMAT_CHECK=true
VERBOSE=false

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --unit-only)
            RUN_INTEGRATION_TESTS=false
            RUN_BENCHMARK_TESTS=false
            shift
            ;;
        --integration-only)
            RUN_UNIT_TESTS=false
            RUN_BENCHMARK_TESTS=false
            shift
            ;;
        --benchmark|--bench)
            RUN_BENCHMARK_TESTS=true
            shift
            ;;
        --no-clippy)
            RUN_CLIPPY=false
            shift
            ;;
        --no-format)
            RUN_FORMAT_CHECK=false
            shift
            ;;
        --verbose|-v)
            VERBOSE=true
            shift
            ;;
        --help|-h)
            echo "Usage: $0 [OPTIONS]"
            echo ""
            echo "Options:"
            echo "  --unit-only        Run only unit tests"
            echo "  --integration-only Run only integration tests"
            echo "  --benchmark        Run benchmark tests"
            echo "  --no-clippy        Skip clippy linting"
            echo "  --no-format        Skip format checking"
            echo "  --verbose, -v      Enable verbose output"
            echo "  --help, -h         Show this help message"
            exit 0
            ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Set verbosity flags
if [ "$VERBOSE" = true ]; then
    CARGO_VERBOSE_FLAG="--verbose"
else
    CARGO_VERBOSE_FLAG=""
fi

# Color output functions
red() { echo -e "\033[31m$1\033[0m"; }
green() { echo -e "\033[32m$1\033[0m"; }
yellow() { echo -e "\033[33m$1\033[0m"; }
blue() { echo -e "\033[34m$1\033[0m"; }

# Function to run command with status reporting
run_test() {
    local test_name="$1"
    local command="$2"
    
    echo "ğŸ” Running $test_name..."
    if eval "$command"; then
        green "âœ… $test_name passed"
        return 0
    else
        red "âŒ $test_name failed"
        return 1
    fi
}

# Track test results
TESTS_PASSED=0
TESTS_FAILED=0

# Format checking
if [ "$RUN_FORMAT_CHECK" = true ]; then
    if run_test "format check" "cargo fmt --check $CARGO_VERBOSE_FLAG"; then
        ((TESTS_PASSED++))
    else
        ((TESTS_FAILED++))
        yellow "ğŸ’¡ Run 'cargo fmt' to fix formatting issues"
    fi
fi

# Clippy linting
if [ "$RUN_CLIPPY" = true ]; then
    if run_test "clippy linting" "cargo clippy --all-targets --all-features $CARGO_VERBOSE_FLAG -- -D warnings"; then
        ((TESTS_PASSED++))
    else
        ((TESTS_FAILED++))
        yellow "ğŸ’¡ Fix clippy warnings before proceeding"
    fi
fi

# Unit tests
if [ "$RUN_UNIT_TESTS" = true ]; then
    echo "ğŸ§ª Running unit tests..."
    
    # Test src/ modules
    if run_test "library unit tests" "cargo test --lib $CARGO_VERBOSE_FLAG"; then
        ((TESTS_PASSED++))
    else
        ((TESTS_FAILED++))
    fi
    
    # Test specific unit test modules
    if run_test "URL extraction tests" "cargo test --test '' tests::unit::url_tests $CARGO_VERBOSE_FLAG"; then
        ((TESTS_PASSED++))
    else
        ((TESTS_FAILED++))
    fi
fi

# Integration tests
if [ "$RUN_INTEGRATION_TESTS" = true ]; then
    echo "ğŸ”— Running integration tests..."
    
    # Note: Integration tests might be disabled due to mockito version issues
    # Check if integration tests are enabled
    if grep -q "mod integration;" tests/tests.rs; then
        if run_test "integration tests" "cargo test --test '' tests::integration $CARGO_VERBOSE_FLAG"; then
            ((TESTS_PASSED++))
        else
            ((TESTS_FAILED++))
        fi
    else
        yellow "âš ï¸  Integration tests are disabled (mockito compatibility)"
    fi
    
    # Run end-to-end tests directly
    if run_test "end-to-end tests" "cargo test --test '' e2e_tests $CARGO_VERBOSE_FLAG"; then
        ((TESTS_PASSED++))
    else
        ((TESTS_FAILED++))
    fi
fi

# Benchmark tests
if [ "$RUN_BENCHMARK_TESTS" = true ]; then
    echo "âš¡ Running benchmark tests..."
    
    if run_test "benchmark tests" "cargo test --test '' benchmark_tests $CARGO_VERBOSE_FLAG"; then
        ((TESTS_PASSED++))
    else
        ((TESTS_FAILED++))
    fi
fi

# Documentation tests
echo "ğŸ“š Running documentation tests..."
if run_test "doc tests" "cargo test --doc $CARGO_VERBOSE_FLAG"; then
    ((TESTS_PASSED++))
else
    ((TESTS_FAILED++))
fi

# Test build
echo "ğŸ—ï¸  Testing build..."
if run_test "build test" "cargo build --all-features $CARGO_VERBOSE_FLAG"; then
    ((TESTS_PASSED++))
else
    ((TESTS_FAILED++))
fi

# Summary
echo ""
echo "ğŸ“Š Test Summary:"
echo "=================="
green "âœ… Passed: $TESTS_PASSED"
if [ $TESTS_FAILED -gt 0 ]; then
    red "âŒ Failed: $TESTS_FAILED"
else
    green "âŒ Failed: $TESTS_FAILED"
fi

echo ""
if [ $TESTS_FAILED -eq 0 ]; then
    green "ğŸ‰ All tests passed! The codebase is healthy."
    exit 0
else
    red "ğŸ’¥ Some tests failed. Please fix the issues before proceeding."
    exit 1
fi
</document_content>
</document>

# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/src/cli.rs
# Language: rust

mod tests;

struct Cli {
}


# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/src/html.rs
# Language: rust

mod tests;


# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/src/lib.rs
# Language: rust

mod cli;

mod html;

mod markdown;

mod url;

struct Config {
}


# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/src/main.rs
# Language: rust



# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/src/markdown.rs
# Language: rust

mod tests;


# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/src/tests.rs
# Language: rust



# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/src/url.rs
# Language: rust

mod tests;


<document index="27">
<source>src_docs/md/advanced.md</source>
<document_content>
# Advanced Features

Explore powerful features and sophisticated workflows that make `twars-url2md` suitable for complex content processing scenarios.

## URL Extraction and Processing

### Smart URL Detection

`twars-url2md` uses intelligent URL extraction to find links in various content formats:

```bash
# Extract from HTML page with mixed content
curl -s https://news.ycombinator.com | \
  twars-url2md --stdin --base-url https://news.ycombinator.com -o hn_links/
```

**Supports**:
- HTML `<a href="...">` tags
- Markdown `[text](url)` links  
- Plain text URLs (http/https)
- Relative URLs with base URL resolution
- Multiple URL formats in single input

### Base URL Resolution

Resolve relative URLs found in content:

```bash
# HTML content with relative links
cat << 'EOF' > content.html
<a href="/docs/guide.html">Guide</a>
<a href="../api/reference.html">API</a>
<a href="https://external.com/page">External</a>  
EOF

twars-url2md -i content.html --base-url https://mysite.com/current/page -o resolved/
```

**Result**: Relative URLs become:
- `/docs/guide.html` â†’ `https://mysite.com/docs/guide.html`
- `../api/reference.html` â†’ `https://mysite.com/api/reference.html`
- External URLs remain unchanged

### URL Filtering and Validation

Built-in URL validation and filtering:

```bash
# Only HTTP/HTTPS URLs are processed
cat << 'EOF' > mixed_urls.txt
https://valid-site.com
http://another-site.com  
ftp://ignored-ftp-site.com
mailto:ignored@email.com
https://valid-secure-site.com
EOF

twars-url2md -i mixed_urls.txt -o filtered/ -v
```

**Automatic filtering**:
- âœ… `http://` and `https://` URLs
- âŒ `ftp://`, `mailto:`, `file://` (except local files)
- âŒ Invalid or malformed URLs
- âŒ Duplicate URLs (automatic deduplication)

## Advanced Output Options

### Packed Output with Custom Headers

The `--pack` option creates structured single-file output:

```bash
twars-url2md -i research_papers.txt --pack "research-$(date +%Y-%m-%d).md" -v
```

**Packed format features**:
- URL as primary heading (`# https://example.com`)
- Preserves original URL order
- Clear content separation
- Maintains markdown structure within each section

### Hybrid Output Mode

Combine individual files with packed output:

```bash
# Creates both directory structure AND packed file
twars-url2md -i urls.txt -o individual_files/ --pack archive.md -v
```

**Use cases**:
- Individual files for browsing/editing
- Packed file for searching/archiving
- Different output formats for different workflows

### Output Path Generation

Understanding output path creation:

```bash
# URL: https://doc.rust-lang.org/book/ch01-01-installation.html
twars-url2md "https://doc.rust-lang.org/book/ch01-01-installation.html" -o output/
```

**Generated path**: `output/doc.rust-lang.org/book/ch01-01-installation.md`

**Path generation rules**:
1. Domain becomes top-level directory
2. Path segments become subdirectories
3. Final segment becomes filename with `.md` extension
4. Query parameters and fragments are normalized/removed

## Local File Processing

### HTML File Conversion

Process local HTML files with full feature support:

```bash
# Single local file
twars-url2md /path/to/document.html -o converted/

# Multiple files with find
find ./html_docs -name "*.html" -print0 | \
  xargs -0 -I {} twars-url2md {} -o converted/

# Using file:// URLs
twars-url2md file:///absolute/path/to/doc.html -o output/
```

### Batch Local Processing

```bash
# Process entire directory structure  
find ./website_backup -type f -name "*.html" | \
  twars-url2md --stdin -o markdown_site/ -v

# Preserve directory structure in output
for html_file in $(find ./src -name "*.html"); do
  rel_path="${html_file#./src/}"
  output_path="./md/${rel_path%.html}.md"
  mkdir -p "$(dirname "$output_path")"
  twars-url2md "$html_file" -o "$output_path"
done
```

## Content Processing Pipeline

### Multi-stage Processing

Complex workflows combining extraction and conversion:

```bash
#!/bin/bash
# Advanced content pipeline

# Stage 1: Extract URLs from multiple sources
{
  curl -s https://awesome-rust.com | grep -o 'https://[^"]*github.com[^"]*'
  curl -s https://crates.io/categories | grep -o 'https://[^"]*crates.io[^"]*'
  cat manually_curated_urls.txt
} | sort -u > all_urls.txt

# Stage 2: Filter and categorize
grep 'github.com' all_urls.txt > github_repos.txt
grep 'crates.io' all_urls.txt > crate_pages.txt  
grep -v -E '(github\.com|crates\.io)' all_urls.txt > other_sites.txt

# Stage 3: Process each category
twars-url2md -i github_repos.txt -o repos/ --pack github-projects.md -v &
twars-url2md -i crate_pages.txt -o crates/ --pack rust-crates.md -v &
twars-url2md -i other_sites.txt -o misc/ --pack other-resources.md -v &

wait

# Stage 4: Create master index
cat > complete-rust-resources.md << 'EOF'
# Complete Rust Resources Archive

## GitHub Projects
EOF
cat github-projects.md >> complete-rust-resources.md

echo -e "\n## Rust Crates" >> complete-rust-resources.md
cat rust-crates.md >> complete-rust-resources.md

echo -e "\n## Other Resources" >> complete-rust-resources.md
cat other-resources.md >> complete-rust-resources.md
```

### Content Filtering and Processing

```bash
# Extract specific domains only
curl -s https://link-aggregator.com | \
  grep -o 'https://[^"]*' | \
  grep -E '\.(edu|gov|org)/' | \
  twars-url2md --stdin -o academic_content/ -v

# Process recent articles only (if timestamps available)
awk -F',' '$2 > "'$(date -d '30 days ago' +%s)'" {print $1}' timestamped_urls.csv | \
  twars-url2md --stdin -o recent_articles/ --pack recent.md
```

## Integration Patterns

### CI/CD Integration

#### GitHub Actions Workflow

```yaml
name: Documentation Sync
on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM
  workflow_dispatch:

jobs:
  sync-docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Install twars-url2md
        run: |
          curl -fsSL https://raw.githubusercontent.com/twardoch/twars-url2md/main/install.sh | bash
          echo "$HOME/.local/bin" >> $GITHUB_PATH
      
      - name: Process documentation URLs
        run: |
          twars-url2md -i .github/doc_urls.txt -o docs/ --pack docs-complete.md -v
      
      - name: Commit updates
        run: |
          git config --local user.email "action@github.com"  
          git config --local user.name "GitHub Action"
          git add docs/
          git diff --staged --quiet || git commit -m "Update documentation $(date +%Y-%m-%d)"
          git push
```

#### Jenkins Pipeline

```groovy
pipeline {
    agent any
    triggers {
        cron('H 2 * * *')
    }
    stages {
        stage('Install Tools') {
            steps {
                sh '''
                    curl -fsSL https://raw.githubusercontent.com/twardoch/twars-url2md/main/install.sh | bash
                    export PATH="$HOME/.local/bin:$PATH"
                '''
            }
        }
        stage('Convert Documentation') {
            steps {
                sh '''
                    export PATH="$HOME/.local/bin:$PATH"
                    twars-url2md -i urls.txt -o output/ --pack archive.md -v
                '''
            }
        }
        stage('Archive Results') {
            steps {
                archiveArtifacts artifacts: 'output/**/*.md, archive.md'
            }
        }
    }
}
```

### API Integration

#### REST API Processing

```bash
#!/bin/bash
# Process URLs from REST API

# Fetch URLs from API
curl -H "Authorization: Bearer $API_TOKEN" \
     https://api.example.com/urls | \
     jq -r '.urls[]' > api_urls.txt

# Process with error handling
if twars-url2md -i api_urls.txt -o api_content/ --pack "api-content-$(date +%Y%m%d).md" -v; then
    echo "Success: $(wc -l < api_urls.txt) URLs processed"
    # Upload results
    aws s3 cp api-content-*.md s3://my-bucket/archives/
else
    echo "Error: Processing failed"
    exit 1
fi
```

#### Webhook Integration

```python
#!/usr/bin/env python3
# Webhook handler for URL processing

from flask import Flask, request, jsonify
import subprocess
import tempfile
import os

app = Flask(__name__)

@app.route('/process-urls', methods=['POST'])
def process_urls():
    try:
        urls = request.json.get('urls', [])
        
        # Create temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            for url in urls:
                f.write(f"{url}\n")
            temp_file = f.name
        
        # Process URLs
        result = subprocess.run([
            'twars-url2md', '-i', temp_file, 
            '-o', 'webhook_output/', 
            '--pack', f'webhook-{int(time.time())}.md',
            '-v'
        ], capture_output=True, text=True)
        
        # Cleanup
        os.unlink(temp_file)
        
        if result.returncode == 0:
            return jsonify({'status': 'success', 'message': 'URLs processed'})
        else:
            return jsonify({'status': 'error', 'message': result.stderr}), 500
            
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

## Performance Optimization

### Parallel Processing Strategies

For very large URL sets:

```bash
# Split large URL list into chunks
split -l 100 huge_urls.txt chunk_

# Process chunks in parallel with resource limits
MAX_JOBS=4
job_count=0

for chunk in chunk_*; do
    if ((job_count >= MAX_JOBS)); then
        wait -n  # Wait for any job to finish
        ((job_count--))
    fi
    
    twars-url2md -i "$chunk" -o "output_${chunk}/" -v &
    ((job_count++))
done

wait  # Wait for all remaining jobs

# Combine results
find output_chunk_* -name "*.md" -exec cp {} final_output/ \;
```

### Resource Management

```bash
# Monitor resource usage during processing
{
    echo "Starting URL processing at $(date)"
    echo "System resources before:"
    free -h
    df -h
    
    # Process URLs with monitoring
    twars-url2md -i large_url_list.txt -o output/ -v &
    PID=$!
    
    # Monitor during processing
    while kill -0 $PID 2>/dev/null; do
        echo "$(date): Memory usage:"
        ps -p $PID -o pid,vsz,rss,pcpu
        sleep 30
    done
    
    wait $PID
    EXIT_CODE=$?
    
    echo "Processing completed with exit code: $EXIT_CODE"
    echo "Final system resources:"
    free -h
    df -h
} | tee processing.log
```

### Optimization Tips

=== "Network Optimization"
    ```bash
    # For sites behind CDNs, verbose mode helps debug
    twars-url2md https://cdn-protected-site.com -v
    
    # Batch similar domains together for connection reuse
    grep 'github.com' urls.txt | twars-url2md --stdin -o github/ -v
    grep 'docs.rs' urls.txt | twars-url2md --stdin -o rust_docs/ -v
    ```

=== "Storage Optimization"  
    ```bash
    # Use packed output for space efficiency
    twars-url2md -i urls.txt --pack compressed-archive.md
    
    # Clean up intermediate files
    twars-url2md -i urls.txt -o temp/ --pack final.md && rm -rf temp/
    ```

=== "Processing Optimization"
    ```bash
    # Pre-filter URLs to reduce processing time
    grep -E '\.(html|htm|php)(\?|$)' urls.txt | \
      twars-url2md --stdin -o filtered_output/ -v
    ```

## Error Handling and Recovery

### Robust Error Handling

```bash
#!/bin/bash
# Robust processing with error recovery

set -euo pipefail

# Function to process URLs with retry
process_with_retry() {
    local input_file="$1"
    local output_dir="$2"
    local max_attempts=3
    local attempt=1
    
    while ((attempt <= max_attempts)); do
        echo "Attempt $attempt of $max_attempts"
        
        if twars-url2md -i "$input_file" -o "$output_dir" -v; then
            echo "Success on attempt $attempt"
            return 0
        else
            echo "Failed attempt $attempt"
            if ((attempt < max_attempts)); then
                echo "Retrying in $((attempt * 10)) seconds..."
                sleep $((attempt * 10))
            fi
            ((attempt++))
        fi
    done
    
    echo "All attempts failed"
    return 1
}

# Main processing
if ! process_with_retry "urls.txt" "output/"; then
    # Fallback: process individual URLs to identify problems
    echo "Batch processing failed, trying individual URLs..."
    
    while IFS= read -r url; do
        if ! twars-url2md "$url" -o "individual_output/" -v; then
            echo "Failed URL: $url" >> failed_urls.txt
        fi
    done < urls.txt
    
    echo "Check failed_urls.txt for problematic URLs"
fi
```

### Logging and Monitoring

```bash
# Comprehensive logging setup
export RUST_LOG=info,twars_url2md=debug

# Process with full logging
twars-url2md -i urls.txt -o output/ -v 2>&1 | \
  tee >(grep "ERROR" > errors.log) | \
  tee >(grep "WARN" > warnings.log) > full.log

# Generate processing report
{
    echo "Processing Report - $(date)"
    echo "===================="
    echo "Total URLs processed: $(wc -l < urls.txt)"
    echo "Errors encountered: $(wc -l < errors.log)"
    echo "Warnings: $(wc -l < warnings.log)"
    echo "Output files created: $(find output/ -name "*.md" | wc -l)"
    echo ""
    echo "Top errors:"
    sort errors.log | uniq -c | sort -nr | head -5
} > processing_report.txt
```

---

!!! tip "Advanced Workflows"
    - Combine multiple processing strategies for complex content pipelines
    - Use shell scripting to create custom workflows tailored to your needs
    - Monitor system resources during large batch operations
    - Implement proper error handling and recovery mechanisms for production use
    - Consider using container orchestration for very large-scale processing
</document_content>
</document>

<document index="28">
<source>src_docs/md/api.md</source>
<document_content>
# API Reference

Complete API documentation for using `twars-url2md` as a Rust library in your applications.

## Getting Started

Add `twars-url2md` to your `Cargo.toml`:

```toml
[dependencies]
twars-url2md = "0.3.0"  # Check crates.io for latest version
tokio = { version = "1", features = ["full"] }
anyhow = "1"
```

## Core API

### Main Processing Function

#### `process_urls`

The primary function for processing multiple URLs:

```rust
pub async fn process_urls(
    urls: Vec<String>, 
    config: Config
) -> Result<Vec<(String, anyhow::Error)>>
```

**Parameters:**
- `urls`: Vector of URLs to process
- `config`: Processing configuration

**Returns:**
- `Ok(Vec<(String, Error)>)`: List of failed URLs with their errors
- `Err(anyhow::Error)`: Critical error that stopped processing

**Example:**
```rust
use twars_url2md::{process_urls, Config};
use std::path::PathBuf;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let urls = vec![
        "https://www.rust-lang.org".to_string(),
        "https://crates.io".to_string(),
    ];
    
    let config = Config {
        output_base: PathBuf::from("./output"),
        verbose: true,
        max_retries: 3,
        single_file: false,
        has_output: true,
        pack_file: None,
    };
    
    match process_urls(urls, config).await {
        Ok(errors) => {
            if errors.is_empty() {
                println!("All URLs processed successfully!");
            } else {
                eprintln!("Some URLs failed:");
                for (url, error) in errors {
                    eprintln!("  {}: {}", url, error);
                }
            }
        }
        Err(e) => {
            eprintln!("Critical error: {}", e);
        }
    }
    
    Ok(())
}
```

### Configuration

#### `Config` Struct

Configuration for URL processing:

```rust
pub struct Config {
    pub verbose: bool,
    pub max_retries: u32,
    pub output_base: PathBuf,
    pub single_file: bool,
    pub has_output: bool,
    pub pack_file: Option<PathBuf>,
}
```

**Fields:**
- `verbose`: Enable detailed logging
- `max_retries`: Maximum retry attempts for failed requests
- `output_base`: Base path for output files
- `single_file`: Whether output_base is a single file
- `has_output`: Whether to write individual files
- `pack_file`: Optional path for packed output file

**Default Implementation:**
```rust
impl Default for Config {
    fn default() -> Self {
        Self {
            verbose: false,
            max_retries: 2,
            output_base: PathBuf::from("./output"),
            single_file: false,
            has_output: true,
            pack_file: None,
        }
    }
}
```

## URL Processing

### URL Extraction

#### `extract_urls_from_text`

Extract URLs from various text formats:

```rust
pub fn extract_urls_from_text(text: &str, base_url: Option<&str>) -> Vec<String>
```

**Parameters:**
- `text`: Input text containing URLs
- `base_url`: Base URL for resolving relative links

**Returns:**
- Vector of extracted and validated URLs

**Example:**
```rust
use twars_url2md::url::extract_urls_from_text;

// Extract from plain text
let text = "Visit https://example.com and https://rust-lang.org";
let urls = extract_urls_from_text(text, None);
assert_eq!(urls.len(), 2);

// Extract from HTML with base URL
let html = r#"<a href="/docs">Documentation</a> <a href="https://external.com">External</a>"#;
let urls = extract_urls_from_text(html, Some("https://mysite.com"));
// Results: ["https://mysite.com/docs", "https://external.com"]
```

#### `validate_url`

Validate a single URL:

```rust
pub fn validate_url(url: &str) -> Result<String, anyhow::Error>
```

**Example:**
```rust
use twars_url2md::url::validate_url;

assert!(validate_url("https://example.com").is_ok());
assert!(validate_url("invalid-url").is_err());
assert!(validate_url("ftp://example.com").is_err()); // Only HTTP/HTTPS allowed
```

### Path Generation

#### `url_to_file_path`

Generate output file path from URL:

```rust
pub fn url_to_file_path(url: &str, base_path: &Path) -> PathBuf
```

**Example:**
```rust
use twars_url2md::url::url_to_file_path;
use std::path::PathBuf;

let url = "https://doc.rust-lang.org/book/ch01-01-installation.html";
let base = PathBuf::from("output");
let path = url_to_file_path(url, &base);

// Result: output/doc.rust-lang.org/book/ch01-01-installation.md
assert_eq!(
    path,
    PathBuf::from("output/doc.rust-lang.org/book/ch01-01-installation.md")
);
```

## HTML Processing

### Content Fetching

#### `fetch_and_clean_html`

Fetch and clean HTML content from URL:

```rust
pub async fn fetch_and_clean_html(url: &str) -> Result<String, anyhow::Error>
```

**Example:**
```rust
use twars_url2md::html::fetch_and_clean_html;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let cleaned_html = fetch_and_clean_html("https://example.com").await?;
    println!("Cleaned HTML length: {}", cleaned_html.len());
    Ok(())
}
```

#### `fetch_html_content`

Fetch raw HTML content:

```rust
pub async fn fetch_html_content(url: &str) -> Result<String, anyhow::Error>
```

### Local File Processing

#### `process_local_file`

Process local HTML file:

```rust
pub async fn process_local_file(file_path: &str) -> Result<String, anyhow::Error>
```

**Example:**
```rust
use twars_url2md::html::process_local_file;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let content = process_local_file("/path/to/document.html").await?;
    println!("Processed content: {}", content);
    Ok(())
}
```

## Markdown Conversion

### HTML to Markdown

#### `html_to_markdown`

Convert HTML to Markdown:

```rust
pub fn html_to_markdown(html: &str) -> Result<String, anyhow::Error>
```

**Example:**
```rust
use twars_url2md::markdown::html_to_markdown;

let html = r#"
    <h1>Title</h1>
    <p>This is <strong>bold</strong> text with a 
    <a href="https://example.com">link</a>.</p>
    <ul><li>Item 1</li><li>Item 2</li></ul>
"#;

let markdown = html_to_markdown(html)?;
println!("{}", markdown);
// Output:
// # Title
// 
// This is **bold** text with a [link](https://example.com).
// 
// - Item 1
// - Item 2
```

## Advanced Usage

### Custom Processing Pipeline

Create a custom processing pipeline:

```rust
use twars_url2md::{url, html, markdown};
use std::path::PathBuf;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // Step 1: Extract URLs from text
    let text = std::fs::read_to_string("input.html")?;
    let urls = url::extract_urls_from_text(&text, Some("https://base.com"));
    
    // Step 2: Process each URL
    for url in urls {
        // Fetch and clean HTML
        let cleaned_html = html::fetch_and_clean_html(&url).await?;
        
        // Convert to Markdown
        let markdown = markdown::html_to_markdown(&cleaned_html)?;
        
        // Generate output path
        let output_path = url::url_to_file_path(&url, &PathBuf::from("output"));
        
        // Create directory if needed
        if let Some(parent) = output_path.parent() {
            tokio::fs::create_dir_all(parent).await?;
        }
        
        // Write output
        tokio::fs::write(&output_path, markdown).await?;
        
        println!("Processed: {} -> {}", url, output_path.display());
    }
    
    Ok(())
}
```

### Concurrent Processing with Custom Control

Implement custom concurrency control:

```rust
use twars_url2md::{html, markdown, url};
use futures::stream::{self, StreamExt};
use std::sync::Arc;
use tokio::sync::Semaphore;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let urls = vec![
        "https://example.com/page1".to_string(),
        "https://example.com/page2".to_string(),
        "https://example.com/page3".to_string(),
    ];
    
    // Limit concurrent requests
    let semaphore = Arc::new(Semaphore::new(3));
    
    let results = stream::iter(urls)
        .map(|url| {
            let semaphore = semaphore.clone();
            async move {
                let _permit = semaphore.acquire().await.unwrap();
                process_single_url(&url).await
            }
        })
        .buffer_unordered(10)
        .collect::<Vec<_>>()
        .await;
    
    for result in results {
        match result {
            Ok(path) => println!("Success: {}", path),
            Err(e) => eprintln!("Error: {}", e),
        }
    }
    
    Ok(())
}

async fn process_single_url(url: &str) -> anyhow::Result<String> {
    let html = html::fetch_and_clean_html(url).await?;
    let markdown = markdown::html_to_markdown(&html)?;
    
    let output_path = url::url_to_file_path(url, &std::path::PathBuf::from("output"));
    
    if let Some(parent) = output_path.parent() {
        tokio::fs::create_dir_all(parent).await?;
    }
    
    tokio::fs::write(&output_path, markdown).await?;
    
    Ok(output_path.to_string_lossy().to_string())
}
```

### Error Handling Patterns

Comprehensive error handling:

```rust
use twars_url2md::{process_urls, Config};
use anyhow::{Context, Result};
use std::path::PathBuf;

#[derive(Debug)]
struct ProcessingStats {
    total_urls: usize,
    successful: usize,
    failed: usize,
    errors: Vec<(String, String)>,
}

#[tokio::main]
async fn main() -> Result<()> {
    let urls = load_urls_from_file("urls.txt")
        .context("Failed to load URLs from file")?;
    
    let config = Config {
        output_base: PathBuf::from("./output"),
        verbose: true,
        max_retries: 3,
        single_file: false,
        has_output: true,
        pack_file: Some(PathBuf::from("archive.md")),
    };
    
    let stats = process_with_stats(urls, config).await?;
    
    print_processing_report(&stats);
    
    if stats.failed > 0 {
        std::process::exit(1);
    }
    
    Ok(())
}

async fn process_with_stats(urls: Vec<String>, config: Config) -> Result<ProcessingStats> {
    let total_urls = urls.len();
    
    let errors = process_urls(urls, config).await
        .context("Critical error during URL processing")?;
    
    let failed = errors.len();
    let successful = total_urls - failed;
    
    let error_details = errors
        .into_iter()
        .map(|(url, error)| (url, error.to_string()))
        .collect();
    
    Ok(ProcessingStats {
        total_urls,
        successful,
        failed,
        errors: error_details,
    })
}

fn load_urls_from_file(path: &str) -> Result<Vec<String>> {
    let content = std::fs::read_to_string(path)
        .with_context(|| format!("Failed to read file: {}", path))?;
    
    let urls = content
        .lines()
        .map(|line| line.trim())
        .filter(|line| !line.is_empty() && !line.starts_with('#'))
        .map(|line| line.to_string())
        .collect();
    
    Ok(urls)
}

fn print_processing_report(stats: &ProcessingStats) {
    println!("Processing Report:");
    println!("  Total URLs: {}", stats.total_urls);
    println!("  Successful: {}", stats.successful);
    println!("  Failed: {}", stats.failed);
    
    if !stats.errors.is_empty() {
        println!("\nFailed URLs:");
        for (url, error) in &stats.errors {
            println!("  {}: {}", url, error);
        }
    }
}
```

## Library Integration Examples

### Web Server Integration

Using with a web framework:

```rust
use axum::{
    extract::Json,
    http::StatusCode,
    response::Json as ResponseJson,
    routing::post,
    Router,
};
use serde::{Deserialize, Serialize};
use twars_url2md::{process_urls, Config};
use std::path::PathBuf;

#[derive(Deserialize)]
struct ConvertRequest {
    urls: Vec<String>,
    output_dir: Option<String>,
    pack_output: Option<bool>,
}

#[derive(Serialize)]
struct ConvertResponse {
    success: bool,
    message: String,
    failed_urls: Vec<String>,
}

async fn convert_urls(
    Json(request): Json<ConvertRequest>,
) -> Result<ResponseJson<ConvertResponse>, StatusCode> {
    let output_dir = request.output_dir
        .unwrap_or_else(|| format!("output_{}", chrono::Utc::now().timestamp()));
    
    let pack_file = if request.pack_output.unwrap_or(false) {
        Some(PathBuf::from(format!("{}/archive.md", output_dir)))
    } else {
        None
    };
    
    let config = Config {
        output_base: PathBuf::from(&output_dir),
        verbose: false,
        max_retries: 2,
        single_file: false,
        has_output: true,
        pack_file,
    };
    
    match process_urls(request.urls, config).await {
        Ok(errors) => {
            let failed_urls: Vec<String> = errors.into_iter().map(|(url, _)| url).collect();
            let success = failed_urls.is_empty();
            
            Ok(ResponseJson(ConvertResponse {
                success,
                message: if success {
                    format!("All URLs processed successfully. Output: {}", output_dir)
                } else {
                    format!("Some URLs failed. Output: {}", output_dir)
                },
                failed_urls,
            }))
        }
        Err(e) => {
            eprintln!("Critical error: {}", e);
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        }
    }
}

#[tokio::main]
async fn main() {
    let app = Router::new()
        .route("/convert", post(convert_urls));
    
    let listener = tokio::net::TcpListener::bind("0.0.0.0:3000")
        .await
        .unwrap();
    
    axum::serve(listener, app).await.unwrap();
}
```

### CLI Tool Integration

Building a CLI tool with the library:

```rust
use clap::{Arg, Command};
use twars_url2md::{process_urls, Config, url::extract_urls_from_text};
use std::path::PathBuf;
use anyhow::Result;

#[tokio::main]
async fn main() -> Result<()> {
    let matches = Command::new("my-converter")
        .arg(Arg::new("input")
            .short('i')
            .long("input")
            .value_name("FILE")
            .help("Input file with URLs"))
        .arg(Arg::new("output")
            .short('o')  
            .long("output")
            .value_name("PATH")
            .help("Output directory"))
        .arg(Arg::new("extract")
            .long("extract")
            .help("Extract URLs from HTML/Markdown content"))
        .get_matches();
    
    let input_file = matches.get_one::<String>("input").unwrap();
    let output_dir = matches.get_one::<String>("output")
        .map(PathBuf::from)
        .unwrap_or_else(|| PathBuf::from("output"));
    
    let content = tokio::fs::read_to_string(input_file).await?;
    
    let urls = if matches.get_flag("extract") {
        extract_urls_from_text(&content, None)
    } else {
        content.lines()
            .map(|line| line.trim().to_string())
            .filter(|line| !line.is_empty())
            .collect()
    };
    
    let config = Config {
        output_base: output_dir,
        verbose: true,
        ..Default::default()
    };
    
    let errors = process_urls(urls, config).await?;
    
    if !errors.is_empty() {
        eprintln!("Some URLs failed to process:");
        for (url, error) in errors {
            eprintln!("  {}: {}", url, error);
        }
        std::process::exit(1);
    }
    
    println!("All URLs processed successfully!");
    Ok(())
}
```

---

!!! tip "API Best Practices"
    - Always handle the `Result` types properly
    - Use structured error handling with `anyhow`
    - Implement proper async patterns with Tokio
    - Consider implementing custom `Config` builders for complex scenarios
    - Use the `verbose` flag during development for debugging

!!! note "Performance Considerations"
    - The library handles concurrency automatically
    - For very large URL lists, consider processing in batches
    - Monitor memory usage with large concurrent operations
    - Use the retry mechanism appropriately for your use case
</document_content>
</document>

<document index="29">
<source>src_docs/md/architecture.md</source>
<document_content>
# Architecture

Deep dive into the technical architecture, design decisions, and implementation details of `twars-url2md`.

## System Overview

`twars-url2md` is built with a modular, async-first architecture in Rust, emphasizing performance, reliability, and maintainability.

<div class="arch-diagram">
```mermaid
graph TB
    CLI[CLI Interface<br/>src/cli.rs] --> URLExt[URL Extractor<br/>src/url.rs]
    CLI --> Lib[Core Library<br/>src/lib.rs]
    
    URLExt --> Valid[URL Validation<br/>& Normalization]
    Valid --> HTTP[HTTP Client<br/>src/html.rs]
    
    HTTP --> Mono[Monolith<br/>HTML Cleaner]
    Mono --> Conv[Markdown Converter<br/>src/markdown.rs]
    
    Conv --> Output[Output Writer<br/>File System]
    
    Lib --> Async[Tokio Runtime<br/>Async Orchestration]
    Async --> Conc[Concurrent Processing<br/>Buffer Unordered]
    
    Error[Error Handler<br/>anyhow + tracing] --> All[All Components]
    
    style CLI fill:#e1f5fe
    style HTTP fill:#f3e5f5
    style Mono fill:#e8f5e8
    style Conv fill:#fff3e0
    style Output fill:#fce4ec
```
</div>

## Core Components

### 1. CLI Interface (`src/cli.rs`)

Command-line argument parsing and user interaction layer.

**Key Responsibilities:**
- Argument parsing with `clap` derive macros
- Input validation and error handling
- Configuration setup and environment detection
- User-friendly error messages

**Design Patterns:**
```rust
#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
pub struct Args {
    #[arg(help = "URLs to process")]
    pub urls: Vec<String>,
    
    #[arg(short, long, help = "Input file containing URLs")]
    pub input: Option<PathBuf>,
    
    #[arg(short, long, help = "Output directory or file")]
    pub output: Option<PathBuf>,
    
    // ... additional arguments
}
```

### 2. URL Processing (`src/url.rs`)

URL extraction, validation, and normalization engine.

**Architecture Features:**
- **Multi-format URL extraction** using `linkify` crate
- **Base URL resolution** for relative links
- **URL validation and deduplication**
- **Output path generation** from URL structure

**Processing Pipeline:**
```rust
pub fn extract_urls_from_text(text: &str, base_url: Option<&str>) -> Vec<String> {
    // 1. Extract URLs using linkify
    let mut urls = extract_raw_urls(text);
    
    // 2. Extract from HTML/Markdown if present
    urls.extend(extract_from_html(text));
    urls.extend(extract_from_markdown(text));
    
    // 3. Resolve relative URLs
    if let Some(base) = base_url {
        urls = resolve_relative_urls(urls, base);
    }
    
    // 4. Validate and deduplicate
    validate_and_deduplicate(urls)
}
```

### 3. HTTP Client (`src/html.rs`)

Robust HTTP client built on `curl` for maximum compatibility.

**CDN Compatibility Features:**
- **Browser-like User-Agent** to avoid bot detection
- **HTTP/2 auto-negotiation** preferred by modern CDNs
- **Comprehensive header set** mimicking real browsers
- **Connection pooling** for efficient resource usage

**Request Configuration:**
```rust
const USER_AGENT_STRING: &str = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \
    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36";

fn configure_request(easy: &mut Easy) -> Result<()> {
    easy.useragent(USER_AGENT_STRING)?;
    easy.http_version(HttpVersion::V2)?;
    easy.follow_location(true)?;
    easy.max_redirections(10)?;
    easy.timeout(Duration::from_secs(60))?;
    easy.connect_timeout(Duration::from_secs(20))?;
    
    // Browser-like headers
    let headers = vec![
        "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language: en-US,en;q=0.5",
        "Accept-Encoding: gzip, deflate",
        "Sec-Fetch-Dest: document",
        "Sec-Fetch-Mode: navigate",
        "Sec-Fetch-Site: none",
        "Upgrade-Insecure-Requests: 1",
    ];
    
    easy.http_headers(headers)?;
    Ok(())
}
```

### 4. HTML Processing Pipeline

Two-stage HTML cleaning and conversion process.

#### Stage 1: Monolith Integration
- **Content Extraction**: Removes scripts, styles, ads
- **Resource Handling**: Processes images and links
- **Panic Recovery**: Catches Monolith panics to prevent crashes

```rust
pub async fn fetch_and_clean_html(url: &str) -> Result<String> {
    // Fetch HTML content
    let html_content = fetch_html_content(url).await?;
    
    // Clean with Monolith (with panic recovery)
    let cleaned_html = std::panic::catch_unwind(|| {
        monolith::html::clean(&html_content, &url, &monolith_options())
    }).map_err(|_| anyhow!("HTML cleaning panicked"))?;
    
    Ok(cleaned_html)
}
```

#### Stage 2: Markdown Conversion
- **Semantic Preservation**: Maintains heading hierarchy
- **Link Processing**: Preserves and normalizes links
- **Structure Retention**: Tables, lists, code blocks

### 5. Async Processing Engine (`src/lib.rs`)

High-performance concurrent processing using Tokio.

**Concurrency Architecture:**
```rust
pub async fn process_urls(urls: Vec<String>, config: Config) -> Result<Vec<(String, Error)>> {
    let concurrency_limit = determine_optimal_concurrency();
    
    let results = futures::stream::iter(urls)
        .map(|url| process_single_url(url, &config))
        .buffer_unordered(concurrency_limit)
        .collect::<Vec<_>>()
        .await;
    
    handle_results(results)
}

fn determine_optimal_concurrency() -> usize {
    let cpu_count = num_cpus::get();
    std::cmp::min(cpu_count * 2, 16) // I/O bound, cap at 16
}
```

**Error Aggregation:**
- Individual URL failures don't stop batch processing
- Comprehensive error reporting at completion
- Retry logic with exponential backoff

### 6. Output Management

Flexible output system supporting multiple formats and structures.

**Output Modes:**
- **Directory Structure**: Mirrors URL hierarchy
- **Single File**: Concatenated content
- **Packed Format**: URL-separated sections

```rust
pub enum OutputMode {
    Directory { base_path: PathBuf },
    SingleFile { file_path: PathBuf },
    Packed { file_path: PathBuf },
    Combined { dir_path: PathBuf, pack_path: PathBuf },
}

impl OutputMode {
    pub async fn write_content(&self, url: &str, content: &str) -> Result<()> {
        match self {
            Self::Directory { base_path } => {
                let file_path = url_to_file_path(url, base_path);
                write_file_with_dirs(&file_path, content).await
            },
            Self::Packed { file_path } => {
                append_with_header(file_path, url, content).await
            },
            // ... other modes
        }
    }
}
```

## Performance Optimizations

### 1. Adaptive Concurrency

Dynamic concurrency adjustment based on system resources:
- **CPU Detection**: Uses `num_cpus` crate for core count
- **I/O Optimization**: Higher concurrency for I/O-bound operations
- **Resource Limits**: Caps maximum concurrent operations

### 2. Connection Reuse

HTTP client optimizations:
- **Connection Pooling**: Reuses connections for same domains
- **Keep-Alive**: Maintains connections between requests
- **HTTP/2 Multiplexing**: Multiple requests per connection

### 3. Memory Management

Efficient memory usage patterns:
- **Streaming Processing**: Avoids loading all content in memory
- **Lazy Evaluation**: Processes URLs on-demand
- **Resource Cleanup**: Proper cleanup of temporary resources

### 4. Build Optimizations

Compile-time optimizations for release builds:
```toml
[profile.release]
lto = true                # Link-time optimization
codegen-units = 1         # Single codegen unit for better optimization
panic = "unwind"          # Stack unwinding for panic recovery
strip = true              # Strip debug symbols
opt-level = 3             # Maximum optimization
```

## Error Handling Strategy

### Multi-Layer Error Handling

1. **Network Layer**: Connection timeouts, DNS failures
2. **Protocol Layer**: HTTP errors, redirects
3. **Content Layer**: HTML parsing, encoding issues
4. **File System Layer**: Permission errors, disk space

### Error Recovery Mechanisms

```rust
#[derive(Debug)]
pub enum ProcessingError {
    NetworkError { url: String, cause: String, retry_count: u32 },
    ParsingError { url: String, stage: String, cause: String },
    FileSystemError { path: PathBuf, operation: String, cause: String },
    ContentError { url: String, issue: String },
}

impl ProcessingError {
    pub fn is_retryable(&self) -> bool {
        matches!(self, 
            Self::NetworkError { retry_count, .. } if *retry_count < MAX_RETRIES
        )
    }
    
    pub fn should_skip(&self) -> bool {
        matches!(self, 
            Self::ContentError { .. } | 
            Self::ParsingError { .. }
        )
    }
}
```

### Panic Recovery

Specific panic recovery for the Monolith library:
```rust
pub fn safe_clean_html(html: &str, url: &str) -> Result<String> {
    std::panic::catch_unwind(|| {
        monolith::clean(html, url, &get_monolith_options())
    })
    .map_err(|_| anyhow!("HTML cleaning panicked for URL: {}", url))?
    .map_err(|e| anyhow!("Monolith error: {}", e))
}
```

## Logging and Observability

### Structured Logging

Using `tracing` crate for structured, contextual logging:
```rust
#[tracing::instrument(skip(content), fields(url = %url, content_size = content.len()))]
pub async fn process_content(url: &str, content: &str) -> Result<String> {
    tracing::info!("Starting content processing");
    
    let cleaned = clean_html(content).await
        .map_err(|e| {
            tracing::error!(error = %e, "HTML cleaning failed");
            e
        })?;
    
    tracing::debug!(cleaned_size = cleaned.len(), "HTML cleaning completed");
    
    let markdown = convert_to_markdown(&cleaned).await?;
    
    tracing::info!(
        markdown_size = markdown.len(),
        "Content processing completed successfully"
    );
    
    Ok(markdown)
}
```

### Performance Metrics

Built-in performance tracking:
- Request timing and throughput
- Memory usage monitoring  
- Error rate tracking
- Retry attempt statistics

## Security Considerations

### Input Validation

- **URL Validation**: Strict URL format checking
- **Path Traversal Prevention**: Safe output path generation
- **Content Size Limits**: Protection against memory exhaustion

### Network Security

- **TLS Verification**: Certificate validation enabled by default
- **Redirect Limits**: Maximum redirect count to prevent loops
- **Timeout Enforcement**: Request timeouts to prevent hanging

### File System Safety

```rust
pub fn safe_output_path(base: &Path, url: &str) -> Result<PathBuf> {
    let parsed_url = Url::parse(url)?;
    let host = parsed_url.host_str()
        .ok_or_else(|| anyhow!("Invalid host in URL"))?;
    
    // Sanitize path components
    let path_segments: Vec<String> = parsed_url
        .path_segments()
        .unwrap_or_default()
        .map(|s| sanitize_filename(s))
        .filter(|s| !s.is_empty() && s != "." && s != "..")
        .collect();
    
    let mut output_path = base.join(sanitize_filename(host));
    for segment in path_segments {
        output_path = output_path.join(segment);
    }
    
    // Ensure path is within base directory
    if !output_path.starts_with(base) {
        return Err(anyhow!("Generated path outside base directory"));
    }
    
    Ok(output_path.with_extension("md"))
}
```

## Build System

### Build Metadata Integration

Runtime build information embedded during compilation:
```rust
// build.rs
use built::write_built_file;

fn main() {
    write_built_file().expect("Failed to acquire build info");
}

// src/lib.rs  
pub fn version() -> String {
    format!(
        "{} (built {} for {})",
        built_info::PKG_VERSION,
        built_info::BUILT_TIME_UTC,
        built_info::TARGET
    )
}
```

### Cross-Platform Builds

Multi-target build configuration:
- **Linux**: x86_64, aarch64, musl variants
- **macOS**: Intel and Apple Silicon
- **Windows**: x86_64 MSVC

### Dependency Management

Strategic dependency choices:
- **Core Dependencies**: Minimal, well-maintained crates
- **Feature Flags**: Optional functionality to reduce binary size
- **Version Pinning**: Careful balance between updates and stability

## Testing Architecture

### Test Structure

```
tests/
â”œâ”€â”€ unit/           # Unit tests for individual components
â”œâ”€â”€ integration/    # Integration tests for workflows  
â”œâ”€â”€ fixtures/       # Test data and expected outputs
â””â”€â”€ benchmarks/     # Performance benchmarks
```

### Test Categories

1. **Unit Tests**: Individual component functionality
2. **Integration Tests**: End-to-end workflow testing
3. **Performance Tests**: Benchmark critical paths
4. **Compatibility Tests**: Different input formats and edge cases

---

!!! note "Design Philosophy"
    The architecture prioritizes:
    
    - **Reliability**: Graceful error handling and recovery
    - **Performance**: Async processing and efficient resource usage  
    - **Maintainability**: Modular design with clear separation of concerns
    - **Extensibility**: Easy to add new input/output formats
    - **User Experience**: Informative feedback and intuitive behavior
</document_content>
</document>

<document index="30">
<source>src_docs/md/assets/extra.css</source>
<document_content>
/* Custom CSS for twars-url2md documentation */

/* Code block improvements */
.highlight pre {
    padding: 1em;
    border-radius: 0.3em;
    font-size: 0.85em;
}

/* Custom admonition styles */
.md-typeset .admonition.example {
    border-color: #448aff;
}

.md-typeset .admonition.example > .admonition-title {
    background-color: rgba(68, 138, 255, 0.1);
    border-color: #448aff;
}

.md-typeset .admonition.example > .admonition-title::before {
    background-color: #448aff;
    -webkit-mask-image: var(--md-admonition-icon--example);
    mask-image: var(--md-admonition-icon--example);
}

/* Command line styling */
.command-line {
    background-color: var(--md-code-bg-color);
    border-left: 4px solid var(--md-primary-fg-color);
    padding: 0.8em 1em;
    margin: 1em 0;
    font-family: var(--md-code-font);
    border-radius: 0.1em;
}

/* Performance metrics styling */
.performance-metric {
    display: inline-block;
    background: linear-gradient(45deg, #1976d2, #42a5f5);
    color: white;
    padding: 0.2em 0.6em;
    border-radius: 0.3em;
    font-weight: bold;
    font-size: 0.9em;
    margin: 0.1em;
}

/* Feature highlights */
.feature-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    gap: 1rem;
    margin: 1.5rem 0;
}

.feature-card {
    border: 1px solid var(--md-default-fg-color--lighter);
    border-radius: 0.5rem;
    padding: 1rem;
    background: var(--md-default-bg-color);
}

.feature-card h3 {
    margin-top: 0;
    color: var(--md-primary-fg-color);
}

/* Architecture diagram improvements */
.arch-diagram {
    text-align: center;
    margin: 2rem 0;
    padding: 1rem;
    background: var(--md-code-bg-color);
    border-radius: 0.5rem;
}

/* Responsive improvements */
@media (max-width: 768px) {
    .feature-grid {
        grid-template-columns: 1fr;
    }
    
    .performance-metric {
        display: block;
        margin: 0.2em 0;
        text-align: center;
    }
}
</document_content>
</document>

<document index="31">
<source>src_docs/md/configuration.md</source>
<document_content>
# Configuration

Customize `twars-url2md` behavior through command-line options, environment variables, and advanced configuration techniques.

## Runtime Configuration

### Logging Configuration

Control logging output with environment variables and command-line flags:

#### Basic Logging

```bash
# Enable verbose mode (INFO + DEBUG for twars-url2md)
twars-url2md -i urls.txt -o output/ -v

# Equivalent environment variable
RUST_LOG=info,twars_url2md=debug twars-url2md -i urls.txt -o output/
```

#### Advanced Logging Control

```bash
# Maximum verbosity for debugging
RUST_LOG=trace twars-url2md -i urls.txt -o output/

# Module-specific logging
RUST_LOG=twars_url2md::html=debug,twars_url2md::markdown=info twars-url2md -i urls.txt -o output/

# Focus on specific components
RUST_LOG=twars_url2md=debug,reqwest=info,curl=warn twars-url2md -i urls.txt -o output/
```

#### Log Format Customization

```bash
# Structured JSON logging (for log analysis tools)
RUST_LOG=info RUST_LOG_FORMAT=json twars-url2md -i urls.txt -o output/ -v

# Minimal logging output
RUST_LOG=error twars-url2md -i urls.txt -o output/

# Log to file
twars-url2md -i urls.txt -o output/ -v 2> conversion.log
```

### Retry Configuration

While retry counts aren't directly configurable via CLI, you can implement custom retry logic:

```bash
#!/bin/bash
# Custom retry wrapper

retry_command() {
    local max_attempts=5
    local delay=2
    local attempt=1
    
    while ((attempt <= max_attempts)); do
        if twars-url2md "$@"; then
            return 0
        fi
        
        echo "Attempt $attempt failed, retrying in ${delay}s..."
        sleep $delay
        delay=$((delay * 2))  # Exponential backoff
        ((attempt++))
    done
    
    echo "All $max_attempts attempts failed"
    return 1
}

# Usage
retry_command -i urls.txt -o output/ -v
```

## Environment Variables

### System Environment

Key environment variables that affect behavior:

| Variable | Purpose | Example |
|----------|---------|---------|
| `RUST_LOG` | Logging configuration | `RUST_LOG=debug` |
| `RUST_BACKTRACE` | Error backtraces | `RUST_BACKTRACE=1` |
| `HTTP_PROXY` | HTTP proxy settings | `HTTP_PROXY=http://proxy:8080` |
| `HTTPS_PROXY` | HTTPS proxy settings | `HTTPS_PROXY=http://proxy:8080` |
| `NO_PROXY` | Proxy bypass list | `NO_PROXY=localhost,127.0.0.1` |

### Proxy Configuration

For corporate or restricted environments:

```bash
# HTTP proxy
export HTTP_PROXY=http://proxy.company.com:8080
export HTTPS_PROXY=http://proxy.company.com:8080
export NO_PROXY=localhost,127.0.0.1,*.local

twars-url2md -i urls.txt -o output/ -v

# Authenticated proxy
export HTTP_PROXY=http://username:password@proxy.company.com:8080
twars-url2md -i urls.txt -o output/ -v

# SOCKS proxy (if curl supports it)
export ALL_PROXY=socks5://proxy.company.com:1080
twars-url2md -i urls.txt -o output/ -v
```

### Debug Configuration

Enhanced debugging for troubleshooting:

```bash
# Full debug mode with backtraces
RUST_LOG=trace RUST_BACKTRACE=full twars-url2md -i problematic_urls.txt -o debug_output/ -v

# Memory debugging (if compiled with debug info)
RUST_LOG=debug RUST_BACKTRACE=1 valgrind --tool=memcheck twars-url2md -i urls.txt -o output/
```

## Configuration Files

### URL Configuration Files

Organize URLs with comments and metadata:

```txt
# urls.txt - Main documentation URLs
https://doc.rust-lang.org/book/
https://doc.rust-lang.org/std/
https://doc.rust-lang.org/cargo/

# API documentation  
https://docs.rs/serde/
https://docs.rs/tokio/
https://docs.rs/clap/

# Community resources
https://forge.rust-lang.org/
https://rustc-dev-guide.rust-lang.org/
```

### Batch Configuration Scripts

Create reusable configuration scripts:

```bash
#!/bin/bash
# config/rust-docs.sh - Rust documentation collection

set -euo pipefail

# Configuration
BASE_OUTPUT="rust_documentation"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT_DIR="${BASE_OUTPUT}_${TIMESTAMP}"
PACK_FILE="${BASE_OUTPUT}_complete_${TIMESTAMP}.md"

# Logging configuration
export RUST_LOG=info,twars_url2md=debug

# URL categories
CORE_URLS="config/rust_core_urls.txt"
CRATE_URLS="config/rust_crate_urls.txt"
COMMUNITY_URLS="config/rust_community_urls.txt"

# Processing function
process_category() {
    local name="$1"
    local url_file="$2"
    local output_subdir="$3"
    
    echo "Processing $name URLs..."
    mkdir -p "$OUTPUT_DIR/$output_subdir"
    
    if twars-url2md -i "$url_file" -o "$OUTPUT_DIR/$output_subdir/" -v; then
        echo "âœ“ $name processing completed"
        return 0
    else
        echo "âœ— $name processing failed"
        return 1
    fi
}

# Main processing
main() {
    echo "Starting Rust documentation collection..."
    echo "Output directory: $OUTPUT_DIR"
    echo "Pack file: $PACK_FILE"
    
    # Process categories
    process_category "Core" "$CORE_URLS" "core"
    process_category "Crates" "$CRATE_URLS" "crates"  
    process_category "Community" "$COMMUNITY_URLS" "community"
    
    # Create combined archive
    echo "Creating combined archive..."
    find "$OUTPUT_DIR" -name "*.md" | \
        twars-url2md --stdin --pack "$PACK_FILE" -v
    
    echo "Collection completed successfully!"
    echo "Individual files: $OUTPUT_DIR/"
    echo "Combined archive: $PACK_FILE"
}

main "$@"
```

### Environment-Specific Configurations

=== "Development"
    ```bash
    #!/bin/bash
    # config/dev.sh
    
    export RUST_LOG=debug
    export RUST_BACKTRACE=1
    
    # Use local test URLs
    twars-url2md -i test_urls.txt -o dev_output/ -v
    ```

=== "Production"
    ```bash
    #!/bin/bash
    # config/prod.sh
    
    export RUST_LOG=info
    
    # Production processing with error handling
    if ! twars-url2md -i prod_urls.txt -o prod_output/ --pack prod_archive.md -v; then
        echo "Production processing failed" >&2
        exit 1
    fi
    ```

=== "CI/CD"
    ```bash
    #!/bin/bash
    # config/ci.sh
    
    set -euo pipefail
    
    export RUST_LOG=info,twars_url2md=debug
    
    # CI-specific output paths
    OUTPUT_DIR="${CI_PROJECT_DIR}/generated_docs"
    ARTIFACT_FILE="${CI_PROJECT_DIR}/docs_archive_${CI_COMMIT_SHA}.md"
    
    mkdir -p "$OUTPUT_DIR"
    
    twars-url2md -i ci_urls.txt -o "$OUTPUT_DIR/" --pack "$ARTIFACT_FILE" -v
    ```

## Performance Tuning

### Concurrency Configuration

While not directly configurable, you can influence concurrency behavior:

```bash
# Limit system resources to control concurrency
ulimit -n 1024  # Limit file descriptors
ulimit -u 512   # Limit processes

# Use nice to lower CPU priority for background processing
nice -n 10 twars-url2md -i large_url_list.txt -o output/ -v

# Process during off-peak hours
echo "0 2 * * * /path/to/twars-url2md -i /path/to/urls.txt -o /path/to/output/ -v" | crontab -
```

### Memory Management

```bash
# Monitor memory usage during processing
{
    twars-url2md -i urls.txt -o output/ -v &
    PID=$!
    
    while kill -0 $PID 2>/dev/null; do
        ps -p $PID -o pid,vsz,rss,pcpu
        sleep 10
    done
    
    wait $PID
} | tee memory_usage.log
```

### Network Configuration

```bash
# Timeout configuration (system-level)
# These affect the underlying curl library

export CURL_CA_BUNDLE=/path/to/custom/ca-bundle.crt  # Custom CA bundle
export SSL_CERT_FILE=/path/to/custom/ca-bundle.crt   # Alternative CA bundle

# Corporate firewall handling
export http_proxy=http://proxy.company.com:8080
export https_proxy=http://proxy.company.com:8080
export no_proxy=localhost,127.0.0.1,*.company.com

twars-url2md -i corporate_urls.txt -o output/ -v
```

## Output Customization

### Directory Structure Templates

Customize output organization:

```bash
#!/bin/bash
# Custom output organization

process_by_domain() {
    local url_file="$1"
    
    # Group URLs by domain
    awk -F'/' '{print $3}' "$url_file" | sort -u | while read -r domain; do
        grep "$domain" "$url_file" > "urls_${domain}.txt"
        mkdir -p "output_by_domain/$domain"
        twars-url2md -i "urls_${domain}.txt" -o "output_by_domain/$domain/" -v
        rm "urls_${domain}.txt"
    done
}

process_by_domain "all_urls.txt"
```

### Content Processing Templates

```bash
#!/bin/bash
# Content post-processing

# Standard processing
twars-url2md -i urls.txt -o raw_output/ -v

# Post-process markdown files
find raw_output -name "*.md" | while read -r file; do
    # Add custom header
    {
        echo "---"
        echo "generated: $(date -Iseconds)"
        echo "source: twars-url2md"
        echo "---"
        echo ""
        cat "$file"
    } > "${file}.tmp" && mv "${file}.tmp" "$file"
done

# Create index
{
    echo "# Generated Documentation Index"
    echo ""
    echo "Generated on: $(date)"
    echo ""
    find raw_output -name "*.md" | sort | while read -r file; do
        title=$(head -n 10 "$file" | grep -E '^#' | head -n 1 | sed 's/^# *//')
        echo "- [$title]($file)"
    done
} > raw_output/INDEX.md
```

## Integration Configurations

### Docker Configuration

```dockerfile
# Dockerfile for configured environment
FROM ubuntu:22.04

# Install dependencies
RUN apt-get update && apt-get install -y \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Install twars-url2md
RUN curl -fsSL https://raw.githubusercontent.com/twardoch/twars-url2md/main/install.sh | bash

# Set environment
ENV RUST_LOG=info
ENV PATH=/root/.local/bin:$PATH

# Copy configuration
COPY config/ /app/config/
WORKDIR /app

# Default command
CMD ["twars-url2md", "-i", "config/urls.txt", "-o", "output/", "-v"]
```

### Kubernetes Configuration

```yaml
# k8s-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: url-to-md-processor
spec:
  template:
    spec:
      containers:
      - name: processor
        image: twars-url2md:latest
        env:
        - name: RUST_LOG
          value: "info,twars_url2md=debug"
        - name: HTTP_PROXY
          valueFrom:
            configMapKeyRef:
              name: proxy-config
              key: http_proxy
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
        - name: output-volume
          mountPath: /app/output
        command: ["twars-url2md"]
        args: ["-i", "/app/config/urls.txt", "-o", "/app/output/", "-v"]
      volumes:
      - name: config-volume
        configMap:
          name: url-config
      - name: output-volume
        persistentVolumeClaim:
          claimName: output-pvc
      restartPolicy: Never
```

### Systemd Service Configuration

```ini
# /etc/systemd/system/url-processor.service
[Unit]
Description=URL to Markdown Processor
After=network.target

[Service]
Type=oneshot
User=processor
Group=processor
Environment=RUST_LOG=info,twars_url2md=debug
ExecStart=/usr/local/bin/twars-url2md -i /etc/url-processor/urls.txt -o /var/lib/url-processor/output/ -v
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

```bash
# Timer for periodic processing
# /etc/systemd/system/url-processor.timer
[Unit]
Description=Run URL processor daily
Requires=url-processor.service

[Timer]
OnCalendar=daily
Persistent=true

[Install]
WantedBy=timers.target
```

## Troubleshooting Configuration

### Debug Configuration

```bash
#!/bin/bash
# debug_config.sh - Comprehensive debugging

set -x  # Enable command tracing

# Environment debugging
echo "=== Environment ==="
env | grep -E "(RUST|HTTP|PROXY)" | sort

echo "=== System Information ==="
uname -a
which twars-url2md
twars-url2md --version

echo "=== Network Connectivity ==="
curl -I https://httpbin.org/get

echo "=== Processing Test ==="
RUST_LOG=trace RUST_BACKTRACE=full \
    twars-url2md https://httpbin.org/html -o debug_test/ -v

echo "=== Debug Results ==="
find debug_test -type f -exec ls -la {} \;
find debug_test -name "*.md" -exec head -5 {} \;
```

### Configuration Validation

```bash
#!/bin/bash
# validate_config.sh

validate_urls() {
    local url_file="$1"
    
    echo "Validating URLs in $url_file..."
    
    while IFS= read -r url; do
        # Skip comments and empty lines
        [[ "$url" =~ ^[[:space:]]*# ]] && continue
        [[ -z "${url// }" ]] && continue
        
        if curl -s --head "$url" | head -n 1 | grep -q "200 OK"; then
            echo "âœ“ $url"
        else
            echo "âœ— $url"
        fi
    done < "$url_file"
}

validate_environment() {
    echo "Validating environment..."
    
    # Check binary
    if command -v twars-url2md >/dev/null 2>&1; then
        echo "âœ“ twars-url2md binary found"
    else
        echo "âœ— twars-url2md binary not found in PATH"
        return 1
    fi
    
    # Check version
    twars-url2md --version
    
    # Check network
    if curl -s https://httpbin.org/get >/dev/null; then
        echo "âœ“ Network connectivity OK"
    else
        echo "âœ— Network connectivity failed"
        return 1
    fi
}

# Run validations
validate_environment
validate_urls "$1"
```

---

!!! tip "Configuration Best Practices"
    - Use environment variables for sensitive data (proxies, credentials)
    - Create reusable configuration scripts for common workflows
    - Test configurations in development before production deployment
    - Document custom configurations for team members
    - Use version control for configuration files and scripts
    - Monitor resource usage and adjust configurations accordingly
</document_content>
</document>

<document index="32">
<source>src_docs/md/contributing.md</source>
<document_content>
# Contributing

We welcome contributions to `twars-url2md`! This guide will help you get started with development, understand our workflow, and make meaningful contributions.

## Getting Started

### Development Environment Setup

1. **Prerequisites**
   ```bash
   # Install Rust (1.70.0 or later)
   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
   source $HOME/.cargo/env
   
   # Verify installation
   rustc --version
   cargo --version
   ```

2. **Clone and Build**
   ```bash
   git clone https://github.com/twardoch/twars-url2md.git
   cd twars-url2md
   
   # Build in debug mode
   cargo build
   
   # Run tests
   cargo test --all-features
   
   # Check formatting and linting
   cargo fmt --check
   cargo clippy --all-targets --all-features
   ```

3. **Development Tools**
   ```bash
   # Install additional tools
   rustup component add rustfmt clippy
   cargo install cargo-audit cargo-watch
   
   # For continuous testing during development
   cargo watch -x test
   ```

### Project Structure

Understanding the codebase organization:

```
src/
â”œâ”€â”€ main.rs          # Entry point with panic handling
â”œâ”€â”€ lib.rs           # Core library and orchestration
â”œâ”€â”€ cli.rs           # Command-line interface
â”œâ”€â”€ url.rs           # URL extraction and validation
â”œâ”€â”€ html.rs          # HTTP client and HTML fetching
â”œâ”€â”€ markdown.rs      # HTML to Markdown conversion
â””â”€â”€ tests.rs         # Unit tests

tests/
â”œâ”€â”€ integration/     # Integration tests
â”œâ”€â”€ fixtures/        # Test data and expected outputs
â””â”€â”€ benchmarks/      # Performance benchmarks

.github/
â””â”€â”€ workflows/       # CI/CD configuration

scripts/             # Build and release scripts
docs/               # Generated documentation
```

## Development Workflow

### Making Changes

1. **Create a Feature Branch**
   ```bash
   git checkout -b feature/your-feature-name
   # or
   git checkout -b fix/issue-description
   ```

2. **Make Your Changes**
   - Follow the coding standards (see below)
   - Add tests for new functionality
   - Update documentation as needed

3. **Test Your Changes**
   ```bash
   # Run all tests
   cargo test --all-features
   
   # Run specific test
   cargo test test_url_extraction
   
   # Run integration tests
   cargo test --test integration
   
   # Check formatting
   cargo fmt --check
   
   # Run linter
   cargo clippy --all-targets --all-features
   ```

4. **Commit and Push**
   ```bash
   git add .
   git commit -m "feat: add new URL extraction feature"
   git push origin feature/your-feature-name
   ```

5. **Create Pull Request**
   - Use the GitHub web interface
   - Fill out the PR template
   - Link any related issues

### Commit Message Convention

We use conventional commits for clear history:

- `feat:` - New features
- `fix:` - Bug fixes  
- `docs:` - Documentation changes
- `style:` - Code style changes (formatting, etc.)
- `refactor:` - Code refactoring
- `test:` - Adding or updating tests
- `chore:` - Maintenance tasks

Examples:
```
feat: add support for custom User-Agent headers
fix: handle malformed URLs gracefully
docs: update installation instructions
test: add integration tests for packed output
```

## Coding Standards

### Rust Code Style

Follow standard Rust conventions with these specifics:

1. **Formatting**
   ```bash
   # Use rustfmt with default settings
   cargo fmt
   ```

2. **Naming Conventions**
   ```rust
   // Functions and variables: snake_case
   fn process_urls() {}
   let url_count = 0;
   
   // Types and traits: PascalCase
   struct UrlProcessor {}
   trait ContentConverter {}
   
   // Constants: SCREAMING_SNAKE_CASE
   const MAX_RETRIES: u32 = 3;
   ```

3. **Error Handling**
   ```rust
   // Use anyhow::Result for application errors
   use anyhow::{Result, Context};
   
   fn process_url(url: &str) -> Result<String> {
       fetch_content(url)
           .with_context(|| format!("Failed to fetch URL: {}", url))?;
       // ...
   }
   ```

4. **Documentation**
   ```rust
   /// Processes a URL and converts it to Markdown.
   /// 
   /// # Arguments
   /// 
   /// * `url` - The URL to process
   /// * `config` - Processing configuration
   /// 
   /// # Returns
   /// 
   /// The converted Markdown content or an error if processing fails.
   /// 
   /// # Example
   /// 
   /// ```
   /// let markdown = process_url("https://example.com", &config)?;
   /// ```
   pub fn process_url(url: &str, config: &Config) -> Result<String> {
       // Implementation
   }
   ```

### Code Organization

1. **Module Structure**
   - Keep modules focused and cohesive
   - Use `pub(crate)` for internal APIs
   - Document public APIs thoroughly

2. **Error Types**
   ```rust
   // Use thiserror for custom error types
   use thiserror::Error;
   
   #[derive(Error, Debug)]
   pub enum ProcessingError {
       #[error("Network error: {0}")]
       Network(#[from] reqwest::Error),
       
       #[error("Invalid URL: {url}")]
       InvalidUrl { url: String },
   }
   ```

3. **Async Code**
   ```rust
   // Use proper async/await patterns
   pub async fn fetch_multiple_urls(urls: Vec<String>) -> Result<Vec<String>> {
       let futures = urls.into_iter()
           .map(|url| fetch_single_url(url));
       
       let results = futures::future::try_join_all(futures).await?;
       Ok(results)
   }
   ```

## Testing Guidelines

### Test Categories

1. **Unit Tests** (`src/tests.rs`)
   ```rust
   #[cfg(test)]
   mod tests {
       use super::*;
       
       #[test]
       fn test_url_extraction() {
           let text = "Visit https://example.com for more info";
           let urls = extract_urls_from_text(text, None);
           assert_eq!(urls.len(), 1);
           assert_eq!(urls[0], "https://example.com");
       }
       
       #[tokio::test]
       async fn test_async_processing() {
           let result = process_url("https://httpbin.org/html").await;
           assert!(result.is_ok());
       }
   }
   ```

2. **Integration Tests** (`tests/integration/`)
   ```rust
   use twars_url2md::{process_urls, Config};
   use tempfile::tempdir;
   
   #[tokio::test]
   async fn test_end_to_end_processing() {
       let temp_dir = tempdir().unwrap();
       let config = Config {
           output_base: temp_dir.path().to_path_buf(),
           verbose: false,
           // ... other config
       };
       
       let urls = vec!["https://httpbin.org/html".to_string()];
       let result = process_urls(urls, config).await;
       
       assert!(result.is_ok());
       // Verify output files exist
   }
   ```

3. **Benchmark Tests** (`tests/benchmarks/`)
   ```rust
   use criterion::{criterion_group, criterion_main, Criterion};
   
   fn benchmark_url_extraction(c: &mut Criterion) {
       let text = include_str!("../fixtures/large_html_file.html");
       
       c.bench_function("extract_urls", |b| {
           b.iter(|| extract_urls_from_text(text, None))
       });
   }
   
   criterion_group!(benches, benchmark_url_extraction);
   criterion_main!(benches);
   ```

### Testing Best Practices

- **Test both success and failure cases**
- **Use realistic test data** (fixtures)
- **Mock external dependencies** when appropriate
- **Write readable test names** that describe the scenario
- **Keep tests fast and independent**

## Contributing Areas

### High-Priority Areas

1. **Performance Optimizations**
   - Memory usage improvements
   - Faster HTML processing
   - Better concurrency patterns

2. **Content Processing**
   - Enhanced HTML cleaning rules
   - Better Markdown conversion quality
   - Support for more content types

3. **Error Handling**
   - More specific error messages
   - Better recovery strategies
   - Improved debugging information

4. **Platform Support**
   - Additional architecture support
   - Package manager integrations
   - Container optimizations

### Medium-Priority Areas

1. **Output Formats**
   - Additional export formats (AsciiDoc, reStructuredText)
   - Custom output templates
   - Metadata extraction and embedding

2. **Configuration**
   - Configuration file support
   - More granular control options
   - Environment-specific presets

3. **Monitoring and Observability**
   - Progress reporting improvements
   - Metrics collection
   - Better logging structured data

### Documentation Improvements

1. **User Documentation**
   - More usage examples
   - Troubleshooting guides
   - Video tutorials

2. **Developer Documentation**
   - Architecture deep dives
   - API documentation
   - Integration examples

3. **Community Resources**
   - FAQ compilation
   - Community showcase
   - Best practices guide

## Pull Request Process

### PR Checklist

Before submitting your PR, ensure:

- [ ] Code follows style guidelines (`cargo fmt`)
- [ ] All tests pass (`cargo test --all-features`)
- [ ] Linting passes (`cargo clippy --all-targets --all-features`)
- [ ] Documentation is updated
- [ ] New functionality includes tests
- [ ] Commit messages follow convention
- [ ] PR description explains changes clearly

### PR Template

```markdown
## Description
Brief description of changes made.

## Type of Change
- [ ] Bug fix (non-breaking change fixing an issue)
- [ ] New feature (non-breaking change adding functionality)
- [ ] Breaking change (fix or feature causing existing functionality to change)
- [ ] Documentation update

## Testing
Describe how you tested your changes:
- [ ] Unit tests added/updated
- [ ] Integration tests added/updated  
- [ ] Manual testing performed

## Related Issues
Closes #123, Addresses #456

## Additional Notes
Any additional context or considerations.
```

### Review Process

1. **Automated Checks**: CI runs tests and linting
2. **Code Review**: Maintainers review for quality and design
3. **Discussion**: Address feedback and make necessary changes
4. **Approval**: Once approved, PR will be merged

### After Your PR is Merged

- Your contribution will be included in the next release
- You'll be added to the contributors list
- Consider helping with code review for other contributors

## Release Process

### Version Management

We use semantic versioning (SemVer):
- **MAJOR**: Breaking changes
- **MINOR**: New features (backward compatible)
- **PATCH**: Bug fixes (backward compatible)

### Release Workflow

1. **Prepare Release**
   ```bash
   # Update version in Cargo.toml
   # Update CHANGELOG.md
   # Create release commit
   git commit -m "chore: release v1.2.3"
   ```

2. **Create Tag**
   ```bash
   git tag -a v1.2.3 -m "Release v1.2.3"
   git push origin v1.2.3
   ```

3. **Automated Release**
   - GitHub Actions builds binaries
   - Creates GitHub release
   - Publishes to crates.io

## Community Guidelines

### Code of Conduct

We follow the [Rust Code of Conduct](https://www.rust-lang.org/policies/code-of-conduct):

- Be friendly and welcoming
- Be patient and constructive
- Be respectful of different viewpoints
- Focus on what's best for the community

### Communication Channels

- **GitHub Issues**: Bug reports, feature requests
- **GitHub Discussions**: General questions, ideas
- **Pull Requests**: Code contributions and reviews

### Getting Help

- **Documentation**: Check docs for common questions
- **Issues**: Search existing issues first
- **Discussions**: Ask questions in GitHub Discussions
- **Code Review**: Request review from maintainers

---

!!! tip "First-Time Contributors"
    Look for issues labeled `good first issue` or `help wanted` to find beginner-friendly tasks. Don't hesitate to ask questions - we're here to help you succeed!

!!! note "Recognition"
    All contributors are recognized in our README and release notes. We appreciate every contribution, no matter how small!
</document_content>
</document>

<document index="33">
<source>src_docs/md/index.md</source>
<document_content>
# twars-url2md Documentation

[![Crates.io](https://img.shields.io/crates/v/twars-url2md)](https://crates.io/crates/twars-url2md)
[![Downloads](https://img.shields.io/crates/d/twars-url2md)](https://crates.io/crates/twars-url2md)
[![Documentation](https://docs.rs/twars-url2md/badge.svg)](https://docs.rs/twars-url2md)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![CI](https://github.com/twardoch/twars-url2md/actions/workflows/ci.yml/badge.svg)](https://github.com/twardoch/twars-url2md/actions)

## TL;DR

**`twars-url2md`** is a blazingly fast Rust CLI tool that converts web pages into clean, readable Markdown files. It handles batch processing, works with modern CDNs, and produces high-quality output perfect for documentation, archiving, or content conversion workflows.

!!! example "Quick Start"
    ```bash
    # Install via pre-compiled binary (recommended)
    curl -fsSL https://raw.githubusercontent.com/twardoch/twars-url2md/main/install.sh | bash
    
    # Convert a single URL
    twars-url2md https://example.com -o output/
    
    # Process multiple URLs from a file
    twars-url2md -i urls.txt -o markdown_files/
    
    # Pack multiple URLs into one file
    twars-url2md -i urls.txt --pack combined.md
    ```

<div class="feature-grid">
<div class="feature-card">
<h3>ğŸš€ High Performance</h3>
<p>Async processing with CPU-adaptive concurrency for batch operations</p>
</div>

<div class="feature-card">
<h3>ğŸ§¹ Clean Output</h3>
<p>Advanced HTML cleaning removes ads, scripts, and clutter</p>
</div>

<div class="feature-card">
<h3>ğŸ”„ Robust</h3>
<p>Automatic retries, panic recovery, and graceful error handling</p>
</div>

<div class="feature-card">
<h3>ğŸ›¡ï¸ CDN Compatible</h3>
<p>Works with Cloudflare, Fastly, Akamai and other modern CDNs</p>
</div>

<div class="feature-card">
<h3>ğŸ“‚ Flexible I/O</h3>
<p>Multiple input sources, structured or packed output options</p>
</div>

<div class="feature-card">
<h3>ğŸ–¥ï¸ Cross-Platform</h3>
<p>Native binaries for Linux, macOS, and Windows</p>
</div>
</div>

## What It Does

`twars-url2md` transforms web pages into clean Markdown through a sophisticated pipeline:

1. **Fetches** web content with robust HTTP client handling
2. **Cleans** HTML by removing scripts, ads, and non-essential elements  
3. **Converts** to well-structured Markdown preserving semantic meaning
4. **Organizes** output with flexible file structure options

## Performance Metrics

<div style="text-align: center; margin: 2rem 0;">
<span class="performance-metric">âš¡ 100+ URLs/min</span>
<span class="performance-metric">ğŸ”„ CPU-adaptive concurrency</span>
<span class="performance-metric">ğŸ“¦ Single 8MB binary</span>
<span class="performance-metric">ğŸš« Zero runtime dependencies</span>
</div>

## Documentation Table of Contents

This documentation is organized into the following sections:

### Getting Started
- **[Installation](installation.md)** - Multiple installation methods including pre-compiled binaries, Cargo, and building from source
- **[Quick Start](quickstart.md)** - Essential examples to get you productive immediately

### User Guide  
- **[Basic Usage](usage.md)** - Command-line interface, input formats, and common workflows
- **[Advanced Features](advanced.md)** - Packed output, URL extraction, local file processing, and complex scenarios
- **[Configuration](configuration.md)** - Customization options, logging, retries, and environment variables

### Development
- **[Architecture](architecture.md)** - Technical deep dive into design, components, and performance optimizations
- **[Contributing](contributing.md)** - Development setup, coding standards, and contribution guidelines  
- **[Testing](testing.md)** - Test suite overview, running tests, and benchmarking
- **[API Reference](api.md)** - Library usage, Rust API documentation, and integration examples

## Key Use Cases

=== "Content Archiving"
    Perfect for researchers, academics, or knowledge workers who need to preserve web content for offline access or citation.
    
    ```bash
    # Archive a list of research papers
    twars-url2md -i research_urls.txt -o archive/ -v
    ```

=== "Documentation Migration"
    Convert existing web-based documentation to Markdown for integration with static site generators.
    
    ```bash
    # Convert documentation sites to markdown
    echo "https://old-docs.example.com" | twars-url2md --stdin --pack new-docs.md
    ```

=== "Content Curation"
    Build curated collections by combining multiple web sources into organized Markdown files.
    
    ```bash
    # Curate articles into a single document
    twars-url2md -i article_list.txt --pack weekly-digest.md
    ```

=== "Build Pipeline Integration"
    Integrate into CI/CD workflows or build systems for automated content processing.
    
    ```bash
    # Process in CI environment  
    twars-url2md -i $INPUT_FILE -o $OUTPUT_DIR --verbose
    ```

## Quick Navigation

| Section | Description | Best For |
|---------|-------------|----------|
| [Installation](installation.md) | Get up and running | New users |
| [Usage](usage.md) | Core functionality | Daily users |
| [Advanced Features](advanced.md) | Power user features | Complex workflows |
| [Architecture](architecture.md) | Technical details | Developers |
| [API Reference](api.md) | Library integration | Rust developers |

---

!!! tip "Need Help?"
    - Check the [troubleshooting section](usage.md#troubleshooting) for common issues
    - Visit the [GitHub repository](https://github.com/twardoch/twars-url2md) for bug reports and feature requests
    - Review the [API documentation](https://docs.rs/twars-url2md) for library usage

*Built with â¤ï¸ in Rust by [Adam Twardoch](https://github.com/twardoch)*
</document_content>
</document>

<document index="34">
<source>src_docs/md/installation.md</source>
<document_content>
# Installation

`twars-url2md` provides multiple installation methods to suit different needs and environments. Choose the method that works best for your setup.

## Pre-compiled Binaries (Recommended)

Pre-compiled binaries are the fastest way to get started and require no additional dependencies.

### One-Line Installation

=== "Linux & macOS"
    ```bash
    curl -fsSL https://raw.githubusercontent.com/twardoch/twars-url2md/main/install.sh | bash
    ```

=== "Custom Directory"
    ```bash
    curl -fsSL https://raw.githubusercontent.com/twardoch/twars-url2md/main/install.sh | bash -s -- --install-dir ~/.local/bin
    ```

=== "Windows PowerShell"
    ```powershell
    # Download and run the Windows installer (manual process)
    # See manual installation section below
    ```

### Manual Binary Installation

Download the latest release for your platform from the [GitHub Releases page](https://github.com/twardoch/twars-url2md/releases/latest).

=== "macOS"
    ```bash
    # Intel x86_64
    curl -L https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-macos-x86_64.tar.gz | tar xz
    
    # Apple Silicon (M1/M2/M3)
    curl -L https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-macos-aarch64.tar.gz | tar xz
    
    # Move to PATH
    sudo mv twars-url2md /usr/local/bin/
    ```

=== "Linux"
    ```bash
    # x86_64
    curl -L https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-linux-x86_64.tar.gz | tar xz
    
    # ARM64 (aarch64)
    curl -L https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-linux-aarch64.tar.gz | tar xz
    
    # Static binary (musl) - works on any Linux
    curl -L https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-linux-x86_64-musl.tar.gz | tar xz
    
    # Move to PATH
    sudo mv twars-url2md /usr/local/bin/
    ```

=== "Windows"
    ```powershell
    # Download the zip file
    Invoke-WebRequest -Uri https://github.com/twardoch/twars-url2md/releases/latest/download/twars-url2md-windows-x86_64.zip -OutFile twars-url2md.zip
    
    # Extract
    Expand-Archive twars-url2md.zip -DestinationPath .
    
    # Move to a directory in your PATH (example: C:\Windows\System32)
    # Or add the current directory to your PATH environment variable
    ```

## Package Managers

### Cargo (Rust Package Manager)

If you have Rust installed (version 1.70.0 or later):

```bash
cargo install twars-url2md
```

This method compiles from source and may take a few minutes but ensures you get the latest version optimized for your system.

### Homebrew (macOS/Linux)

!!! warning "Coming Soon"
    Homebrew formula is planned for a future release.

### Chocolatey (Windows)

!!! warning "Coming Soon"
    Chocolatey package is planned for a future release.

## Building from Source

For developers or users who want the latest features:

### Prerequisites

- **Rust**: Version 1.70.0 or later ([Install Rust](https://rustup.rs/))
- **Git**: For cloning the repository
- **C compiler**: Usually included with system development tools

### Build Process

```bash
# Clone the repository
git clone https://github.com/twardoch/twars-url2md.git
cd twars-url2md

# Build in release mode (optimized)
cargo build --release

# The binary will be in target/release/twars-url2md
```

### Install Locally

```bash
# Install to ~/.cargo/bin (must be in your PATH)
cargo install --path .
```

### Development Build

For development or testing:

```bash
# Debug build (faster compilation, slower execution)
cargo build

# Run directly without installing
cargo run -- --help
```

## Verification

After installation, verify that `twars-url2md` is working correctly:

```bash
# Check version and build info
twars-url2md --version

# Test with a simple URL
twars-url2md https://httpbin.org/html -o test_output/
```

Expected output should show version information and successfully create a markdown file.

## Container Usage

### Docker

While there's no official Docker image yet, you can create one using the static binary:

```dockerfile
FROM alpine:latest
RUN apk add --no-cache ca-certificates
COPY twars-url2md /usr/local/bin/
ENTRYPOINT ["twars-url2md"]
```

Build and run:

```bash
# Build the Docker image
docker build -t twars-url2md .

# Run with volume mount for output
docker run -v $(pwd)/output:/output twars-url2md https://example.com -o /output
```

## Troubleshooting Installation

### Common Issues

=== "Permission Denied"
    ```bash
    # If you get permission denied when moving to /usr/local/bin
    sudo mv twars-url2md /usr/local/bin/
    
    # Or install to a user directory
    mkdir -p ~/.local/bin
    mv twars-url2md ~/.local/bin/
    echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
    source ~/.bashrc
    ```

=== "Command Not Found"
    ```bash
    # Verify the binary is in your PATH
    which twars-url2md
    
    # Check your PATH
    echo $PATH
    
    # Add directory to PATH if needed
    export PATH="/usr/local/bin:$PATH"
    ```

=== "SSL/TLS Errors"
    ```bash
    # Update CA certificates (Linux)
    sudo apt-get update && sudo apt-get install ca-certificates
    
    # macOS - update certificates
    brew update && brew upgrade ca-certificates
    ```

=== "Rust Compilation Errors"
    ```bash
    # Update Rust toolchain
    rustup update
    
    # Install required components
    rustup component add rustfmt clippy
    
    # Clear cache and rebuild
    cargo clean
    cargo build --release
    ```

### Platform-Specific Notes

=== "Linux"
    - **Static binary** (`musl`) works on any Linux distribution
    - **Dynamic binary** requires glibc 2.17+ (available on most modern distributions)
    - For older distributions, use the static binary or build from source

=== "macOS"
    - **x86_64** version works on Intel Macs and Apple Silicon under Rosetta 2
    - **aarch64** version is optimized for Apple Silicon (M1/M2/M3)
    - macOS 10.15+ required for pre-compiled binaries

=== "Windows"
    - Requires Windows 10 or later
    - Binary is built with MSVC toolchain
    - No additional runtime dependencies required

## Next Steps

Once installed, proceed to the [Quick Start Guide](quickstart.md) to learn basic usage, or jump to [Basic Usage](usage.md) for comprehensive command-line documentation.

---

!!! tip "Stay Updated"
    - Watch the [GitHub repository](https://github.com/twardoch/twars-url2md) for new releases
    - Check the [changelog](https://github.com/twardoch/twars-url2md/releases) for version updates
    - Consider enabling GitHub notifications for release announcements
</document_content>
</document>

<document index="35">
<source>src_docs/md/quickstart.md</source>
<document_content>
# Quick Start

Get up and running with `twars-url2md` in minutes. This guide covers the most common use cases with practical examples.

## Basic Conversion

### Single URL

Convert one URL to Markdown:

```bash
# Convert to stdout (preview)
twars-url2md https://www.rust-lang.org

# Save to a directory structure
twars-url2md https://www.rust-lang.org -o output/

# Save to a specific file
twars-url2md https://www.rust-lang.org -o rust-lang.md
```

### Multiple URLs

```bash
# Multiple URLs as arguments
twars-url2md https://www.rust-lang.org https://crates.io -o output/

# From a file (one URL per line)
echo -e "https://www.rust-lang.org\nhttps://crates.io" > urls.txt
twars-url2md -i urls.txt -o output/
```

## Input Methods

### From Files

Create a file with URLs:

```bash
# Create URL list
cat > my_urls.txt << EOF
https://doc.rust-lang.org/book/
https://doc.rust-lang.org/std/
https://doc.rust-lang.org/cargo/
EOF

# Process the file
twars-url2md -i my_urls.txt -o rust_docs/
```

### From Standard Input

```bash
# Pipe URLs from another command
curl -s https://example.com/links.html | \
  twars-url2md --stdin --base-url https://example.com -o extracted/

# Manual input (Ctrl+D to finish)
twars-url2md --stdin -o manual_input/
```

### From HTML/Markdown Content

`twars-url2md` can extract URLs from HTML or Markdown content:

```bash
# Extract links from a webpage
curl -s https://awesome-rust.com | \
  twars-url2md --stdin --base-url https://awesome-rust.com -o awesome_rust/

# Process a markdown file with links
twars-url2md -i README.md --base-url https://github.com/user/repo -o docs/
```

## Output Options

### Directory Structure (Default)

Creates organized directory structure:

```bash
twars-url2md https://doc.rust-lang.org/book/ch01-01-installation.html -o output/
```

Creates: `output/doc.rust-lang.org/book/ch01-01-installation.md`

### Single File Output

```bash
# Specify .md extension
twars-url2md https://doc.rust-lang.org/book/ -o rust-book.md
```

### Packed Output

Combine multiple URLs into one file with headers:

```bash
# Pack multiple URLs
twars-url2md -i urls.txt --pack combined-docs.md

# Pack with individual files too
twars-url2md -i urls.txt -o individual/ --pack combined.md
```

Example packed output:
```markdown
# https://www.rust-lang.org

Content from rust-lang.org...

# https://crates.io

Content from crates.io...
```

## Common Workflows

### Documentation Archiving

```bash
# Archive a project's documentation
cat > project_docs.txt << EOF
https://project.example.com/docs/
https://project.example.com/api/
https://project.example.com/tutorials/
EOF

twars-url2md -i project_docs.txt -o project_archive/ --pack project_complete.md -v
```

### Research Collection

```bash
# Collect research papers
twars-url2md \
  https://arxiv.org/abs/2301.12345 \
  https://papers.nips.cc/paper/2023/hash/abcd1234 \
  --pack research_collection.md \
  -v
```

### Blog Post Conversion

```bash
# Convert blog posts to markdown
find blog_urls.txt -type f | \
  twars-url2md --stdin -o blog_archive/ \
  --verbose
```

## Local File Processing

Process local HTML files:

```bash
# Single local file
twars-url2md /path/to/document.html -o converted/

# Multiple local files
find . -name "*.html" | twars-url2md --stdin -o html_converted/

# With file:// URLs
twars-url2md file:///absolute/path/to/document.html -o output/
```

## Useful Options

### Verbose Mode

See detailed processing information:

```bash
twars-url2md -i urls.txt -o output/ -v
```

### Custom Base URL

Resolve relative links in extracted content:

```bash
curl -s https://news.ycombinator.com | \
  twars-url2md --stdin --base-url https://news.ycombinator.com -o hn_links/
```

## Quick Examples by Use Case

=== "Website Backup"
    ```bash
    # Backup important pages
    cat > backup_urls.txt << EOF
    https://company.com/important-doc
    https://company.com/api-reference  
    https://company.com/user-guide
    EOF
    
    twars-url2md -i backup_urls.txt -o backup/ --pack complete-backup.md -v
    ```

=== "Research Collection"
    ```bash
    # Academic papers and articles
    twars-url2md \
      "https://arxiv.org/abs/2301.07041" \
      "https://openreview.net/forum?id=abc123" \
      --pack research-$(date +%Y%m%d).md
    ```

=== "Tutorial Archive"
    ```bash
    # Save programming tutorials
    echo "https://doc.rust-lang.org/book/" | \
      twars-url2md --stdin -o tutorials/ -v
    ```

=== "News Articles"
    ```bash
    # Daily news collection
    cat today_articles.txt | \
      twars-url2md --stdin --pack "news-$(date +%Y-%m-%d).md"
    ```

## Performance Tips

### Batch Processing

For large numbers of URLs:

```bash
# Enable verbose logging to monitor progress
twars-url2md -i large_url_list.txt -o output/ -v

# Split large files for parallel processing
split -l 100 huge_urls.txt chunk_
for chunk in chunk_*; do
  twars-url2md -i "$chunk" -o "output_${chunk}/" &
done
wait
```

### Resource Management

```bash
# Monitor system resources during large jobs
htop &  # or your preferred system monitor
twars-url2md -i many_urls.txt -o output/ -v
```

## Troubleshooting Quick Fixes

### Common Issues

```bash
# SSL certificate issues
twars-url2md https://site-with-ssl-issues.com -v  # Check verbose output

# Timeout issues
twars-url2md https://slow-site.com -v  # Monitor with verbose logging

# Permission issues
mkdir -p ~/my_output
twars-url2md https://example.com -o ~/my_output/
```

### Verification

```bash
# Test installation
twars-url2md --version

# Test with a reliable URL
twars-url2md https://httpbin.org/html -o test/

# Verify output
ls -la test/
cat test/httpbin.org/html.md
```

## Next Steps

Now that you're familiar with basic usage:

- Learn about [Advanced Features](advanced.md) for complex workflows  
- Review [Configuration](configuration.md) for customization options
- Check [Usage](usage.md) for comprehensive command reference
- Explore [API Reference](api.md) for library integration

---

!!! tip "Pro Tips"
    - Use `-v` (verbose) to monitor progress on large jobs
    - The `--pack` option is great for creating single-file archives
    - Directory structure output mirrors the original URL hierarchy
    - Local HTML files can be processed just like remote URLs
</document_content>
</document>

<document index="36">
<source>src_docs/md/testing.md</source>
<document_content>
# Testing

Comprehensive testing strategy and guidelines for `twars-url2md`, covering unit tests, integration tests, benchmarks, and quality assurance processes.

## Test Architecture

### Test Organization

```
tests/
â”œâ”€â”€ unit/                    # Component-specific tests
â”‚   â”œâ”€â”€ mod.rs              # Test module organization
â”‚   â””â”€â”€ url_tests.rs        # URL processing tests
â”œâ”€â”€ integration/            # End-to-end workflow tests
â”‚   â”œâ”€â”€ e2e_tests.rs       # Complete processing workflows
â”‚   â””â”€â”€ mod.rs             # Integration test utilities
â”œâ”€â”€ fixtures/              # Test data and expected outputs
â”‚   â”œâ”€â”€ html/              # Sample HTML files
â”‚   â”‚   â”œâ”€â”€ simple.html    # Basic HTML structure
â”‚   â”‚   â””â”€â”€ complex.html   # Complex HTML with edge cases
â”‚   â”œâ”€â”€ expected/          # Expected conversion outputs
â”‚   â”‚   â””â”€â”€ simple_output.md
â”‚   â””â”€â”€ urls/              # URL test lists
â”‚       â”œâ”€â”€ test_urls.txt  # Basic URL list
â”‚       â””â”€â”€ mixed_content.txt # URLs with mixed content
â”œâ”€â”€ benchmarks/            # Performance benchmarks
â”‚   â””â”€â”€ benchmarks.rs      # Performance test suite
â”œâ”€â”€ common/                # Shared test utilities
â”‚   â””â”€â”€ mod.rs            # Common test functions and setup
â””â”€â”€ tests.rs              # Main test runner
```

### Test Categories

1. **Unit Tests**: Individual component functionality
2. **Integration Tests**: End-to-end workflows
3. **Performance Tests**: Benchmarks and profiling
4. **Compatibility Tests**: Edge cases and different inputs
5. **Security Tests**: Input validation and safety

## Running Tests

### Basic Test Commands

```bash
# Run all tests
cargo test --all-features

# Run specific test module
cargo test url_tests --all-features

# Run integration tests only
cargo test --test integration --all-features

# Run with output (for debugging)
cargo test --all-features -- --nocapture

# Run single test function
cargo test test_url_extraction --all-features
```

### Advanced Test Execution

```bash
# Run tests with verbose output
cargo test --all-features --verbose

# Run tests in single thread (for debugging)
cargo test --all-features -- --test-threads=1

# Run ignored tests (marked with #[ignore])
cargo test --all-features -- --ignored

# Show test timing
cargo test --all-features -- --report-time

# Filter tests by name pattern  
cargo test url --all-features  # Runs tests with "url" in name
```

### Test Configuration

```bash
# Set environment variables for tests
RUST_LOG=debug cargo test --all-features

# Test with specific configuration
TEST_TIMEOUT=30 cargo test integration --all-features

# Test with network access disabled
OFFLINE_MODE=1 cargo test unit --all-features
```

## Unit Tests

### URL Processing Tests

```rust
#[cfg(test)]
mod url_tests {
    use super::*;
    
    #[test]
    fn test_url_extraction_from_plain_text() {
        let text = "Check out https://example.com and https://rust-lang.org";
        let urls = extract_urls_from_text(text, None);
        
        assert_eq!(urls.len(), 2);
        assert!(urls.contains(&"https://example.com".to_string()));
        assert!(urls.contains(&"https://rust-lang.org".to_string()));
    }
    
    #[test]
    fn test_url_extraction_from_html() {
        let html = r#"
            <html>
                <body>
                    <a href="https://example.com">Example</a>
                    <a href="/relative/path">Relative</a>
                </body>
            </html>
        "#;
        
        let urls = extract_urls_from_text(html, Some("https://base.com"));
        
        assert!(urls.contains(&"https://example.com".to_string()));
        assert!(urls.contains(&"https://base.com/relative/path".to_string()));
    }
    
    #[test]
    fn test_url_validation() {
        let invalid_urls = vec![
            "not-a-url",
            "ftp://invalid-scheme.com",
            "mailto:test@example.com",
            "",
        ];
        
        for url in invalid_urls {
            let result = validate_url(url);
            assert!(result.is_err(), "URL should be invalid: {}", url);
        }
    }
    
    #[test]
    fn test_path_generation() {
        let url = "https://doc.rust-lang.org/book/ch01-01-installation.html";
        let base_path = PathBuf::from("output");
        
        let result = url_to_file_path(url, &base_path);
        let expected = PathBuf::from("output/doc.rust-lang.org/book/ch01-01-installation.md");
        
        assert_eq!(result, expected);
    }
}
```

### HTML Processing Tests

```rust
#[cfg(test)]
mod html_tests {
    use super::*;
    use mockito::Server;
    
    #[tokio::test]
    async fn test_html_fetching() {
        let mut server = Server::new_async().await;
        let mock = server
            .mock("GET", "/test")
            .with_status(200)
            .with_header("content-type", "text/html")
            .with_body("<html><body>Test content</body></html>")
            .create_async()
            .await;
        
        let url = format!("{}/test", server.url());
        let result = fetch_html_content(&url).await;
        
        assert!(result.is_ok());
        let content = result.unwrap();
        assert!(content.contains("Test content"));
        
        mock.assert_async().await;
    }
    
    #[tokio::test]
    async fn test_retry_logic() {
        let mut server = Server::new_async().await;
        
        // First request fails, second succeeds
        let mock_fail = server
            .mock("GET", "/retry-test")
            .with_status(500)
            .create_async()
            .await;
        
        let mock_success = server
            .mock("GET", "/retry-test")
            .with_status(200)
            .with_body("Success")
            .create_async()
            .await;
        
        let url = format!("{}/retry-test", server.url());
        let result = fetch_with_retry(&url, 3).await;
        
        assert!(result.is_ok());
        mock_fail.assert_async().await;
        mock_success.assert_async().await;
    }
}
```

### Markdown Conversion Tests

```rust
#[cfg(test)]
mod markdown_tests {
    use super::*;
    
    #[test]
    fn test_html_to_markdown_conversion() {
        let html = r#"
            <html>
                <head><title>Test Page</title></head>
                <body>
                    <h1>Main Heading</h1>
                    <p>This is a <strong>bold</strong> paragraph with a 
                       <a href="https://example.com">link</a>.</p>
                    <ul>
                        <li>First item</li>
                        <li>Second item</li>
                    </ul>
                </body>
            </html>
        "#;
        
        let markdown = html_to_markdown(html).unwrap();
        
        assert!(markdown.contains("# Main Heading"));
        assert!(markdown.contains("**bold**"));
        assert!(markdown.contains("[link](https://example.com)"));
        assert!(markdown.contains("- First item"));
        assert!(markdown.contains("- Second item"));
    }
    
    #[test]
    fn test_complex_html_structures() {
        let html = include_str!("../fixtures/html/complex.html");
        let result = html_to_markdown(html);
        
        assert!(result.is_ok());
        let markdown = result.unwrap();
        
        // Verify specific structures are preserved
        assert!(markdown.contains("# "));  // Headers preserved
        assert!(markdown.contains("| "));  // Tables preserved
        assert!(markdown.contains("```")); // Code blocks preserved
    }
}
```

## Integration Tests

### End-to-End Workflow Tests

```rust
// tests/integration/e2e_tests.rs
use twars_url2md::{process_urls, Config};
use tempfile::TempDir;
use std::fs;

#[tokio::test]
async fn test_single_url_processing() {
    let temp_dir = TempDir::new().unwrap();
    let config = Config {
        output_base: temp_dir.path().to_path_buf(),
        verbose: false,
        max_retries: 2,
        single_file: false,
        has_output: true,
        pack_file: None,
    };
    
    let urls = vec!["https://httpbin.org/html".to_string()];
    let result = process_urls(urls, config).await;
    
    assert!(result.is_ok());
    
    // Verify output file was created
    let output_files: Vec<_> = fs::read_dir(temp_dir.path())
        .unwrap()
        .map(|entry| entry.unwrap().path())
        .collect();
    
    assert!(!output_files.is_empty());
    assert!(output_files.iter().any(|p| p.extension().unwrap() == "md"));
}

#[tokio::test]
async fn test_packed_output_format() {
    let temp_dir = TempDir::new().unwrap();
    let pack_file = temp_dir.path().join("packed.md");
    
    let config = Config {
        output_base: temp_dir.path().to_path_buf(),
        verbose: false,
        max_retries: 2,
        single_file: false,
        has_output: false,
        pack_file: Some(pack_file.clone()),
    };
    
    let urls = vec![
        "https://httpbin.org/html".to_string(),
        "https://httpbin.org/json".to_string(),
    ];
    
    let result = process_urls(urls, config).await;
    assert!(result.is_ok());
    
    // Verify packed file was created with proper format
    let content = fs::read_to_string(&pack_file).unwrap();
    assert!(content.contains("# https://httpbin.org/html"));
    assert!(content.contains("# https://httpbin.org/json"));
}

#[tokio::test]
async fn test_error_handling() {
    let temp_dir = TempDir::new().unwrap();
    let config = Config {
        output_base: temp_dir.path().to_path_buf(),
        verbose: false,
        max_retries: 1,
        single_file: false,
        has_output: true,
        pack_file: None,
    };
    
    let urls = vec![
        "https://httpbin.org/html".to_string(),      // Valid URL
        "https://invalid-domain-12345.com".to_string(), // Invalid URL
        "https://httpbin.org/status/404".to_string(),   // 404 error
    ];
    
    let result = process_urls(urls, config).await;
    
    // Should complete but with some errors
    assert!(result.is_ok());
    let errors = result.unwrap();
    assert!(!errors.is_empty()); // Should have errors for invalid URLs
}
```

### File Input/Output Tests

```rust
#[tokio::test]
async fn test_file_input_processing() {
    let temp_dir = TempDir::new().unwrap();
    
    // Create input file with URLs
    let input_file = temp_dir.path().join("urls.txt");
    fs::write(&input_file, "https://httpbin.org/html\nhttps://httpbin.org/json").unwrap();
    
    // Process using file input
    let output = Command::new("cargo")
        .args(&["run", "--", "-i"])
        .arg(&input_file)
        .arg("-o")
        .arg(temp_dir.path().join("output"))
        .output()
        .await
        .unwrap();
    
    assert!(output.status.success());
    
    // Verify output files
    let output_dir = temp_dir.path().join("output");
    let entries: Vec<_> = fs::read_dir(output_dir).unwrap().collect();
    assert!(!entries.is_empty());
}

#[tokio::test]
async fn test_stdin_input() {
    let temp_dir = TempDir::new().unwrap();
    
    let mut child = Command::new("cargo")
        .args(&["run", "--", "--stdin", "-o"])
        .arg(temp_dir.path().join("output"))
        .stdin(Stdio::piped())
        .stdout(Stdio::piped())
        .stderr(Stdio::piped())
        .spawn()
        .unwrap();
    
    // Send URLs via stdin
    let stdin = child.stdin.as_mut().unwrap();
    stdin.write_all(b"https://httpbin.org/html\n").await.unwrap();
    stdin.flush().await.unwrap();
    drop(child.stdin.take());
    
    let output = child.wait_with_output().await.unwrap();
    assert!(output.status.success());
}
```

## Performance Testing

### Benchmark Tests

```rust
// tests/benchmarks/benchmarks.rs
use criterion::{criterion_group, criterion_main, BenchmarkId, Criterion, Throughput};
use twars_url2md::*;

fn benchmark_url_extraction(c: &mut Criterion) {
    let mut group = c.benchmark_group("url_extraction");
    
    let test_cases = vec![
        ("small", include_str!("../fixtures/html/simple.html")),
        ("medium", include_str!("../fixtures/html/medium.html")),
        ("large", include_str!("../fixtures/html/complex.html")),
    ];
    
    for (name, content) in test_cases {
        group.throughput(Throughput::Bytes(content.len() as u64));
        group.bench_with_input(
            BenchmarkId::new("extract_urls", name),
            content,
            |b, content| {
                b.iter(|| extract_urls_from_text(content, None))
            },
        );
    }
    
    group.finish();
}

fn benchmark_markdown_conversion(c: &mut Criterion) {
    let html_content = include_str!("../fixtures/html/complex.html");
    
    c.bench_function("html_to_markdown", |b| {
        b.iter(|| html_to_markdown(html_content))
    });
}

fn benchmark_concurrent_processing(c: &mut Criterion) {
    let mut group = c.benchmark_group("concurrent_processing");
    
    for url_count in [1, 5, 10, 20].iter() {
        group.bench_with_input(
            BenchmarkId::new("process_urls", url_count),
            url_count,
            |b, &url_count| {
                let urls: Vec<String> = (0..url_count)
                    .map(|i| format!("https://httpbin.org/html?id={}", i))
                    .collect();
                
                b.to_async(tokio::runtime::Runtime::new().unwrap())
                    .iter(|| async {
                        // Benchmark implementation
                    });
            },
        );
    }
    
    group.finish();
}

criterion_group!(
    benches,
    benchmark_url_extraction,
    benchmark_markdown_conversion,
    benchmark_concurrent_processing
);
criterion_main!(benches);
```

### Memory Usage Tests

```rust
#[cfg(test)]
mod memory_tests {
    use super::*;
    
    #[test]
    fn test_memory_usage_large_files() {
        let large_html = "a".repeat(10_000_000); // 10MB string
        
        // Monitor memory usage during processing
        let initial_memory = get_memory_usage();
        let result = html_to_markdown(&large_html);
        let peak_memory = get_memory_usage();
        
        assert!(result.is_ok());
        
        // Memory usage should be reasonable (not loading everything at once)
        let memory_increase = peak_memory - initial_memory;
        assert!(memory_increase < 50_000_000); // Less than 50MB increase
    }
    
    #[tokio::test]
    async fn test_concurrent_memory_usage() {
        let urls: Vec<String> = (0..100)
            .map(|i| format!("https://httpbin.org/html?id={}", i))
            .collect();
        
        let initial_memory = get_memory_usage();
        
        // Process many URLs concurrently
        let _results = process_urls_concurrent(&urls).await;
        
        let final_memory = get_memory_usage();
        let memory_increase = final_memory - initial_memory;
        
        // Memory usage should scale reasonably with concurrent processing
        assert!(memory_increase < 100_000_000); // Less than 100MB
    }
    
    fn get_memory_usage() -> usize {
        // Platform-specific memory usage measurement
        #[cfg(target_os = "linux")]
        {
            use std::fs;
            let status = fs::read_to_string("/proc/self/status").unwrap();
            for line in status.lines() {
                if line.starts_with("VmRSS:") {
                    let parts: Vec<&str> = line.split_whitespace().collect();
                    return parts[1].parse::<usize>().unwrap() * 1024; // Convert KB to bytes
                }
            }
        }
        0
    }
}
```

## Quality Assurance

### Test Data Management

```rust
// tests/common/mod.rs
use std::path::PathBuf;

pub struct TestFixtures;

impl TestFixtures {
    pub fn html_simple() -> &'static str {
        include_str!("../fixtures/html/simple.html")
    }
    
    pub fn html_complex() -> &'static str {
        include_str!("../fixtures/html/complex.html")
    }
    
    pub fn expected_simple_md() -> &'static str {
        include_str!("../fixtures/expected/simple_output.md")
    }
    
    pub fn test_urls() -> Vec<String> {
        include_str!("../fixtures/urls/test_urls.txt")
            .lines()
            .filter(|line| !line.trim().is_empty() && !line.starts_with('#'))
            .map(|line| line.trim().to_string())
            .collect()
    }
}

pub fn setup_test_environment() -> tempfile::TempDir {
    tempfile::tempdir().expect("Failed to create temp directory")
}

pub async fn create_mock_server() -> mockito::Server {
    mockito::Server::new_async().await
}
```

### Automated Test Execution

```yaml
# .github/workflows/test.yml
name: Test Suite

on: [push, pull_request]

jobs:
  test:
    name: Tests
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        rust: [stable, beta]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust
        uses: dtolnay/rust-toolchain@master
        with:
          toolchain: ${{ matrix.rust }}
          components: rustfmt, clippy
      
      - name: Cache dependencies
        uses: Swatinem/rust-cache@v2
      
      - name: Check formatting
        run: cargo fmt --check
        
      - name: Run clippy
        run: cargo clippy --all-targets --all-features -- -D warnings
      
      - name: Run unit tests
        run: cargo test --lib --all-features
      
      - name: Run integration tests
        run: cargo test --test '*' --all-features
      
      - name: Run doc tests
        run: cargo test --doc --all-features

  coverage:
    name: Code Coverage
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
      
      - name: Install cargo-tarpaulin
        run: cargo install cargo-tarpaulin
      
      - name: Generate coverage report
        run: cargo tarpaulin --out Xml --all-features
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

### Local Test Scripts

```bash
#!/bin/bash
# scripts/test.sh - Comprehensive local testing

set -euo pipefail

echo "ğŸ§ª Running comprehensive test suite..."

# Code formatting
echo "ğŸ“‹ Checking code formatting..."
cargo fmt --check

# Linting
echo "ğŸ” Running clippy..."
cargo clippy --all-targets --all-features -- -D warnings

# Unit tests
echo "ğŸ”¬ Running unit tests..."
cargo test --lib --all-features --verbose

# Integration tests
echo "ğŸ”— Running integration tests..."
cargo test --test '*' --all-features --verbose

# Doc tests
echo "ğŸ“š Running documentation tests..."
cargo test --doc --all-features

# Security audit
echo "ğŸ›¡ï¸  Running security audit..."
cargo audit

# Benchmarks (optional)
if [[ "${RUN_BENCHMARKS:-false}" == "true" ]]; then
    echo "âš¡ Running benchmarks..."
    cargo bench
fi

echo "âœ… All tests passed!"
```

### Continuous Monitoring

```bash
# scripts/watch-tests.sh - Development test monitoring
#!/bin/bash

# Install cargo-watch if not present
if ! command -v cargo-watch &> /dev/null; then
    cargo install cargo-watch
fi

# Watch for changes and run tests
cargo watch -x "test --all-features" -x "clippy --all-targets --all-features"
```

---

!!! tip "Testing Best Practices"
    - **Write tests first** for new features (TDD approach)
    - **Test edge cases** and error conditions
    - **Use realistic test data** from fixtures
    - **Keep tests fast** and independent
    - **Mock external dependencies** appropriately
    - **Measure and maintain** good test coverage

!!! note "Performance Testing"
    - Run benchmarks regularly to catch performance regressions
    - Test with realistic data sizes and network conditions
    - Monitor memory usage during development
    - Profile critical paths for optimization opportunities
</document_content>
</document>

<document index="37">
<source>src_docs/md/usage.md</source>
<document_content>
# Basic Usage

This comprehensive guide covers all command-line options, input formats, output modes, and common usage patterns for `twars-url2md`.

## Command Syntax

```bash
twars-url2md [OPTIONS] [URLS...]
```

## Command-Line Options

### Input Options

| Option | Description | Example |
|--------|-------------|---------|
| `[URLS...]` | URLs as command arguments | `twars-url2md https://example.com https://other.com` |
| `-i, --input <FILE>` | Read URLs from file (one per line or extractable text) | `-i urls.txt` |
| `--stdin` | Read URLs from standard input | `echo "https://example.com" \| twars-url2md --stdin` |
| `--base-url <URL>` | Base URL for resolving relative links | `--base-url https://example.com` |

### Output Options

| Option | Description | Example |
|--------|-------------|---------|
| `-o, --output <PATH>` | Output directory or file path | `-o ./markdown_files` or `-o output.md` |
| `-p, --pack <FILE>` | Pack all content into single file with URL headers | `--pack combined.md` |

### Control Options

| Option | Description | Default |
|--------|-------------|---------|
| `-v, --verbose` | Enable verbose logging (INFO/DEBUG levels) | Disabled |
| `-h, --help` | Show help information | - |
| `-V, --version` | Show version and build information | - |

## Input Formats

### Direct URL Arguments

```bash
# Single URL
twars-url2md https://www.rust-lang.org

# Multiple URLs
twars-url2md https://www.rust-lang.org https://crates.io https://doc.rust-lang.org
```

### File Input (`-i, --input`)

Create a file with URLs (one per line):

```txt
https://www.rust-lang.org
https://crates.io
https://doc.rust-lang.org/book/
# Comments are ignored
https://github.com/rust-lang/rust
```

```bash
twars-url2md -i urls.txt -o output/
```

### Standard Input (`--stdin`)

```bash
# From pipeline
echo "https://example.com" | twars-url2md --stdin -o output/

# From here-document
twars-url2md --stdin -o output/ << EOF
https://www.rust-lang.org
https://crates.io
EOF

# From another command
curl -s https://awesome-rust.com | twars-url2md --stdin --base-url https://awesome-rust.com -o awesome/
```

### URL Extraction from Text

`twars-url2md` can extract URLs from various text formats:

=== "HTML Content"
    ```html
    <!-- Input HTML content -->
    <p>Check out <a href="https://www.rust-lang.org">Rust</a> and 
    <a href="/crates">crates.io</a> for packages.</p>
    ```
    
    ```bash
    # Extract and process URLs
    cat content.html | twars-url2md --stdin --base-url https://example.com -o extracted/
    ```

=== "Markdown Content"
    ```markdown
    # Links in Markdown
    Check out [Rust](https://www.rust-lang.org) and [Crates](https://crates.io).
    Also see [relative link](./docs/guide.html).
    ```
    
    ```bash
    twars-url2md -i content.md --base-url https://site.com -o docs/
    ```

=== "Plain Text"
    ```txt
    Visit https://www.rust-lang.org for documentation.
    The package registry is at https://crates.io.
    Relative URLs like /docs/guide need a base URL.
    ```

### Local File Processing

Process local HTML files:

```bash
# Absolute path
twars-url2md /home/user/document.html -o output/

# Relative path  
twars-url2md ./local/file.html -o converted/

# File URL format
twars-url2md file:///absolute/path/to/document.html -o output/

# Multiple local files via find
find ./html_docs -name "*.html" | twars-url2md --stdin -o converted_docs/
```

## Output Modes

### Directory Structure (Default)

Creates hierarchical directory structure mirroring URL paths:

```bash
twars-url2md https://doc.rust-lang.org/book/ch01-01-installation.html -o output/
```

**Result**: `output/doc.rust-lang.org/book/ch01-01-installation.md`

### Single File Output

When output path ends with `.md`, content is saved to that specific file:

```bash
# Single URL to single file
twars-url2md https://www.rust-lang.org -o rust-homepage.md

# Multiple URLs concatenated  
twars-url2md https://rust-lang.org https://crates.io -o combined.md
```

### Packed Output (`--pack`)

Combines multiple URLs into one file with clear delimiters:

```bash
twars-url2md -i urls.txt --pack documentation.md
```

**Packed output format**:
```markdown
# https://www.rust-lang.org

[Content from rust-lang.org homepage]

# https://crates.io  

[Content from crates.io homepage]

# https://doc.rust-lang.org/book/

[Content from Rust book]
```

### Combined Output Modes

Use both directory structure and packed output:

```bash
twars-url2md -i urls.txt -o individual_files/ --pack combined_archive.md
```

This creates:
- Individual files in `individual_files/` directory  
- Combined content in `combined_archive.md`

## Verbose Logging

Enable detailed logging with `-v` or `--verbose`:

```bash
twars-url2md -i urls.txt -o output/ -v
```

**Verbose output includes**:
- URL processing progress
- HTTP request details
- HTML cleaning steps  
- File writing operations
- Error details and retry attempts

### Advanced Logging Control

Use `RUST_LOG` environment variable for fine-grained control:

```bash
# Debug level for twars-url2md only
RUST_LOG=twars_url2md=debug twars-url2md -i urls.txt -o output/

# Trace level (very detailed)
RUST_LOG=twars_url2md=trace twars-url2md -i urls.txt -o output/

# Info level for all components
RUST_LOG=info twars-url2md -i urls.txt -o output/
```

## Error Handling

### Retry Behavior

`twars-url2md` automatically retries failed requests:

- **Default**: 2 additional retries (3 total attempts)
- **Backoff**: Exponential backoff between retries
- **Errors**: Network timeouts, DNS failures, HTTP 5xx errors

### Graceful Failure

- Individual URL failures don't stop batch processing
- Failed URLs are logged but processing continues
- Final exit code reflects overall success/failure

### Common Error Scenarios

=== "Network Issues"
    ```bash
    # Verbose mode shows retry attempts
    twars-url2md https://unreliable-site.com -v
    ```
    
    **Output**:
    ```
    WARN Failed to fetch https://unreliable-site.com: Connection timeout
    INFO Retrying in 1s... (attempt 2/3)
    INFO Successfully fetched after retry
    ```

=== "SSL/TLS Problems"
    ```bash
    twars-url2md https://site-with-ssl-issues.com -v
    ```
    
    **Troubleshooting**:
    - Update system CA certificates
    - Check if site uses self-signed certificates
    - Verify system date/time is correct

=== "HTML Parsing Issues"
    ```bash
    # Monolith panic recovery
    twars-url2md https://site-with-malformed-html.com -v
    ```
    
    If HTML cleaning fails, the tool attempts fallback processing.

## Advanced Usage Patterns

### Pipeline Integration

```bash
# Extract links from webpage and process them
curl -s https://awesome-rust.com | \
  grep -o 'https://[^"]*' | \
  head -20 | \
  twars-url2md --stdin -o awesome_projects/
```

### Batch Processing with Filtering

```bash
# Process only certain domains
grep 'rust-lang.org' all_urls.txt | \
  twars-url2md --stdin -o rust_docs/

# Skip already processed URLs
comm -23 <(sort all_urls.txt) <(sort processed_urls.txt) | \
  twars-url2md --stdin -o remaining/
```

### Monitoring Progress

```bash
# Large job with progress monitoring
wc -l big_url_list.txt  # Check total count
twars-url2md -i big_url_list.txt -o output/ -v | tee conversion.log
```

### Parallel Processing

For very large URL lists, split into chunks:

```bash
# Split large file
split -l 50 huge_urls.txt chunk_

# Process chunks in parallel
for chunk in chunk_*; do
  twars-url2md -i "$chunk" -o "output_${chunk}/" -v &
done
wait

# Combine results if needed
find output_chunk_* -name "*.md" -exec cp {} final_output/ \;
```

## Integration Examples

### CI/CD Pipeline

```yaml
# GitHub Actions example
- name: Convert documentation URLs
  run: |
    echo "${{ vars.DOC_URLS }}" | \
    twars-url2md --stdin -o docs/ --pack complete-docs.md -v
```

### Cron Job

```bash
#!/bin/bash
# daily-news-archive.sh
DATE=$(date +%Y-%m-%d)
curl -s "https://news-api.com/today" | \
  jq -r '.articles[].url' | \
  twars-url2md --stdin --pack "news-${DATE}.md" -v
```

### Build System Integration

```makefile
# Makefile example
docs: urls.txt
	twars-url2md -i urls.txt -o docs/ --pack docs-complete.md -v
	
.PHONY: docs
```

## Performance Considerations

### Concurrency

- **Automatic**: CPU core detection for optimal concurrency
- **Typical**: 2x CPU cores for I/O bound operations
- **Maximum**: Capped at reasonable limits to avoid overwhelming servers

### Memory Usage

- **Streaming**: Minimal memory footprint per URL
- **Batch Size**: Automatically managed based on available resources
- **Large Files**: Handled efficiently without loading entire content into memory

### Network Behavior

- **HTTP/2**: Preferred when available for better CDN compatibility
- **Connection Reuse**: Efficient connection pooling
- **Rate Limiting**: Respectful of server resources

## Troubleshooting

### Common Issues

=== "Permission Denied"
    ```bash
    # Create output directory first
    mkdir -p output
    twars-url2md https://example.com -o output/
    
    # Or use user directory
    twars-url2md https://example.com -o ~/Documents/markdown/
    ```

=== "Empty Output Files"
    ```bash
    # Enable verbose mode to diagnose
    twars-url2md https://problematic-url.com -v
    
    # Check if content is JavaScript-rendered
    # (not supported - requires pre-rendered HTML)
    ```

=== "Encoding Issues"
    ```bash
    # Modern sites should work automatically
    # Check verbose output for encoding detection
    twars-url2md https://non-english-site.com -v
    ```

### Debug Mode

For development or troubleshooting:

```bash
# Maximum verbosity
RUST_LOG=trace twars-url2md https://example.com -v

# Focus on specific components  
RUST_LOG=twars_url2md::html=debug twars-url2md https://example.com -v
```

---

!!! tip "Best Practices"
    - Always use `-v` for large batch jobs to monitor progress
    - Test with a small set of URLs before processing large lists
    - Use `--pack` for creating single-file archives
    - Monitor system resources during large conversion jobs
    - Keep URL lists organized and documented for reproducibility
</document_content>
</document>

<document index="38">
<source>src_docs/mkdocs.yml</source>
<document_content>
site_name: twars-url2md Documentation
site_description: A powerful CLI tool that fetches web pages and converts them to clean Markdown format
site_author: Adam Twardoch
site_url: https://twardoch.github.io/twars-url2md/

repo_name: twardoch/twars-url2md
repo_url: https://github.com/twardoch/twars-url2md
edit_uri: edit/main/src_docs/md/

theme:
  name: material
  language: en
  palette:
    # Palette toggle for light mode
    - scheme: default
      primary: blue
      accent: blue
      toggle:
        icon: material/brightness-7
        name: Switch to dark mode
    # Palette toggle for dark mode
    - scheme: slate
      primary: blue
      accent: blue
      toggle:
        icon: material/brightness-4
        name: Switch to light mode
  features:
    - navigation.tabs
    - navigation.tabs.sticky
    - navigation.sections
    - navigation.expand
    - navigation.top
    - navigation.footer
    - search.highlight
    - search.share
    - search.suggest
    - content.code.copy
    - content.code.select
    - content.code.annotate
    - content.tabs.link
    - content.action.edit
    - content.action.view
  icon:
    repo: fontawesome/brands/github
    edit: material/pencil
    view: material/eye

plugins:
  - search:
      lang: en
  - minify:
      minify_html: true

markdown_extensions:
  - admonition
  - attr_list
  - codehilite:
      guess_lang: false
  - def_list
  - footnotes
  - md_in_html
  - meta
  - toc:
      permalink: true
      title: On this page
  - tables
  - pymdownx.arithmatex:
      generic: true
  - pymdownx.betterem:
      smart_enable: all
  - pymdownx.caret
  - pymdownx.details
  - pymdownx.emoji:
      emoji_index: !!python/name:material.extensions.emoji.twemoji
      emoji_generator: !!python/name:material.extensions.emoji.to_svg
  - pymdownx.highlight:
      anchor_linenums: true
      line_spans: __span
      pygments_lang_class: true
  - pymdownx.inlinehilite
  - pymdownx.keys
  - pymdownx.magiclink:
      repo_url_shorthand: true
      user: twardoch
      repo: twars-url2md
  - pymdownx.mark
  - pymdownx.smartsymbols
  - pymdownx.superfences:
      custom_fences:
        - name: mermaid
          class: mermaid
          format: !!python/name:pymdownx.superfences.fence_code_format
  - pymdownx.tabbed:
      alternate_style: true
  - pymdownx.tasklist:
      custom_checkbox: true
  - pymdownx.tilde

nav:
  - Home: index.md
  - Getting Started:
    - Installation: installation.md
    - Quick Start: quickstart.md
  - User Guide:
    - Basic Usage: usage.md
    - Advanced Features: advanced.md
    - Configuration: configuration.md
  - Development:
    - Architecture: architecture.md
    - Contributing: contributing.md
    - Testing: testing.md
    - API Reference: api.md

extra:
  version:
    provider: mike
  social:
    - icon: fontawesome/brands/github
      link: https://github.com/twardoch/twars-url2md
    - icon: fontawesome/brands/rust
      link: https://crates.io/crates/twars-url2md

copyright: Copyright &copy; 2024 Adam Twardoch. Licensed under MIT License.

# Custom CSS
extra_css:
  - assets/extra.css

# Custom JavaScript  
extra_javascript:
  - https://polyfill.io/v3/polyfill.min.js?features=es6
  - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js

docs_dir: md
site_dir: ../docs
</document_content>
</document>

# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/test_http.rs
# Language: rust



# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/tests/benchmarks.rs
# Language: rust

mod benchmark_tests;


# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/tests/common/mod.rs
# Language: rust



<document index="39">
<source>tests/fixtures/expected/simple_output.md</source>
<document_content>
# Test Page

This is a simple test page with **bold** and *italic* text.

* First item
* Second item

Visit [our website](https://example.com) for more information.
</document_content>
</document>

<document index="40">
<source>tests/fixtures/html/complex.html</source>
<document_content>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Complex Test Page</title>
    <style>
        body { font-family: Arial, sans-serif; }
        .highlight { background-color: yellow; }
    </style>
    <script>
        console.log('This script should be ignored');
    </script>
</head>
<body>
    <header>
        <h1>Complex Document Structure</h1>
        <nav>
            <a href="#section1">Section 1</a>
            <a href="#section2">Section 2</a>
        </nav>
    </header>
    
    <main>
        <article id="section1">
            <h2>Section 1: Tables and Lists</h2>
            <table>
                <thead>
                    <tr>
                        <th>Name</th>
                        <th>Value</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Alpha</td>
                        <td>100</td>
                        <td>First value</td>
                    </tr>
                    <tr>
                        <td>Beta</td>
                        <td>200</td>
                        <td>Second value</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Nested Lists</h3>
            <ul>
                <li>Parent 1
                    <ul>
                        <li>Child 1.1</li>
                        <li>Child 1.2</li>
                    </ul>
                </li>
                <li>Parent 2
                    <ol>
                        <li>Numbered child 2.1</li>
                        <li>Numbered child 2.2</li>
                    </ol>
                </li>
            </ul>
        </article>
        
        <article id="section2">
            <h2>Section 2: Media and Code</h2>
            <p>Here's an image: <img src="test.jpg" alt="Test Image" /></p>
            <p>And here's some code:</p>
            <pre><code>function hello() {
    console.log("Hello, World!");
    return 42;
}</code></pre>
            
            <blockquote>
                <p>This is a blockquote with <strong>nested <em>formatting</em></strong>.</p>
            </blockquote>
        </article>
    </main>
    
    <footer>
        <p>&copy; 2024 Test Company. All rights reserved.</p>
    </footer>
</body>
</html>
</document_content>
</document>

<document index="41">
<source>tests/fixtures/html/simple.html</source>
<document_content>
<!DOCTYPE html>
<html>
<head>
    <title>Simple Test Page</title>
</head>
<body>
    <h1>Test Page</h1>
    <p>This is a simple test page with <strong>bold</strong> and <em>italic</em> text.</p>
    <ul>
        <li>First item</li>
        <li>Second item</li>
    </ul>
    <p>Visit <a href="https://example.com">our website</a> for more information.</p>
</body>
</html>
</document_content>
</document>

<document index="42">
<source>tests/fixtures/urls/mixed_content.txt</source>
<document_content>
Check out these resources:

1. The Rust Programming Language: https://rust-lang.org
2. <a href="https://github.com">GitHub</a>
3. [Crates.io](https://crates.io) - The Rust package registry
4. Also see: http://example.com/docs

Some invalid URLs that should be ignored:
- ftp://oldserver.com
- javascript:alert('test')
- mailto:test@example.com

<p>HTML content with <a href="https://embedded-link.com">embedded link</a></p>

Local file: /usr/local/share/doc/test.html
</document_content>
</document>

<document index="43">
<source>tests/fixtures/urls/test_urls.txt</source>
<document_content>
https://example.com
https://rust-lang.org
https://github.com/rust-lang/rust
https://docs.rs
https://crates.io
http://httpbin.org/status/200
https://jsonplaceholder.typicode.com/posts/1
file:///tmp/test.html
</document_content>
</document>

# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/tests/integration/e2e_tests.rs
# Language: rust

mod end_to_end_tests;


# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/tests/integration/mod.rs
# Language: rust

mod e2e_tests;


# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/tests/tests.rs
# Language: rust

mod unit;


# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/tests/unit/mod.rs
# Language: rust

mod url_tests;


# File: /Users/adam/Developer/vcs/github.twardoch/pub/twars-url2md/tests/unit/url_tests.rs
# Language: rust

mod url_extraction_tests;

mod html_extraction_tests;

mod output_path_tests;

mod url_validation_tests;


</documents>