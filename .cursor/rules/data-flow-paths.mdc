---
description: Analysis of data flows, transformations and pipelines for web content processing and markdown conversion
globs: src/**/*.rs,src/html.rs,src/markdown.rs,src/url.rs,src/cli.rs
alwaysApply: false
---


# data-flow-paths

## Core Data Flow Pipeline

1. URL Input Collection (`src/cli.rs`)
   - URLs collected from stdin or input file
   - Validates input source specification (--stdin or --input)
   - Extracts and deduplicates URLs from text content

2. URL Processing (`src/url.rs`)
   - Raw URLs/file paths transformed into standardized URL format
   - Local paths converted to file:// URLs
   - Relative URLs resolved against base URL
   - URLs extracted from HTML content including href/src/data attributes

3. HTML Content Retrieval (`src/html.rs`)
   - Fetches HTML content from validated URLs
   - Implements retry mechanism for failed fetches
   - Monolith library used for content cleaning/extraction
   - Fallback to simple HTML transformation if Monolith fails

4. HTML to Markdown Conversion (`src/html.rs`, `src/markdown.rs`)
   - Cleaned HTML processed into Markdown format
   - Custom content transformations applied
   - Error handling with fallback conversion methods

5. Output Path Generation (`src/url.rs`)
   - Creates output directory structure mirroring URL paths
   - Generates appropriate filenames from URL segments
   - Handles index.md for empty/slash-terminated paths

6. Output Writing (`src/lib.rs`)
   - Individual files written to output directory structure
   - Optional packing mode combines all content into single file
   - Content reordered to match original URL sequence
   - Progress feedback provided via progress bar

## Key Transformation Points

Importance Score: 85
- URL standardization and validation (input -> standardized URL)
- HTML content extraction and cleaning (raw HTML -> processed HTML) 
- Markdown conversion (processed HTML -> markdown)
- Output path generation (URL -> filesystem path)
- Content aggregation in packing mode (multiple files -> single file)

The data flow follows a clear pipeline from URL input through content retrieval, transformation and output generation, with multiple validation and error handling steps throughout the process.

$END$

 If you're using this file in context, clearly say in italics in one small line that "Context added by Giga data-flow-paths".